Search.setIndex({"docnames": ["index", "resources/contributing", "resources/support", "tt_metal_models/get_performance", "tt_metal_models/get_started", "ttnn/about", "ttnn/adding_new_ttnn_operation", "ttnn/api", "ttnn/converting_torch_model_to_ttnn", "ttnn/demos", "ttnn/dependencies/examples", "ttnn/dependencies/index", "ttnn/dependencies/tensor", "ttnn/dependencies/tt_lib", "ttnn/get_started", "ttnn/installing", "ttnn/onboarding", "ttnn/profiling_ttnn_operations", "ttnn/tensor", "ttnn/ttnn/GetDefaultDevice", "ttnn/ttnn/MaxPool2d", "ttnn/ttnn/SetDefaultDevice", "ttnn/ttnn/abs", "ttnn/ttnn/abs_bw", "ttnn/ttnn/acos", "ttnn/ttnn/acos_bw", "ttnn/ttnn/acosh", "ttnn/ttnn/acosh_bw", "ttnn/ttnn/add", "ttnn/ttnn/add_bw", "ttnn/ttnn/addalpha", "ttnn/ttnn/addalpha_bw", "ttnn/ttnn/addcdiv", "ttnn/ttnn/addcdiv_bw", "ttnn/ttnn/addcmul", "ttnn/ttnn/addcmul_bw", "ttnn/ttnn/angle", "ttnn/ttnn/angle_bw", "ttnn/ttnn/arange", "ttnn/ttnn/argmax", "ttnn/ttnn/as_tensor", "ttnn/ttnn/asin", "ttnn/ttnn/asin_bw", "ttnn/ttnn/asinh", "ttnn/ttnn/asinh_bw", "ttnn/ttnn/assign_bw", "ttnn/ttnn/atan", "ttnn/ttnn/atan2", "ttnn/ttnn/atan2_bw", "ttnn/ttnn/atan_bw", "ttnn/ttnn/atanh", "ttnn/ttnn/atanh_bw", "ttnn/ttnn/bias_gelu_bw", "ttnn/ttnn/bitwise_and", "ttnn/ttnn/bitwise_left_shift", "ttnn/ttnn/bitwise_not", "ttnn/ttnn/bitwise_or", "ttnn/ttnn/bitwise_right_shift", "ttnn/ttnn/bitwise_xor", "ttnn/ttnn/cbrt", "ttnn/ttnn/ceil", "ttnn/ttnn/ceil_bw", "ttnn/ttnn/celu", "ttnn/ttnn/celu_bw", "ttnn/ttnn/clamp", "ttnn/ttnn/clamp_bw", "ttnn/ttnn/clip", "ttnn/ttnn/clone", "ttnn/ttnn/close_device", "ttnn/ttnn/concat", "ttnn/ttnn/concat_bw", "ttnn/ttnn/conj", "ttnn/ttnn/conj_bw", "ttnn/ttnn/cos", "ttnn/ttnn/cos_bw", "ttnn/ttnn/cosh", "ttnn/ttnn/cosh_bw", "ttnn/ttnn/create_sharded_memory_config", "ttnn/ttnn/deallocate", "ttnn/ttnn/deg2rad", "ttnn/ttnn/deg2rad_bw", "ttnn/ttnn/digamma", "ttnn/ttnn/digamma_bw", "ttnn/ttnn/div", "ttnn/ttnn/div_bw", "ttnn/ttnn/div_no_nan", "ttnn/ttnn/div_no_nan_bw", "ttnn/ttnn/downsample", "ttnn/ttnn/dump_tensor", "ttnn/ttnn/elu", "ttnn/ttnn/elu_bw", "ttnn/ttnn/embedding", "ttnn/ttnn/embedding_bw", "ttnn/ttnn/empty", "ttnn/ttnn/eq", "ttnn/ttnn/eq_", "ttnn/ttnn/eqz", "ttnn/ttnn/erf", "ttnn/ttnn/erf_bw", "ttnn/ttnn/erfc", "ttnn/ttnn/erfc_bw", "ttnn/ttnn/erfinv", "ttnn/ttnn/erfinv_bw", "ttnn/ttnn/exp", "ttnn/ttnn/exp2", "ttnn/ttnn/exp2_bw", "ttnn/ttnn/exp_bw", "ttnn/ttnn/expm1", "ttnn/ttnn/expm1_bw", "ttnn/ttnn/fill_bw", "ttnn/ttnn/fill_ones_rm", "ttnn/ttnn/fill_rm", "ttnn/ttnn/fill_zero_bw", "ttnn/ttnn/floor", "ttnn/ttnn/floor_bw", "ttnn/ttnn/floor_div", "ttnn/ttnn/fmod", "ttnn/ttnn/fmod_bw", "ttnn/ttnn/format_input_tensor", "ttnn/ttnn/format_output_tensor", "ttnn/ttnn/frac", "ttnn/ttnn/frac_bw", "ttnn/ttnn/from_device", "ttnn/ttnn/from_torch", "ttnn/ttnn/full", "ttnn/ttnn/full_like", "ttnn/ttnn/ge", "ttnn/ttnn/ge_", "ttnn/ttnn/geglu", "ttnn/ttnn/gelu", "ttnn/ttnn/gelu_bw", "ttnn/ttnn/gez", "ttnn/ttnn/global_avg_pool2d", "ttnn/ttnn/glu", "ttnn/ttnn/group_norm", "ttnn/ttnn/gt", "ttnn/ttnn/gt_", "ttnn/ttnn/gtz", "ttnn/ttnn/hardshrink", "ttnn/ttnn/hardshrink_bw", "ttnn/ttnn/hardsigmoid", "ttnn/ttnn/hardsigmoid_bw", "ttnn/ttnn/hardswish", "ttnn/ttnn/hardswish_bw", "ttnn/ttnn/hardtanh", "ttnn/ttnn/hardtanh_bw", "ttnn/ttnn/heaviside", "ttnn/ttnn/hypot", "ttnn/ttnn/hypot_bw", "ttnn/ttnn/i0", "ttnn/ttnn/i0_bw", "ttnn/ttnn/identity", "ttnn/ttnn/imag", "ttnn/ttnn/imag_bw", "ttnn/ttnn/indexed_fill", "ttnn/ttnn/is_imag", "ttnn/ttnn/is_real", "ttnn/ttnn/isclose", "ttnn/ttnn/isfinite", "ttnn/ttnn/isinf", "ttnn/ttnn/isnan", "ttnn/ttnn/isneginf", "ttnn/ttnn/isposinf", "ttnn/ttnn/kv_cache/fill_cache_for_user_", "ttnn/ttnn/kv_cache/update_cache_for_token_", "ttnn/ttnn/l1_loss", "ttnn/ttnn/layer_norm", "ttnn/ttnn/ldexp", "ttnn/ttnn/ldexp_bw", "ttnn/ttnn/le", "ttnn/ttnn/le_", "ttnn/ttnn/leaky_relu", "ttnn/ttnn/leaky_relu_bw", "ttnn/ttnn/lerp", "ttnn/ttnn/lerp_bw", "ttnn/ttnn/lez", "ttnn/ttnn/lgamma", "ttnn/ttnn/lgamma_bw", "ttnn/ttnn/linear", "ttnn/ttnn/load_tensor", "ttnn/ttnn/log", "ttnn/ttnn/log10", "ttnn/ttnn/log10_bw", "ttnn/ttnn/log1p", "ttnn/ttnn/log1p_bw", "ttnn/ttnn/log2", "ttnn/ttnn/log2_bw", "ttnn/ttnn/log_bw", "ttnn/ttnn/log_sigmoid", "ttnn/ttnn/log_sigmoid_bw", "ttnn/ttnn/logaddexp", "ttnn/ttnn/logaddexp2", "ttnn/ttnn/logaddexp2_bw", "ttnn/ttnn/logaddexp_bw", "ttnn/ttnn/logical_and", "ttnn/ttnn/logical_and_", "ttnn/ttnn/logical_not", "ttnn/ttnn/logical_not_", "ttnn/ttnn/logical_or", "ttnn/ttnn/logical_or_", "ttnn/ttnn/logical_xor", "ttnn/ttnn/logical_xor_", "ttnn/ttnn/logit", "ttnn/ttnn/logit_bw", "ttnn/ttnn/logiteps_bw", "ttnn/ttnn/lt", "ttnn/ttnn/lt_", "ttnn/ttnn/ltz", "ttnn/ttnn/mac", "ttnn/ttnn/manage_device", "ttnn/ttnn/matmul", "ttnn/ttnn/max", "ttnn/ttnn/max_bw", "ttnn/ttnn/max_pool2d", "ttnn/ttnn/maximum", "ttnn/ttnn/mean", "ttnn/ttnn/min", "ttnn/ttnn/min_bw", "ttnn/ttnn/minimum", "ttnn/ttnn/mish", "ttnn/ttnn/model_preprocessing/preprocess_model", "ttnn/ttnn/model_preprocessing/preprocess_model_parameters", "ttnn/ttnn/mse_loss", "ttnn/ttnn/mul_bw", "ttnn/ttnn/multigammaln", "ttnn/ttnn/multigammaln_bw", "ttnn/ttnn/multiply", "ttnn/ttnn/ne", "ttnn/ttnn/ne_", "ttnn/ttnn/neg", "ttnn/ttnn/neg_bw", "ttnn/ttnn/nextafter", "ttnn/ttnn/nez", "ttnn/ttnn/nonzero", "ttnn/ttnn/normalize_global", "ttnn/ttnn/normalize_hw", "ttnn/ttnn/ones", "ttnn/ttnn/ones_like", "ttnn/ttnn/open_device", "ttnn/ttnn/outer", "ttnn/ttnn/pad", "ttnn/ttnn/pad_to_tile_shape", "ttnn/ttnn/permute", "ttnn/ttnn/polar", "ttnn/ttnn/polar_bw", "ttnn/ttnn/polygamma", "ttnn/ttnn/polygamma_bw", "ttnn/ttnn/polyval", "ttnn/ttnn/pow", "ttnn/ttnn/pow_bw", "ttnn/ttnn/prelu", "ttnn/ttnn/prod", "ttnn/ttnn/prod_bw", "ttnn/ttnn/rad2deg", "ttnn/ttnn/rad2deg_bw", "ttnn/ttnn/rdiv", "ttnn/ttnn/rdiv_bw", "ttnn/ttnn/real", "ttnn/ttnn/real_bw", "ttnn/ttnn/reallocate", "ttnn/ttnn/reciprocal", "ttnn/ttnn/reciprocal_bw", "ttnn/ttnn/register_post_operation_hook", "ttnn/ttnn/register_pre_operation_hook", "ttnn/ttnn/reglu", "ttnn/ttnn/relu", "ttnn/ttnn/relu6", "ttnn/ttnn/relu6_bw", "ttnn/ttnn/relu_bw", "ttnn/ttnn/relu_max", "ttnn/ttnn/relu_min", "ttnn/ttnn/remainder", "ttnn/ttnn/remainder_bw", "ttnn/ttnn/repeat", "ttnn/ttnn/repeat_bw", "ttnn/ttnn/repeat_interleave", "ttnn/ttnn/reshape", "ttnn/ttnn/rms_norm", "ttnn/ttnn/round", "ttnn/ttnn/round_bw", "ttnn/ttnn/rpow", "ttnn/ttnn/rpow_bw", "ttnn/ttnn/rsqrt", "ttnn/ttnn/rsqrt_bw", "ttnn/ttnn/rsub", "ttnn/ttnn/rsub_bw", "ttnn/ttnn/scatter", "ttnn/ttnn/selu", "ttnn/ttnn/selu_bw", "ttnn/ttnn/set_printoptions", "ttnn/ttnn/sigmoid", "ttnn/ttnn/sigmoid_accurate", "ttnn/ttnn/sigmoid_bw", "ttnn/ttnn/sign", "ttnn/ttnn/sign_bw", "ttnn/ttnn/signbit", "ttnn/ttnn/silu", "ttnn/ttnn/silu_bw", "ttnn/ttnn/sin", "ttnn/ttnn/sin_bw", "ttnn/ttnn/sinh", "ttnn/ttnn/sinh_bw", "ttnn/ttnn/softmax", "ttnn/ttnn/softplus", "ttnn/ttnn/softplus_bw", "ttnn/ttnn/softshrink", "ttnn/ttnn/softshrink_bw", "ttnn/ttnn/softsign", "ttnn/ttnn/softsign_bw", "ttnn/ttnn/split", "ttnn/ttnn/sqrt", "ttnn/ttnn/sqrt_bw", "ttnn/ttnn/square", "ttnn/ttnn/square_bw", "ttnn/ttnn/squared_difference", "ttnn/ttnn/squared_difference_bw", "ttnn/ttnn/std", "ttnn/ttnn/sub_bw", "ttnn/ttnn/subalpha", "ttnn/ttnn/subalpha_bw", "ttnn/ttnn/subtract", "ttnn/ttnn/sum", "ttnn/ttnn/swiglu", "ttnn/ttnn/swish", "ttnn/ttnn/synchronize_device", "ttnn/ttnn/tan", "ttnn/ttnn/tan_bw", "ttnn/ttnn/tanh", "ttnn/ttnn/tanh_bw", "ttnn/ttnn/tanhshrink", "ttnn/ttnn/tanhshrink_bw", "ttnn/ttnn/threshold", "ttnn/ttnn/threshold_bw", "ttnn/ttnn/tilize", "ttnn/ttnn/tilize_with_val_padding", "ttnn/ttnn/to_device", "ttnn/ttnn/to_layout", "ttnn/ttnn/to_memory_config", "ttnn/ttnn/to_torch", "ttnn/ttnn/topk", "ttnn/ttnn/transformer/attention_softmax", "ttnn/ttnn/transformer/attention_softmax_", "ttnn/ttnn/transformer/concatenate_heads", "ttnn/ttnn/transformer/rotary_embedding", "ttnn/ttnn/transformer/sdpa", "ttnn/ttnn/transformer/sdpa_decode", "ttnn/ttnn/transformer/sdpa_decode_gqa", "ttnn/ttnn/transformer/split_query_key_value_and_split_heads", "ttnn/ttnn/tril", "ttnn/ttnn/triu", "ttnn/ttnn/trunc", "ttnn/ttnn/trunc_bw", "ttnn/ttnn/untilize", "ttnn/ttnn/untilize_with_halo_v2", "ttnn/ttnn/untilize_with_unpadding", "ttnn/ttnn/upsample", "ttnn/ttnn/var", "ttnn/ttnn/where", "ttnn/ttnn/where_bw", "ttnn/ttnn/xlogy", "ttnn/ttnn/xlogy_bw", "ttnn/ttnn/zeros", "ttnn/ttnn/zeros_like", "ttnn/tutorials", "ttnn/tutorials/graphing_torch_dit", "ttnn/tutorials/matmul", "ttnn/tutorials/multihead-attention", "ttnn/tutorials/profiling", "ttnn/tutorials/resnet-basic-block", "ttnn/tutorials/tensor_and_add_operation", "ttnn/tutorials/ttnn-tracer", "ttnn/tutorials/ttnn_tutorials/001", "ttnn/tutorials/ttnn_tutorials/002", "ttnn/tutorials/ttnn_tutorials/003", "ttnn/tutorials/ttnn_tutorials/004", "ttnn/tutorials/ttnn_tutorials/005", "ttnn/tutorials/ttnn_tutorials/006", "ttnn/tutorials/ttnn_tutorials/007", "ttnn/usage", "ttnn_sweeps/index"], "filenames": ["index.rst", "resources/contributing.rst", "resources/support.rst", "tt_metal_models/get_performance.rst", "tt_metal_models/get_started.rst", "ttnn/about.rst", "ttnn/adding_new_ttnn_operation.rst", "ttnn/api.rst", "ttnn/converting_torch_model_to_ttnn.rst", "ttnn/demos.rst", "ttnn/dependencies/examples.rst", "ttnn/dependencies/index.rst", "ttnn/dependencies/tensor.rst", "ttnn/dependencies/tt_lib.rst", "ttnn/get_started.rst", "ttnn/installing.md", "ttnn/onboarding.rst", "ttnn/profiling_ttnn_operations.rst", "ttnn/tensor.rst", "ttnn/ttnn/GetDefaultDevice.rst", "ttnn/ttnn/MaxPool2d.rst", "ttnn/ttnn/SetDefaultDevice.rst", "ttnn/ttnn/abs.rst", "ttnn/ttnn/abs_bw.rst", "ttnn/ttnn/acos.rst", "ttnn/ttnn/acos_bw.rst", "ttnn/ttnn/acosh.rst", "ttnn/ttnn/acosh_bw.rst", "ttnn/ttnn/add.rst", "ttnn/ttnn/add_bw.rst", "ttnn/ttnn/addalpha.rst", "ttnn/ttnn/addalpha_bw.rst", "ttnn/ttnn/addcdiv.rst", "ttnn/ttnn/addcdiv_bw.rst", "ttnn/ttnn/addcmul.rst", "ttnn/ttnn/addcmul_bw.rst", "ttnn/ttnn/angle.rst", "ttnn/ttnn/angle_bw.rst", "ttnn/ttnn/arange.rst", "ttnn/ttnn/argmax.rst", "ttnn/ttnn/as_tensor.rst", "ttnn/ttnn/asin.rst", "ttnn/ttnn/asin_bw.rst", "ttnn/ttnn/asinh.rst", "ttnn/ttnn/asinh_bw.rst", "ttnn/ttnn/assign_bw.rst", "ttnn/ttnn/atan.rst", "ttnn/ttnn/atan2.rst", "ttnn/ttnn/atan2_bw.rst", "ttnn/ttnn/atan_bw.rst", "ttnn/ttnn/atanh.rst", "ttnn/ttnn/atanh_bw.rst", "ttnn/ttnn/bias_gelu_bw.rst", "ttnn/ttnn/bitwise_and.rst", "ttnn/ttnn/bitwise_left_shift.rst", "ttnn/ttnn/bitwise_not.rst", "ttnn/ttnn/bitwise_or.rst", "ttnn/ttnn/bitwise_right_shift.rst", "ttnn/ttnn/bitwise_xor.rst", "ttnn/ttnn/cbrt.rst", "ttnn/ttnn/ceil.rst", "ttnn/ttnn/ceil_bw.rst", "ttnn/ttnn/celu.rst", "ttnn/ttnn/celu_bw.rst", "ttnn/ttnn/clamp.rst", "ttnn/ttnn/clamp_bw.rst", "ttnn/ttnn/clip.rst", "ttnn/ttnn/clone.rst", "ttnn/ttnn/close_device.rst", "ttnn/ttnn/concat.rst", "ttnn/ttnn/concat_bw.rst", "ttnn/ttnn/conj.rst", "ttnn/ttnn/conj_bw.rst", "ttnn/ttnn/cos.rst", "ttnn/ttnn/cos_bw.rst", "ttnn/ttnn/cosh.rst", "ttnn/ttnn/cosh_bw.rst", "ttnn/ttnn/create_sharded_memory_config.rst", "ttnn/ttnn/deallocate.rst", "ttnn/ttnn/deg2rad.rst", "ttnn/ttnn/deg2rad_bw.rst", "ttnn/ttnn/digamma.rst", "ttnn/ttnn/digamma_bw.rst", "ttnn/ttnn/div.rst", "ttnn/ttnn/div_bw.rst", "ttnn/ttnn/div_no_nan.rst", "ttnn/ttnn/div_no_nan_bw.rst", "ttnn/ttnn/downsample.rst", "ttnn/ttnn/dump_tensor.rst", "ttnn/ttnn/elu.rst", "ttnn/ttnn/elu_bw.rst", "ttnn/ttnn/embedding.rst", "ttnn/ttnn/embedding_bw.rst", "ttnn/ttnn/empty.rst", "ttnn/ttnn/eq.rst", "ttnn/ttnn/eq_.rst", "ttnn/ttnn/eqz.rst", "ttnn/ttnn/erf.rst", "ttnn/ttnn/erf_bw.rst", "ttnn/ttnn/erfc.rst", "ttnn/ttnn/erfc_bw.rst", "ttnn/ttnn/erfinv.rst", "ttnn/ttnn/erfinv_bw.rst", "ttnn/ttnn/exp.rst", "ttnn/ttnn/exp2.rst", "ttnn/ttnn/exp2_bw.rst", "ttnn/ttnn/exp_bw.rst", "ttnn/ttnn/expm1.rst", "ttnn/ttnn/expm1_bw.rst", "ttnn/ttnn/fill_bw.rst", "ttnn/ttnn/fill_ones_rm.rst", "ttnn/ttnn/fill_rm.rst", "ttnn/ttnn/fill_zero_bw.rst", "ttnn/ttnn/floor.rst", "ttnn/ttnn/floor_bw.rst", "ttnn/ttnn/floor_div.rst", "ttnn/ttnn/fmod.rst", "ttnn/ttnn/fmod_bw.rst", "ttnn/ttnn/format_input_tensor.rst", "ttnn/ttnn/format_output_tensor.rst", "ttnn/ttnn/frac.rst", "ttnn/ttnn/frac_bw.rst", "ttnn/ttnn/from_device.rst", "ttnn/ttnn/from_torch.rst", "ttnn/ttnn/full.rst", "ttnn/ttnn/full_like.rst", "ttnn/ttnn/ge.rst", "ttnn/ttnn/ge_.rst", "ttnn/ttnn/geglu.rst", "ttnn/ttnn/gelu.rst", "ttnn/ttnn/gelu_bw.rst", "ttnn/ttnn/gez.rst", "ttnn/ttnn/global_avg_pool2d.rst", "ttnn/ttnn/glu.rst", "ttnn/ttnn/group_norm.rst", "ttnn/ttnn/gt.rst", "ttnn/ttnn/gt_.rst", "ttnn/ttnn/gtz.rst", "ttnn/ttnn/hardshrink.rst", "ttnn/ttnn/hardshrink_bw.rst", "ttnn/ttnn/hardsigmoid.rst", "ttnn/ttnn/hardsigmoid_bw.rst", "ttnn/ttnn/hardswish.rst", "ttnn/ttnn/hardswish_bw.rst", "ttnn/ttnn/hardtanh.rst", "ttnn/ttnn/hardtanh_bw.rst", "ttnn/ttnn/heaviside.rst", "ttnn/ttnn/hypot.rst", "ttnn/ttnn/hypot_bw.rst", "ttnn/ttnn/i0.rst", "ttnn/ttnn/i0_bw.rst", "ttnn/ttnn/identity.rst", "ttnn/ttnn/imag.rst", "ttnn/ttnn/imag_bw.rst", "ttnn/ttnn/indexed_fill.rst", "ttnn/ttnn/is_imag.rst", "ttnn/ttnn/is_real.rst", "ttnn/ttnn/isclose.rst", "ttnn/ttnn/isfinite.rst", "ttnn/ttnn/isinf.rst", "ttnn/ttnn/isnan.rst", "ttnn/ttnn/isneginf.rst", "ttnn/ttnn/isposinf.rst", "ttnn/ttnn/kv_cache/fill_cache_for_user_.rst", "ttnn/ttnn/kv_cache/update_cache_for_token_.rst", "ttnn/ttnn/l1_loss.rst", "ttnn/ttnn/layer_norm.rst", "ttnn/ttnn/ldexp.rst", "ttnn/ttnn/ldexp_bw.rst", "ttnn/ttnn/le.rst", "ttnn/ttnn/le_.rst", "ttnn/ttnn/leaky_relu.rst", "ttnn/ttnn/leaky_relu_bw.rst", "ttnn/ttnn/lerp.rst", "ttnn/ttnn/lerp_bw.rst", "ttnn/ttnn/lez.rst", "ttnn/ttnn/lgamma.rst", "ttnn/ttnn/lgamma_bw.rst", "ttnn/ttnn/linear.rst", "ttnn/ttnn/load_tensor.rst", "ttnn/ttnn/log.rst", "ttnn/ttnn/log10.rst", "ttnn/ttnn/log10_bw.rst", "ttnn/ttnn/log1p.rst", "ttnn/ttnn/log1p_bw.rst", "ttnn/ttnn/log2.rst", "ttnn/ttnn/log2_bw.rst", "ttnn/ttnn/log_bw.rst", "ttnn/ttnn/log_sigmoid.rst", "ttnn/ttnn/log_sigmoid_bw.rst", "ttnn/ttnn/logaddexp.rst", "ttnn/ttnn/logaddexp2.rst", "ttnn/ttnn/logaddexp2_bw.rst", "ttnn/ttnn/logaddexp_bw.rst", "ttnn/ttnn/logical_and.rst", "ttnn/ttnn/logical_and_.rst", "ttnn/ttnn/logical_not.rst", "ttnn/ttnn/logical_not_.rst", "ttnn/ttnn/logical_or.rst", "ttnn/ttnn/logical_or_.rst", "ttnn/ttnn/logical_xor.rst", "ttnn/ttnn/logical_xor_.rst", "ttnn/ttnn/logit.rst", "ttnn/ttnn/logit_bw.rst", "ttnn/ttnn/logiteps_bw.rst", "ttnn/ttnn/lt.rst", "ttnn/ttnn/lt_.rst", "ttnn/ttnn/ltz.rst", "ttnn/ttnn/mac.rst", "ttnn/ttnn/manage_device.rst", "ttnn/ttnn/matmul.rst", "ttnn/ttnn/max.rst", "ttnn/ttnn/max_bw.rst", "ttnn/ttnn/max_pool2d.rst", "ttnn/ttnn/maximum.rst", "ttnn/ttnn/mean.rst", "ttnn/ttnn/min.rst", "ttnn/ttnn/min_bw.rst", "ttnn/ttnn/minimum.rst", "ttnn/ttnn/mish.rst", "ttnn/ttnn/model_preprocessing/preprocess_model.rst", "ttnn/ttnn/model_preprocessing/preprocess_model_parameters.rst", "ttnn/ttnn/mse_loss.rst", "ttnn/ttnn/mul_bw.rst", "ttnn/ttnn/multigammaln.rst", "ttnn/ttnn/multigammaln_bw.rst", "ttnn/ttnn/multiply.rst", "ttnn/ttnn/ne.rst", "ttnn/ttnn/ne_.rst", "ttnn/ttnn/neg.rst", "ttnn/ttnn/neg_bw.rst", "ttnn/ttnn/nextafter.rst", "ttnn/ttnn/nez.rst", "ttnn/ttnn/nonzero.rst", "ttnn/ttnn/normalize_global.rst", "ttnn/ttnn/normalize_hw.rst", "ttnn/ttnn/ones.rst", "ttnn/ttnn/ones_like.rst", "ttnn/ttnn/open_device.rst", "ttnn/ttnn/outer.rst", "ttnn/ttnn/pad.rst", "ttnn/ttnn/pad_to_tile_shape.rst", "ttnn/ttnn/permute.rst", "ttnn/ttnn/polar.rst", "ttnn/ttnn/polar_bw.rst", "ttnn/ttnn/polygamma.rst", "ttnn/ttnn/polygamma_bw.rst", "ttnn/ttnn/polyval.rst", "ttnn/ttnn/pow.rst", "ttnn/ttnn/pow_bw.rst", "ttnn/ttnn/prelu.rst", "ttnn/ttnn/prod.rst", "ttnn/ttnn/prod_bw.rst", "ttnn/ttnn/rad2deg.rst", "ttnn/ttnn/rad2deg_bw.rst", "ttnn/ttnn/rdiv.rst", "ttnn/ttnn/rdiv_bw.rst", "ttnn/ttnn/real.rst", "ttnn/ttnn/real_bw.rst", "ttnn/ttnn/reallocate.rst", "ttnn/ttnn/reciprocal.rst", "ttnn/ttnn/reciprocal_bw.rst", "ttnn/ttnn/register_post_operation_hook.rst", "ttnn/ttnn/register_pre_operation_hook.rst", "ttnn/ttnn/reglu.rst", "ttnn/ttnn/relu.rst", "ttnn/ttnn/relu6.rst", "ttnn/ttnn/relu6_bw.rst", "ttnn/ttnn/relu_bw.rst", "ttnn/ttnn/relu_max.rst", "ttnn/ttnn/relu_min.rst", "ttnn/ttnn/remainder.rst", "ttnn/ttnn/remainder_bw.rst", "ttnn/ttnn/repeat.rst", "ttnn/ttnn/repeat_bw.rst", "ttnn/ttnn/repeat_interleave.rst", "ttnn/ttnn/reshape.rst", "ttnn/ttnn/rms_norm.rst", "ttnn/ttnn/round.rst", "ttnn/ttnn/round_bw.rst", "ttnn/ttnn/rpow.rst", "ttnn/ttnn/rpow_bw.rst", "ttnn/ttnn/rsqrt.rst", "ttnn/ttnn/rsqrt_bw.rst", "ttnn/ttnn/rsub.rst", "ttnn/ttnn/rsub_bw.rst", "ttnn/ttnn/scatter.rst", "ttnn/ttnn/selu.rst", "ttnn/ttnn/selu_bw.rst", "ttnn/ttnn/set_printoptions.rst", "ttnn/ttnn/sigmoid.rst", "ttnn/ttnn/sigmoid_accurate.rst", "ttnn/ttnn/sigmoid_bw.rst", "ttnn/ttnn/sign.rst", "ttnn/ttnn/sign_bw.rst", "ttnn/ttnn/signbit.rst", "ttnn/ttnn/silu.rst", "ttnn/ttnn/silu_bw.rst", "ttnn/ttnn/sin.rst", "ttnn/ttnn/sin_bw.rst", "ttnn/ttnn/sinh.rst", "ttnn/ttnn/sinh_bw.rst", "ttnn/ttnn/softmax.rst", "ttnn/ttnn/softplus.rst", "ttnn/ttnn/softplus_bw.rst", "ttnn/ttnn/softshrink.rst", "ttnn/ttnn/softshrink_bw.rst", "ttnn/ttnn/softsign.rst", "ttnn/ttnn/softsign_bw.rst", "ttnn/ttnn/split.rst", "ttnn/ttnn/sqrt.rst", "ttnn/ttnn/sqrt_bw.rst", "ttnn/ttnn/square.rst", "ttnn/ttnn/square_bw.rst", "ttnn/ttnn/squared_difference.rst", "ttnn/ttnn/squared_difference_bw.rst", "ttnn/ttnn/std.rst", "ttnn/ttnn/sub_bw.rst", "ttnn/ttnn/subalpha.rst", "ttnn/ttnn/subalpha_bw.rst", "ttnn/ttnn/subtract.rst", "ttnn/ttnn/sum.rst", "ttnn/ttnn/swiglu.rst", "ttnn/ttnn/swish.rst", "ttnn/ttnn/synchronize_device.rst", "ttnn/ttnn/tan.rst", "ttnn/ttnn/tan_bw.rst", "ttnn/ttnn/tanh.rst", "ttnn/ttnn/tanh_bw.rst", "ttnn/ttnn/tanhshrink.rst", "ttnn/ttnn/tanhshrink_bw.rst", "ttnn/ttnn/threshold.rst", "ttnn/ttnn/threshold_bw.rst", "ttnn/ttnn/tilize.rst", "ttnn/ttnn/tilize_with_val_padding.rst", "ttnn/ttnn/to_device.rst", "ttnn/ttnn/to_layout.rst", "ttnn/ttnn/to_memory_config.rst", "ttnn/ttnn/to_torch.rst", "ttnn/ttnn/topk.rst", "ttnn/ttnn/transformer/attention_softmax.rst", "ttnn/ttnn/transformer/attention_softmax_.rst", "ttnn/ttnn/transformer/concatenate_heads.rst", "ttnn/ttnn/transformer/rotary_embedding.rst", "ttnn/ttnn/transformer/sdpa.rst", "ttnn/ttnn/transformer/sdpa_decode.rst", "ttnn/ttnn/transformer/sdpa_decode_gqa.rst", "ttnn/ttnn/transformer/split_query_key_value_and_split_heads.rst", "ttnn/ttnn/tril.rst", "ttnn/ttnn/triu.rst", "ttnn/ttnn/trunc.rst", "ttnn/ttnn/trunc_bw.rst", "ttnn/ttnn/untilize.rst", "ttnn/ttnn/untilize_with_halo_v2.rst", "ttnn/ttnn/untilize_with_unpadding.rst", "ttnn/ttnn/upsample.rst", "ttnn/ttnn/var.rst", "ttnn/ttnn/where.rst", "ttnn/ttnn/where_bw.rst", "ttnn/ttnn/xlogy.rst", "ttnn/ttnn/xlogy_bw.rst", "ttnn/ttnn/zeros.rst", "ttnn/ttnn/zeros_like.rst", "ttnn/tutorials.rst", "ttnn/tutorials/graphing_torch_dit.rst", "ttnn/tutorials/matmul.rst", "ttnn/tutorials/multihead-attention.rst", "ttnn/tutorials/profiling.rst", "ttnn/tutorials/resnet-basic-block.rst", "ttnn/tutorials/tensor_and_add_operation.rst", "ttnn/tutorials/ttnn-tracer.rst", "ttnn/tutorials/ttnn_tutorials/001.ipynb", "ttnn/tutorials/ttnn_tutorials/002.ipynb", "ttnn/tutorials/ttnn_tutorials/003.ipynb", "ttnn/tutorials/ttnn_tutorials/004.ipynb", "ttnn/tutorials/ttnn_tutorials/005.ipynb", "ttnn/tutorials/ttnn_tutorials/006.ipynb", "ttnn/tutorials/ttnn_tutorials/007.ipynb", "ttnn/usage.rst", "ttnn_sweeps/index.rst"], "titles": ["Welcome to TT-NN documentation!", "Contributing as a developer", "Support", "Performance", "Getting Started", "What is ttnn?", "Adding New ttnn Operation", "APIs", "Converting torch Model to ttnn", "Building and Uplifting Demos", "Examples of Tensor and TT-LIB Use", "Dependencies", "Tensor", "TT-LIB", "Getting Started", "&lt;no title&gt;", "Onboarding New Functionality", "Profiling ttnn Operations", "Tensor", "ttnn.GetDefaultDevice", "ttnn.MaxPool2d", "ttnn.SetDefaultDevice", "ttnn.abs", "ttnn.abs_bw", "ttnn.acos", "ttnn.acos_bw", "ttnn.acosh", "ttnn.acosh_bw", "ttnn.add", "ttnn.add_bw", "ttnn.addalpha", "ttnn.addalpha_bw", "ttnn.addcdiv", "ttnn.addcdiv_bw", "ttnn.addcmul", "ttnn.addcmul_bw", "ttnn.angle", "ttnn.angle_bw", "ttnn.arange", "ttnn.argmax", "ttnn.as_tensor", "ttnn.asin", "ttnn.asin_bw", "ttnn.asinh", "ttnn.asinh_bw", "ttnn.assign_bw", "ttnn.atan", "ttnn.atan2", "ttnn.atan2_bw", "ttnn.atan_bw", "ttnn.atanh", "ttnn.atanh_bw", "ttnn.bias_gelu_bw", "ttnn.bitwise_and", "ttnn.bitwise_left_shift", "ttnn.bitwise_not", "ttnn.bitwise_or", "ttnn.bitwise_right_shift", "ttnn.bitwise_xor", "ttnn.cbrt", "ttnn.ceil", "ttnn.ceil_bw", "ttnn.celu", "ttnn.celu_bw", "ttnn.clamp", "ttnn.clamp_bw", "ttnn.clip", "ttnn.clone", "ttnn.close_device", "ttnn.concat", "ttnn.concat_bw", "ttnn.conj", "ttnn.conj_bw", "ttnn.cos", "ttnn.cos_bw", "ttnn.cosh", "ttnn.cosh_bw", "ttnn.create_sharded_memory_config", "ttnn.deallocate", "ttnn.deg2rad", "ttnn.deg2rad_bw", "ttnn.digamma", "ttnn.digamma_bw", "ttnn.div", "ttnn.div_bw", "ttnn.div_no_nan", "ttnn.div_no_nan_bw", "ttnn.downsample", "ttnn.dump_tensor", "ttnn.elu", "ttnn.elu_bw", "ttnn.embedding", "ttnn.embedding_bw", "ttnn.empty", "ttnn.eq", "ttnn.eq", "ttnn.eqz", "ttnn.erf", "ttnn.erf_bw", "ttnn.erfc", "ttnn.erfc_bw", "ttnn.erfinv", "ttnn.erfinv_bw", "ttnn.exp", "ttnn.exp2", "ttnn.exp2_bw", "ttnn.exp_bw", "ttnn.expm1", "ttnn.expm1_bw", "ttnn.fill_bw", "ttnn.fill_ones_rm", "ttnn.fill_rm", "ttnn.fill_zero_bw", "ttnn.floor", "ttnn.floor_bw", "ttnn.floor_div", "ttnn.fmod", "ttnn.fmod_bw", "ttnn.format_input_tensor", "ttnn.format_output_tensor", "ttnn.frac", "ttnn.frac_bw", "ttnn.from_device", "ttnn.from_torch", "ttnn.full", "ttnn.full_like", "ttnn.ge", "ttnn.ge", "ttnn.geglu", "ttnn.gelu", "ttnn.gelu_bw", "ttnn.gez", "ttnn.global_avg_pool2d", "ttnn.glu", "ttnn.group_norm", "ttnn.gt", "ttnn.gt", "ttnn.gtz", "ttnn.hardshrink", "ttnn.hardshrink_bw", "ttnn.hardsigmoid", "ttnn.hardsigmoid_bw", "ttnn.hardswish", "ttnn.hardswish_bw", "ttnn.hardtanh", "ttnn.hardtanh_bw", "ttnn.heaviside", "ttnn.hypot", "ttnn.hypot_bw", "ttnn.i0", "ttnn.i0_bw", "ttnn.identity", "ttnn.imag", "ttnn.imag_bw", "ttnn.indexed_fill", "ttnn.is_imag", "ttnn.is_real", "ttnn.isclose", "ttnn.isfinite", "ttnn.isinf", "ttnn.isnan", "ttnn.isneginf", "ttnn.isposinf", "ttnn.kv_cache.fill_cache_for_user_", "ttnn.kv_cache.update_cache_for_token_", "ttnn.l1_loss", "ttnn.layer_norm", "ttnn.ldexp", "ttnn.ldexp_bw", "ttnn.le", "ttnn.le", "ttnn.leaky_relu", "ttnn.leaky_relu_bw", "ttnn.lerp", "ttnn.lerp_bw", "ttnn.lez", "ttnn.lgamma", "ttnn.lgamma_bw", "ttnn.linear", "ttnn.load_tensor", "ttnn.log", "ttnn.log10", "ttnn.log10_bw", "ttnn.log1p", "ttnn.log1p_bw", "ttnn.log2", "ttnn.log2_bw", "ttnn.log_bw", "ttnn.log_sigmoid", "ttnn.log_sigmoid_bw", "ttnn.logaddexp", "ttnn.logaddexp2", "ttnn.logaddexp2_bw", "ttnn.logaddexp_bw", "ttnn.logical_and", "ttnn.logical_and", "ttnn.logical_not", "ttnn.logical_not", "ttnn.logical_or", "ttnn.logical_or", "ttnn.logical_xor", "ttnn.logical_xor", "ttnn.logit", "ttnn.logit_bw", "ttnn.logiteps_bw", "ttnn.lt", "ttnn.lt", "ttnn.ltz", "ttnn.mac", "ttnn.manage_device", "ttnn.matmul", "ttnn.max", "ttnn.max_bw", "ttnn.max_pool2d", "ttnn.maximum", "ttnn.mean", "ttnn.min", "ttnn.min_bw", "ttnn.minimum", "ttnn.mish", "ttnn.model_preprocessing.preprocess_model", "ttnn.model_preprocessing.preprocess_model_parameters", "ttnn.mse_loss", "ttnn.mul_bw", "ttnn.multigammaln", "ttnn.multigammaln_bw", "ttnn.multiply", "ttnn.ne", "ttnn.ne", "ttnn.neg", "ttnn.neg_bw", "ttnn.nextafter", "ttnn.nez", "ttnn.nonzero", "ttnn.normalize_global", "ttnn.normalize_hw", "ttnn.ones", "ttnn.ones_like", "ttnn.open_device", "ttnn.outer", "ttnn.pad", "ttnn.pad_to_tile_shape", "ttnn.permute", "ttnn.polar", "ttnn.polar_bw", "ttnn.polygamma", "ttnn.polygamma_bw", "ttnn.polyval", "ttnn.pow", "ttnn.pow_bw", "ttnn.prelu", "ttnn.prod", "ttnn.prod_bw", "ttnn.rad2deg", "ttnn.rad2deg_bw", "ttnn.rdiv", "ttnn.rdiv_bw", "ttnn.real", "ttnn.real_bw", "ttnn.reallocate", "ttnn.reciprocal", "ttnn.reciprocal_bw", "ttnn.register_post_operation_hook", "ttnn.register_pre_operation_hook", "ttnn.reglu", "ttnn.relu", "ttnn.relu6", "ttnn.relu6_bw", "ttnn.relu_bw", "ttnn.relu_max", "ttnn.relu_min", "ttnn.remainder", "ttnn.remainder_bw", "ttnn.repeat", "ttnn.repeat_bw", "ttnn.repeat_interleave", "ttnn.reshape", "ttnn.rms_norm", "ttnn.round", "ttnn.round_bw", "ttnn.rpow", "ttnn.rpow_bw", "ttnn.rsqrt", "ttnn.rsqrt_bw", "ttnn.rsub", "ttnn.rsub_bw", "ttnn.scatter", "ttnn.selu", "ttnn.selu_bw", "ttnn.set_printoptions", "ttnn.sigmoid", "ttnn.sigmoid_accurate", "ttnn.sigmoid_bw", "ttnn.sign", "ttnn.sign_bw", "ttnn.signbit", "ttnn.silu", "ttnn.silu_bw", "ttnn.sin", "ttnn.sin_bw", "ttnn.sinh", "ttnn.sinh_bw", "ttnn.softmax", "ttnn.softplus", "ttnn.softplus_bw", "ttnn.softshrink", "ttnn.softshrink_bw", "ttnn.softsign", "ttnn.softsign_bw", "ttnn.split", "ttnn.sqrt", "ttnn.sqrt_bw", "ttnn.square", "ttnn.square_bw", "ttnn.squared_difference", "ttnn.squared_difference_bw", "ttnn.std", "ttnn.sub_bw", "ttnn.subalpha", "ttnn.subalpha_bw", "ttnn.subtract", "ttnn.sum", "ttnn.swiglu", "ttnn.swish", "ttnn.synchronize_device", "ttnn.tan", "ttnn.tan_bw", "ttnn.tanh", "ttnn.tanh_bw", "ttnn.tanhshrink", "ttnn.tanhshrink_bw", "ttnn.threshold", "ttnn.threshold_bw", "ttnn.tilize", "ttnn.tilize_with_val_padding", "ttnn.to_device", "ttnn.to_layout", "ttnn.to_memory_config", "ttnn.to_torch", "ttnn.topk", "ttnn.transformer.attention_softmax", "ttnn.transformer.attention_softmax_", "ttnn.transformer.concatenate_heads", "ttnn.experimental.rotary_embedding", "ttnn.transformer.scaled_dot_product_attention", "ttnn.transformer.scaled_dot_product_attention_decode", "ttnn.transformer.scaled_dot_product_attention_decode_gqa", "ttnn.transformer.split_query_key_value_and_split_heads", "ttnn.tril", "ttnn.triu", "ttnn.trunc", "ttnn.trunc_bw", "ttnn.untilize", "ttnn.untilize_with_halo_v2", "ttnn.untilize_with_unpadding", "ttnn.upsample", "ttnn.var", "ttnn.where", "ttnn.where_bw", "ttnn.xlogy", "ttnn.xlogy_bw", "ttnn.zeros", "ttnn.zeros_like", "Tutorials", "Graphing Torch DiT_XL_2 With TTNN", "Matmul Operation", "Multi-Head Attention", "ttnn Profiling", "Resnet Basic Block", "Tensor and Add Operation", "ttnn Tracer", "Tensor and Add Operation", "Matrix Multiplication", "Multi-Head Attention", "Tracing ttnn operations and torch modules/functions", "Profiling ttnn operations", "Resnet Block", "Build a graph of a pytorch based model", "Using ttnn", "Placeholder title"], "terms": {"what": [0, 9, 17, 377], "i": [0, 3, 4, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 29, 31, 32, 34, 39, 48, 52, 67, 70, 77, 83, 84, 87, 91, 92, 94, 110, 111, 126, 132, 135, 148, 154, 155, 156, 165, 168, 169, 192, 193, 205, 210, 212, 217, 220, 221, 222, 223, 225, 227, 238, 240, 243, 245, 248, 251, 255, 262, 263, 275, 284, 285, 302, 309, 315, 317, 319, 324, 336, 338, 339, 340, 341, 343, 344, 345, 346, 347, 355, 360, 363, 364, 367, 368, 371, 372, 373, 374, 375, 376, 377, 378], "kei": [0, 8, 18, 347, 371, 373, 375], "featur": [0, 13, 16, 367, 378], "get": [0, 8, 10, 12, 15, 19, 342, 363, 368, 371, 372, 373, 374, 375], "start": [0, 6, 8, 12, 13, 15, 17, 38, 240, 373, 375], "1": [0, 3, 10, 12, 13, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 120, 121, 123, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 214, 215, 216, 217, 218, 219, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 239, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 336, 343, 344, 345, 346, 347, 348, 349, 350, 351, 356, 357, 358, 359, 360, 371, 372, 373, 374, 375, 376, 377], "instal": [0, 3, 9, 15, 17, 363, 375, 377], "build": [0, 10, 15, 363, 364, 367, 375], "2": [0, 11, 12, 15, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 70, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 120, 121, 123, 126, 127, 128, 129, 130, 131, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 212, 214, 217, 218, 219, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 234, 235, 239, 240, 242, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 258, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 317, 318, 319, 320, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 338, 344, 347, 348, 349, 350, 351, 357, 358, 359, 360, 363, 364, 371, 372, 373, 374, 375, 376], "explor": 0, "our": [0, 3, 4, 5, 9, 15, 16, 18, 371], "demo": [0, 4, 15, 17, 374], "3": [0, 3, 12, 13, 15, 18, 25, 29, 31, 32, 33, 34, 35, 39, 40, 42, 44, 48, 49, 51, 52, 61, 62, 69, 70, 76, 80, 82, 84, 91, 98, 100, 102, 103, 104, 105, 107, 108, 111, 112, 121, 123, 128, 132, 133, 138, 143, 148, 150, 168, 173, 174, 182, 184, 186, 189, 192, 193, 202, 208, 212, 217, 223, 230, 242, 247, 254, 264, 267, 273, 276, 283, 285, 288, 292, 294, 297, 299, 301, 302, 305, 308, 313, 315, 317, 319, 322, 326, 330, 332, 336, 338, 347, 351, 357, 358, 360, 371, 372, 373, 374, 375, 376, 377], "tutori": [0, 15, 364, 367, 368, 373, 377], "multi": [0, 18, 40, 87, 355, 363, 371], "head": [0, 340, 341, 342, 347, 363], "attent": [0, 340, 341, 344, 345, 346, 347, 363], "simpl": [0, 15, 375, 377], "4": [0, 12, 13, 15, 18, 25, 29, 31, 32, 33, 34, 35, 42, 44, 48, 49, 51, 52, 61, 62, 69, 70, 76, 80, 82, 84, 91, 98, 100, 102, 103, 104, 105, 107, 108, 112, 121, 138, 143, 148, 150, 168, 173, 174, 182, 184, 186, 189, 192, 193, 202, 208, 212, 217, 223, 225, 230, 247, 254, 267, 273, 276, 283, 285, 288, 292, 294, 297, 299, 301, 305, 308, 313, 315, 317, 319, 326, 330, 332, 351, 357, 358, 360, 371, 372, 373, 374, 375, 376, 377], "optim": [0, 363, 366, 371], "where": [0, 2, 3, 4, 7, 9, 13, 15, 17, 18, 67, 77, 210, 233, 240, 336, 358, 377], "go": [0, 15, 371], "from": [0, 2, 3, 4, 5, 6, 8, 9, 11, 12, 15, 17, 18, 40, 68, 92, 163, 164, 240, 284, 285, 320, 354, 363, 364, 368, 371, 373, 374, 375], "here": [0, 2, 6, 15, 372, 377, 379], "us": [0, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 40, 52, 69, 77, 91, 92, 97, 99, 103, 111, 129, 130, 151, 178, 210, 220, 221, 255, 256, 262, 263, 273, 274, 282, 302, 303, 333, 334, 336, 337, 338, 339, 343, 345, 346, 347, 352, 354, 363, 365, 366, 369, 370, 375, 377], "basic": [0, 14, 15, 363], "exampl": [0, 5, 9, 11, 13, 14, 15, 16, 17, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 112, 113, 114, 115, 116, 117, 120, 121, 123, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 212, 214, 217, 218, 219, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 239, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 310, 311, 312, 313, 314, 315, 317, 318, 319, 320, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 336, 338, 348, 349, 350, 351, 357, 358, 359, 360, 371, 377], "convert": [0, 5, 6, 10, 11, 40, 67, 123, 220, 221, 337, 338, 363, 366, 369, 372], "torch": [0, 5, 6, 10, 12, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 112, 113, 114, 115, 116, 117, 120, 121, 123, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 212, 214, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 317, 318, 319, 320, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 336, 337, 338, 339, 347, 348, 349, 350, 351, 357, 358, 359, 360, 363, 365, 366, 368, 369, 370, 375, 377], "tensor": [0, 4, 5, 6, 8, 11, 17, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 237, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 345, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 362, 363, 365, 373, 374, 376, 377], "run": [0, 5, 6, 9, 11, 13, 14, 15, 16, 17, 220, 262, 263, 363, 366, 368, 371, 372, 375], "an": [0, 2, 3, 5, 6, 9, 12, 13, 14, 15, 16, 17, 18, 91, 109, 111, 112, 116, 132, 210, 243, 248, 271, 371, 373, 377], "oper": [0, 5, 9, 10, 11, 12, 14, 16, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 65, 67, 69, 70, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 112, 113, 114, 115, 116, 117, 120, 121, 127, 128, 129, 130, 131, 132, 133, 136, 137, 139, 141, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 157, 158, 159, 160, 161, 162, 168, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 192, 193, 196, 197, 200, 201, 203, 204, 206, 207, 208, 210, 212, 214, 217, 218, 219, 223, 224, 225, 228, 229, 230, 231, 232, 234, 235, 239, 244, 246, 247, 248, 249, 251, 252, 253, 254, 256, 257, 258, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 279, 281, 282, 283, 284, 285, 286, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 306, 307, 308, 310, 311, 312, 313, 315, 317, 318, 319, 322, 323, 324, 325, 326, 327, 328, 329, 330, 332, 336, 338, 350, 351, 357, 358, 359, 360, 363, 367, 370, 372, 373], "devic": [0, 5, 8, 9, 11, 12, 15, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 212, 214, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 310, 311, 312, 313, 314, 315, 317, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 348, 349, 350, 351, 352, 354, 357, 358, 359, 360, 361, 362, 363, 365, 366, 369, 374, 375, 376, 377], "__getitem__": 0, "slice": [0, 13], "enabl": [0, 3, 13, 15, 16, 363, 365, 366, 371, 374, 375, 376, 377], "program": [0, 3, 5, 6, 11, 17, 178, 210, 340, 341, 363, 365, 366, 371, 374, 375, 376], "cach": [0, 3, 5, 11, 17, 18, 40, 68, 163, 164, 220, 221, 343, 346, 363, 365, 366, 371, 374, 375, 376, 377], "5": [0, 12, 15, 77, 91, 138, 139, 140, 142, 276, 305, 306, 371, 372, 373, 374, 375, 376, 377], "debug": [0, 5, 6, 13, 16, 371, 372, 373, 374, 376], "intermedi": 0, "6": [0, 12, 18, 224, 371, 372, 373, 374, 375, 376, 377], "trace": [0, 5, 363, 368, 370, 377], "graph": [0, 5, 11, 220, 363, 368, 374], "7": [0, 12, 91, 371, 372, 373, 374, 375, 376, 377], "tt_lib": [0, 5, 10, 11, 110, 111], "8": [0, 12, 15, 17, 18, 77, 91, 371, 372, 373, 374, 375, 376, 377], "log": [0, 7, 11, 17, 189, 372, 374, 375], "9": [0, 12, 13, 15, 75, 91, 371, 372, 373, 374, 375, 376, 377], "support": [0, 1, 5, 10, 12, 13, 15, 18, 25, 28, 29, 31, 32, 33, 34, 35, 39, 42, 44, 45, 48, 49, 51, 52, 53, 54, 55, 56, 57, 58, 61, 62, 70, 75, 76, 77, 80, 81, 82, 84, 94, 98, 100, 102, 103, 104, 105, 107, 108, 112, 116, 117, 121, 126, 128, 133, 135, 138, 143, 148, 150, 165, 167, 168, 169, 173, 174, 176, 182, 183, 184, 186, 189, 190, 191, 192, 193, 194, 196, 198, 202, 205, 208, 212, 217, 219, 222, 223, 224, 226, 227, 230, 245, 254, 255, 264, 267, 271, 272, 278, 283, 285, 288, 292, 294, 297, 299, 300, 301, 305, 308, 313, 314, 315, 317, 319, 320, 322, 326, 330, 332, 344, 345, 350, 351, 357, 358, 360, 371, 376], "python": [0, 4, 13, 15, 16, 17, 375, 377], "10": [0, 10, 12, 15, 91, 132, 178, 210, 245, 336, 337, 371, 372, 373, 374, 375, 377], "chang": [0, 13, 67, 333, 334, 352, 354, 371, 375], "string": [0, 13, 52, 84, 130, 220, 221, 255, 256, 274], "represent": [0, 13, 303], "11": [0, 371, 372, 373, 374, 375, 376], "visual": [0, 5, 374, 376, 377], "web": 0, "browser": [0, 363], "12": [0, 8, 15, 134, 166, 277, 371, 372, 373, 375, 376, 377], "regist": [0, 6, 262, 263], "pre": [0, 6, 15, 263, 363, 364, 366, 375], "post": [0, 17, 262], "hook": [0, 262, 263, 375], "13": [0, 371, 372, 373, 375, 377], "queri": [0, 13, 347, 373], "all": [0, 5, 6, 8, 9, 12, 13, 15, 16, 17, 18, 39, 132, 220, 221, 251, 252, 324, 346, 371, 373, 375], "14": [0, 3, 18, 371, 372, 373, 375], "fall": 0, "back": [0, 6, 10, 17, 342, 371], "15": [0, 373, 375], "captur": [0, 377], "c": [0, 13, 17, 18, 87, 110, 111, 355, 371, 372, 373, 374, 375, 376], "function": [0, 8, 9, 10, 12, 13, 26, 28, 37, 40, 43, 50, 59, 62, 64, 66, 72, 75, 79, 81, 89, 94, 106, 120, 126, 128, 133, 135, 138, 140, 142, 144, 145, 146, 151, 153, 165, 167, 169, 171, 176, 178, 183, 190, 191, 194, 197, 198, 202, 205, 210, 219, 220, 221, 222, 224, 226, 227, 234, 235, 244, 245, 251, 253, 258, 264, 269, 270, 276, 278, 280, 287, 300, 303, 305, 307, 314, 320, 322, 323, 328, 329, 331, 335, 348, 349, 363, 370, 371], "buffer": [0, 6, 11, 12, 13, 17, 18], "alloc": [0, 6, 18, 371, 372, 373, 374, 375, 376], "etc": [0, 6, 12, 15], "shape": [0, 6, 8, 12, 13, 17, 69, 77, 93, 118, 119, 124, 132, 178, 210, 233, 236, 240, 241, 242, 273, 274, 275, 276, 334, 342, 347, 354, 361, 371, 372, 373, 376, 377], "layout": [0, 6, 8, 10, 12, 13, 17, 25, 28, 29, 31, 32, 33, 34, 35, 39, 40, 42, 44, 45, 48, 49, 51, 52, 61, 62, 67, 70, 76, 80, 82, 84, 91, 92, 93, 94, 95, 98, 100, 102, 103, 104, 105, 107, 108, 112, 117, 118, 119, 121, 123, 124, 125, 126, 127, 135, 136, 138, 143, 148, 150, 167, 168, 169, 170, 173, 174, 182, 184, 186, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 202, 205, 206, 208, 212, 217, 223, 226, 227, 228, 230, 233, 236, 237, 254, 267, 272, 275, 283, 285, 288, 292, 294, 297, 299, 301, 305, 308, 313, 314, 315, 317, 319, 320, 326, 330, 332, 333, 334, 336, 339, 351, 352, 354, 357, 358, 360, 361, 362, 363, 365, 369, 373, 376, 378], "data": [0, 8, 10, 12, 13, 17, 21, 28, 39, 40, 67, 87, 91, 92, 94, 110, 111, 123, 126, 132, 135, 167, 169, 178, 190, 191, 194, 198, 205, 210, 213, 226, 227, 255, 314, 320, 333, 334, 336, 337, 339, 352, 353, 354, 355, 363, 369, 375, 378], "type": [0, 6, 8, 10, 11, 12, 15, 17, 21, 28, 37, 39, 40, 52, 67, 72, 91, 92, 94, 110, 111, 123, 126, 130, 132, 135, 153, 167, 169, 178, 190, 191, 194, 198, 205, 210, 213, 226, 227, 244, 255, 258, 276, 314, 320, 333, 334, 336, 337, 339, 352, 354, 363, 369, 375, 377], "storag": [0, 11, 13, 363, 369], "shard": [0, 77, 178, 210, 337, 347, 353], "memori": [0, 8, 9, 12, 17, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 112, 113, 114, 115, 116, 117, 120, 121, 123, 126, 128, 129, 130, 131, 132, 133, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 200, 201, 202, 203, 204, 205, 207, 208, 210, 212, 213, 214, 217, 218, 219, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 233, 234, 235, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 317, 318, 319, 320, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 336, 339, 340, 341, 342, 343, 347, 348, 349, 350, 351, 352, 353, 354, 357, 358, 359, 360, 371, 373], "config": [0, 8, 15, 23, 25, 27, 28, 29, 31, 33, 35, 36, 37, 39, 42, 44, 45, 48, 49, 51, 52, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 74, 76, 80, 82, 84, 86, 90, 91, 92, 94, 98, 100, 102, 105, 106, 108, 109, 112, 114, 117, 121, 126, 130, 135, 138, 139, 140, 141, 142, 143, 144, 145, 148, 150, 152, 153, 154, 155, 156, 165, 167, 168, 169, 172, 174, 177, 182, 184, 186, 187, 189, 190, 191, 192, 193, 194, 198, 202, 203, 204, 205, 212, 213, 217, 222, 223, 225, 226, 227, 230, 233, 240, 242, 243, 244, 245, 246, 249, 252, 254, 255, 256, 257, 258, 261, 267, 268, 272, 274, 278, 279, 280, 281, 283, 285, 287, 288, 292, 294, 297, 299, 301, 304, 305, 306, 308, 309, 311, 313, 314, 315, 317, 319, 320, 326, 328, 330, 331, 332, 333, 334, 339, 340, 341, 342, 343, 347, 348, 349, 351, 352, 353, 354, 358, 360, 363, 365, 371, 373, 374, 375, 376, 378], "api": [0, 4, 5, 6, 8, 11, 14, 15, 16, 344, 373, 378], "rank": [0, 12, 18, 25, 29, 31, 32, 33, 34, 35, 42, 44, 45, 48, 49, 51, 52, 61, 62, 70, 76, 80, 82, 84, 98, 100, 102, 103, 104, 105, 107, 108, 112, 117, 121, 138, 143, 148, 150, 168, 173, 174, 182, 184, 186, 189, 192, 193, 202, 208, 212, 217, 223, 230, 254, 267, 272, 283, 285, 288, 292, 294, 297, 299, 301, 305, 308, 313, 315, 317, 319, 326, 330, 332, 338, 351, 357, 358, 360], "with_tile_pad": [0, 18], "open_devic": [0, 7, 91, 92, 336, 337, 371, 372, 373, 374, 378], "close_devic": [0, 7, 371, 372, 373, 374, 376, 378], "manage_devic": [0, 7, 378], "synchronize_devic": [0, 7], "setdefaultdevic": [0, 7, 10, 13], "getdefaultdevic": [0, 7], "format_input_tensor": [0, 7], "format_output_tensor": [0, 7], "pad_to_tile_shap": [0, 7], "create_sharded_memory_config": [0, 7, 18], "core": [0, 6, 8, 13, 17, 18, 77, 178, 210, 353, 372, 373, 375], "as_tensor": [0, 7, 371, 372, 373, 374, 376], "from_torch": [0, 6, 7, 8, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 112, 113, 114, 115, 116, 117, 120, 121, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 212, 214, 217, 218, 219, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 239, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 310, 311, 312, 313, 314, 315, 317, 318, 319, 320, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 336, 337, 338, 348, 349, 350, 351, 357, 358, 359, 360, 371, 372, 373, 374, 376, 378], "to_torch": [0, 6, 7, 8, 12, 371, 372, 373, 374, 376, 378], "to_devic": [0, 7, 23, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 42, 44, 45, 47, 48, 49, 51, 52, 61, 63, 65, 70, 71, 72, 74, 76, 80, 82, 83, 84, 85, 86, 90, 91, 94, 98, 100, 102, 105, 106, 108, 109, 112, 114, 115, 117, 121, 126, 130, 135, 139, 141, 143, 145, 147, 148, 150, 152, 153, 154, 155, 156, 157, 165, 167, 168, 169, 172, 173, 174, 177, 178, 182, 184, 186, 187, 189, 190, 191, 192, 193, 194, 198, 200, 201, 203, 204, 205, 208, 210, 212, 214, 217, 218, 222, 223, 225, 226, 227, 230, 231, 233, 239, 242, 243, 244, 246, 247, 249, 252, 254, 256, 257, 258, 261, 267, 268, 272, 274, 279, 281, 283, 285, 286, 288, 292, 294, 297, 299, 301, 302, 304, 306, 308, 311, 313, 314, 315, 317, 318, 319, 320, 326, 328, 330, 332, 336, 337, 351, 357, 358, 359, 360, 371, 372, 373, 374, 376], "from_devic": [0, 7, 371, 372, 373, 374, 375, 376], "to_layout": [0, 5, 7, 18, 371, 372, 373], "dump_tensor": [0, 7, 371, 372, 373, 374, 376], "load_tensor": [0, 7, 371, 372, 373, 374, 375, 376], "dealloc": [0, 7, 18, 259, 371, 372, 373, 374, 376, 378], "realloc": [0, 7, 371, 372, 373, 374, 376], "to_memory_config": [0, 7, 18, 376], "creation": [0, 12], "arang": [0, 7, 12, 371, 372, 373, 374], "empti": [0, 6, 7, 13, 210, 377, 378], "zero": [0, 7, 12, 13, 69, 83, 112, 233, 242, 302, 334, 374, 378], "zeros_lik": [0, 7], "ones": [0, 7, 13], "ones_lik": [0, 7], "full": [0, 5, 7, 9, 11, 13, 17, 378], "full_lik": [0, 7], "matrix": [0, 15, 18, 91, 178, 210, 363, 365, 371, 378], "multipl": [0, 6, 12, 13, 17, 67, 111, 210, 220, 334, 336, 363, 365], "matmul": [0, 7, 178, 363, 371, 372, 373, 374, 375], "linear": [0, 7, 8, 11, 13, 303, 371, 372, 373, 374], "pointwis": 0, "unari": [0, 6, 256], "ab": [0, 7, 23], "aco": [0, 7, 27], "logical_not": [0, 7], "logical_not_": [0, 197], "acosh": [0, 7, 25], "asin": [0, 7, 42], "asinh": [0, 7, 44], "atan": [0, 7, 49], "atan2": [0, 7, 48, 371, 372, 373, 374], "atanh": [0, 7, 51], "bitwise_and": [0, 7], "bitwise_or": [0, 7], "bitwise_xor": [0, 7], "bitwise_not": [0, 7, 11, 13], "bitwise_left_shift": [0, 7], "bitwise_right_shift": [0, 7], "cbrt": [0, 7], "celu": [0, 7, 63, 371, 372, 373, 374], "clamp": [0, 7, 65], "clip": [0, 7, 371, 372, 373, 374], "clone": [0, 7, 15, 151, 347, 363, 364, 371, 372, 373, 374], "co": [0, 7, 74], "cosh": [0, 7, 76], "deg2rad": [0, 7, 80], "digamma": [0, 7, 82], "elu": [0, 7, 90], "erf": [0, 7, 98], "erfc": [0, 7, 100], "erfinv": [0, 7, 102], "exp": [0, 7, 10, 13, 374, 378], "exp2": [0, 7, 105, 108], "expm1": [0, 7], "floor": [0, 7, 11, 13, 114, 256, 271], "ceil": [0, 7, 11, 13, 61], "geglu": [0, 7, 371, 372, 373, 374], "gelu": [0, 7, 8, 13, 128, 130], "glu": [0, 7, 371, 372, 373, 374], "hardshrink": [0, 7, 139, 172, 371, 372, 373, 374], "normalize_glob": [0, 7], "hardsigmoid": [0, 7, 141], "hardswish": [0, 7, 143], "hardtanh": [0, 7, 145], "heavisid": [0, 7], "hypot": [0, 7, 148, 371, 372, 373, 374], "i0": [0, 7, 150], "ident": [0, 7, 376], "isfinit": [0, 7], "isinf": [0, 7], "isnan": [0, 7], "isneginf": [0, 7], "isposinf": [0, 7], "leaky_relu": [0, 7, 172], "lerp": [0, 7, 174, 371, 372, 373, 374], "lgamma": [0, 7, 177], "log10": [0, 7, 182], "log1p": [0, 7, 184], "log2": [0, 7, 186], "log_sigmoid": [0, 7], "frac": [0, 7, 13, 121], "logit": [0, 7, 203, 371, 372, 373, 374], "mish": [0, 7], "multigammaln": [0, 7, 225], "neg": [0, 7, 13, 230], "normalize_hw": [0, 7], "prelu": [0, 7], "reglu": [0, 7, 371, 372, 373, 374], "relu": [0, 7, 10, 171, 264, 268, 269, 270, 303, 376], "relu_max": [0, 7], "relu_min": [0, 7], "relu6": [0, 7, 267], "remaind": [0, 7, 272], "rsqrt": [0, 7, 283], "rdiv": [0, 7, 256], "rsub": [0, 7], "selu": [0, 7, 288], "sigmoid": [0, 7, 13, 189, 292], "sigmoid_accur": [0, 7], "sign": [0, 7, 13, 294], "silu": [0, 7, 10, 11, 13, 297, 322, 378], "sin": [0, 7, 299], "sinh": [0, 7, 301], "softmax": [0, 7, 11, 13, 340, 341, 373], "softplu": [0, 7, 304], "softshrink": [0, 7, 306, 371, 372, 373, 374], "softsign": [0, 7, 308], "swish": [0, 7, 13], "tan": [0, 7, 326], "tanh": [0, 7, 52, 130, 328], "signbit": [0, 7], "polygamma": [0, 7, 246, 371, 372, 373, 374], "rad2deg": [0, 7, 254], "reciproc": [0, 7, 261], "round": [0, 7, 255, 256, 279], "sqrt": [0, 7, 13], "squar": [0, 7, 18, 222, 311, 313, 314, 340, 341], "swiglu": [0, 7, 371, 372, 373, 374], "tril": [0, 7], "triu": [0, 7], "tanhshrink": [0, 7, 330], "threshold": [0, 7, 303, 304, 332, 371, 372, 373, 374], "trunc": [0, 7, 11, 13, 116, 256], "mul_bw": [0, 7], "clamp_bw": [0, 7], "hardtanh_bw": [0, 7], "threshold_bw": [0, 7], "softplus_bw": [0, 7], "div_bw": [0, 7], "rdiv_bw": [0, 7], "bias_gelu_bw": [0, 7], "pow_bw": [0, 7], "exp_bw": [0, 7], "tanh_bw": [0, 7], "sqrt_bw": [0, 7], "assign_bw": [0, 7], "multigammaln_bw": [0, 7], "add_bw": [0, 7], "lgamma_bw": [0, 7], "fill_bw": [0, 7], "hardsigmoid_bw": [0, 7], "cos_bw": [0, 7], "acosh_bw": [0, 7], "acos_bw": [0, 7], "atan_bw": [0, 7], "rad2deg_bw": [0, 7], "sub_bw": [0, 7], "frac_bw": [0, 7], "trunc_bw": [0, 7], "log_sigmoid_bw": [0, 7], "fill_zero_bw": [0, 7], "i0_bw": [0, 7], "tan_bw": [0, 7], "sigmoid_bw": [0, 7], "rsqrt_bw": [0, 7], "neg_bw": [0, 7], "relu_bw": [0, 7], "logit_bw": [0, 7], "hardshrink_bw": [0, 7], "softshrink_bw": [0, 7], "leaky_relu_bw": [0, 7], "elu_bw": [0, 7], "celu_bw": [0, 7], "rpow_bw": [0, 7], "floor_bw": [0, 7], "round_bw": [0, 7], "log_bw": [0, 7], "relu6_bw": [0, 7], "abs_bw": [0, 7], "silu_bw": [0, 7], "selu_bw": [0, 7], "square_bw": [0, 7], "prod_bw": [0, 7], "hardswish_bw": [0, 7], "tanhshrink_bw": [0, 7], "atanh_bw": [0, 7], "asin_bw": [0, 7], "asinh_bw": [0, 7], "sin_bw": [0, 7], "sinh_bw": [0, 7], "log10_bw": [0, 7], "log1p_bw": [0, 7], "erfc_bw": [0, 7], "ceil_bw": [0, 7], "softsign_bw": [0, 7], "cosh_bw": [0, 7], "logiteps_bw": [0, 7], "log2_bw": [0, 7], "sign_bw": [0, 7], "fmod_bw": [0, 7], "remainder_bw": [0, 7], "div_no_nan_bw": [0, 7], "exp2_bw": [0, 7], "expm1_bw": [0, 7], "reciprocal_bw": [0, 7], "digamma_bw": [0, 7], "erfinv_bw": [0, 7], "erf_bw": [0, 7], "deg2rad_bw": [0, 7], "polygamma_bw": [0, 7], "gelu_bw": [0, 7], "repeat_bw": [0, 7], "real": [0, 7, 9, 12, 156, 243, 258], "imag": [0, 7, 9, 13, 17, 18, 155, 243, 377], "angl": [0, 7, 37], "is_imag": [0, 7], "is_real": [0, 7], "polar_bw": [0, 7], "imag_bw": [0, 7], "real_bw": [0, 7], "angle_bw": [0, 7], "conj_bw": [0, 7], "conj": [0, 7, 72], "polar": [0, 7, 244], "binari": [0, 13], "add": [0, 7, 9, 13, 16, 29, 30, 47, 84, 94, 126, 135, 147, 157, 167, 169, 190, 191, 194, 198, 200, 201, 205, 214, 218, 223, 226, 227, 231, 239, 240, 286, 314, 317, 318, 320, 340, 341, 359, 363, 373, 375, 376, 378], "addalpha": [0, 7, 31], "subalpha": [0, 7, 319], "multipli": [0, 7, 13, 178, 210, 223, 355, 363, 365, 378], "subtract": [0, 3, 7, 284, 285, 317, 347, 378], "div": [0, 7, 116, 271], "div_no_nan": [0, 7, 86], "floor_div": [0, 7], "fmod": [0, 7, 117], "logical_and": [0, 7], "logical_and_": [0, 195], "logical_or": [0, 7], "logical_or_": [0, 199], "logical_xor": [0, 7, 371, 372, 373, 374], "logical_xor_": [0, 201], "pow": [0, 7, 10], "rpow": [0, 7, 281], "ldexp": [0, 7, 168], "logaddexp": [0, 7, 193], "logaddexp2": [0, 7, 192], "xlogi": [0, 7, 360, 371, 372, 373, 374], "squared_differ": [0, 7, 315], "gtz": [0, 7], "ltz": [0, 7], "gez": [0, 7], "lez": [0, 7], "nez": [0, 7], "eqz": [0, 7], "gt": [0, 7, 371, 372, 373, 374, 375, 376, 377], "gt_": [0, 136], "lt": [0, 7, 371, 372, 373, 374, 375, 376, 377], "lt_": [0, 206], "ge": [0, 7], "ge_": [0, 127], "le": [0, 7], "le_": [0, 170], "eq": [0, 7], "eq_": [0, 95], "ne": [0, 7], "ne_": [0, 228], "isclos": [0, 7, 371, 372, 373, 374], "nextaft": [0, 7, 371, 372, 373, 374], "maximum": [0, 7, 13, 39, 65, 145, 212, 371, 372, 373, 374], "minimum": [0, 6, 7, 13, 18, 65, 145, 217, 371, 372, 373, 374], "outer": [0, 7], "polyv": [0, 7, 371, 372, 373, 374], "scatter": [0, 7], "atan2_bw": [0, 7], "embedding_bw": [0, 7], "addalpha_bw": [0, 7], "subalpha_bw": [0, 7], "xlogy_bw": [0, 7], "hypot_bw": [0, 7], "ldexp_bw": [0, 7], "logaddexp_bw": [0, 7], "logaddexp2_bw": [0, 7], "squared_difference_bw": [0, 7], "concat_bw": [0, 7], "rsub_bw": [0, 7], "min_bw": [0, 7], "max_bw": [0, 7], "lerp_bw": [0, 7], "ternari": 0, "addcdiv": [0, 7, 33, 371, 372, 373, 374], "addcmul": [0, 7, 35, 371, 372, 373, 374], "mac": [0, 7, 371, 372, 373, 374], "addcmul_bw": [0, 7], "addcdiv_bw": [0, 7], "where_bw": [0, 7], "loss": [0, 165, 222], "l1_loss": [0, 7, 371, 372, 373, 374], "mse_loss": [0, 7, 371, 372, 373, 374], "reduct": [0, 39, 165, 222], "max": [0, 7, 13, 65, 144, 145, 213, 269, 332, 353, 375, 377], "mean": [0, 7, 10, 13, 18, 165, 222], "min": [0, 7, 65, 144, 145, 269, 270, 332, 377], "std": [0, 6, 7, 12, 13, 25, 27, 31, 33, 35, 37, 39, 42, 44, 45, 48, 49, 51, 52, 61, 62, 63, 64, 65, 66, 70, 72, 74, 76, 80, 82, 86, 90, 98, 100, 102, 105, 106, 108, 109, 112, 114, 117, 121, 130, 138, 139, 140, 141, 142, 143, 144, 145, 148, 150, 153, 154, 168, 172, 174, 177, 182, 184, 186, 187, 189, 192, 193, 202, 203, 204, 212, 217, 225, 230, 233, 242, 244, 245, 246, 249, 252, 254, 255, 256, 258, 267, 268, 272, 274, 278, 279, 280, 281, 283, 285, 287, 288, 292, 294, 297, 299, 301, 304, 305, 306, 308, 311, 313, 315, 319, 326, 328, 330, 331, 332, 339, 348, 349, 351, 358, 360, 376], "sum": [0, 7, 13, 109, 165, 222], "var": [0, 7, 13], "argmax": [0, 7, 13], "prod": [0, 7, 77, 252], "topk": [0, 7], "movement": 0, "concat": [0, 7, 11, 13, 16, 70], "nonzero": [0, 7], "pad": [0, 7, 10, 11, 12, 13, 18, 67, 91, 111, 118, 213, 241, 334, 336, 353, 371, 376], "permut": [0, 7, 347, 373, 376], "reshap": [0, 7, 10, 11, 12, 13, 347, 371, 372, 373, 374, 375, 376], "repeat": [0, 5, 7, 11, 13, 274, 275], "repeat_interleav": [0, 7, 11, 13], "tiliz": [0, 7, 13, 40, 372, 373], "tilize_with_val_pad": [0, 7], "tilize_with_zero_pad": [0, 334], "fill_rm": [0, 7, 110], "fill_ones_rm": [0, 7], "until": [0, 7, 338, 353, 354], "untilize_with_unpad": [0, 7], "untilize_with_halo_v2": [0, 7], "indexed_fil": [0, 7], "normal": [0, 13, 151, 375, 377], "group_norm": [0, 7, 11, 13], "layer_norm": [0, 7, 11, 13], "rms_norm": [0, 7, 166], "transform": [0, 8, 178, 243, 373, 374, 375], "split_query_key_value_and_split_head": [0, 7, 373], "concatenate_head": [0, 7, 373], "attention_softmax": [0, 7], "attention_softmax_": [0, 7, 373], "experiment": [0, 4, 7, 9, 11, 375], "rotary_embed": [0, 7], "scaled_dot_product_attent": [0, 7], "scaled_dot_product_attention_decod": [0, 7], "scaled_dot_product_attention_decode_gqa": [0, 7], "embed": [0, 92, 343, 371], "pool": [0, 13, 132, 213], "global_avg_pool2d": [0, 7], "max_pool2d": [0, 7], "vision": 0, "upsampl": [0, 7, 13], "downsampl": [0, 7, 376], "kv": 0, "kv_cach": [0, 7], "fill_cache_for_user_": [0, 7], "update_cache_for_token_": [0, 7], "convers": [0, 10, 12, 254, 336, 371], "model_preprocess": [0, 7, 8, 373, 374, 375, 376], "preprocess_model": [0, 7, 374, 375, 376], "preprocess_model_paramet": [0, 7, 8, 374, 376], "report": [0, 9, 371, 372, 373, 374, 375, 376, 378], "set_printopt": [0, 7, 378], "register_pre_operation_hook": [0, 7, 378], "register_post_operation_hook": [0, 7, 378], "creat": [0, 4, 6, 10, 12, 13, 16, 18, 77, 363, 368, 369, 373, 377], "host": [0, 10, 12, 13, 17, 18, 40, 240, 324, 336, 363, 369, 372, 373, 374, 375, 376], "borrow": [0, 12, 18, 363, 369], "v": [0, 40, 111, 344, 345, 346, 363, 369], "own": [0, 12, 15, 18, 363, 369], "open": [0, 209, 238, 363, 369, 372, 373, 374, 375, 376, 377, 378], "initi": [0, 8, 10, 13, 220, 221, 363, 365, 366, 369, 374, 375, 376], "b": [0, 6, 116, 271, 275, 343, 344, 345, 346, 363, 365, 369], "random": [0, 10, 363, 365, 369], "valu": [0, 9, 10, 12, 13, 31, 32, 34, 39, 52, 53, 55, 56, 58, 62, 63, 64, 65, 66, 70, 71, 77, 81, 90, 109, 110, 111, 130, 132, 138, 139, 140, 142, 144, 145, 146, 154, 155, 156, 163, 164, 172, 176, 202, 204, 225, 240, 245, 249, 251, 252, 255, 256, 269, 270, 278, 284, 287, 303, 304, 305, 306, 319, 331, 332, 334, 339, 347, 348, 349, 353, 363, 365, 369, 373, 378], "inspect": [0, 363, 365, 369], "output": [0, 3, 6, 8, 9, 10, 11, 12, 17, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 119, 120, 121, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 212, 213, 214, 217, 218, 219, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 262, 264, 265, 266, 267, 268, 269, 270, 271, 272, 274, 275, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 317, 318, 319, 320, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 336, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 357, 358, 359, 360, 363, 365, 366, 369, 374, 378], "attribut": [0, 6, 8, 13, 17, 18, 363, 369, 375], "close": [0, 10, 68, 209, 363, 365, 366, 369, 374, 375, 376], "configur": [0, 3, 4, 9, 12, 22, 24, 26, 30, 32, 34, 40, 41, 43, 46, 47, 50, 53, 54, 55, 56, 57, 58, 59, 60, 69, 73, 75, 79, 81, 83, 85, 89, 91, 92, 96, 97, 99, 101, 103, 104, 107, 113, 115, 116, 120, 123, 128, 129, 131, 132, 133, 137, 146, 147, 149, 151, 157, 158, 159, 160, 161, 162, 165, 171, 173, 175, 176, 178, 180, 181, 183, 185, 188, 196, 197, 200, 201, 207, 208, 210, 214, 218, 219, 222, 224, 229, 231, 232, 234, 235, 239, 247, 248, 251, 253, 260, 264, 265, 266, 269, 270, 271, 273, 282, 284, 286, 290, 291, 293, 295, 296, 298, 300, 302, 303, 307, 310, 312, 318, 322, 323, 325, 327, 329, 336, 350, 357, 359, 363, 365, 366, 374], "result": [0, 3, 5, 10, 13, 17, 111, 178, 210, 255, 363, 365], "more": [0, 1, 6, 13, 14, 17, 18, 363, 365, 373, 375, 377], "perform": [0, 8, 9, 12, 13, 14, 16, 17, 23, 25, 26, 27, 29, 31, 33, 35, 36, 37, 42, 43, 44, 45, 48, 49, 50, 51, 52, 59, 61, 62, 63, 64, 65, 66, 70, 72, 74, 75, 76, 79, 80, 81, 82, 84, 86, 90, 95, 98, 100, 102, 105, 106, 108, 109, 112, 114, 116, 117, 120, 121, 127, 130, 132, 136, 138, 139, 140, 141, 142, 143, 144, 145, 148, 150, 151, 152, 153, 168, 170, 172, 174, 176, 177, 182, 183, 184, 186, 187, 189, 192, 193, 197, 202, 203, 204, 206, 212, 217, 219, 223, 224, 225, 228, 230, 234, 235, 240, 243, 244, 245, 246, 249, 251, 252, 253, 254, 255, 256, 257, 258, 261, 267, 268, 271, 272, 274, 278, 279, 280, 281, 283, 285, 287, 288, 292, 294, 297, 299, 300, 301, 304, 305, 306, 307, 308, 311, 313, 315, 317, 319, 323, 326, 328, 329, 330, 331, 332, 348, 349, 351, 358, 360, 363, 365, 373], "write": [0, 1, 3, 4, 6, 8, 18, 77, 178, 210, 363, 366, 375], "activ": [0, 4, 8, 15, 28, 29, 84, 94, 126, 135, 145, 167, 169, 178, 190, 191, 194, 198, 205, 210, 213, 223, 226, 227, 314, 317, 320, 363, 366, 376], "weight": [0, 8, 13, 91, 92, 134, 166, 178, 277, 363, 366, 374, 375, 376], "first": [0, 3, 8, 10, 12, 13, 15, 17, 128, 133, 178, 210, 233, 240, 264, 322, 363, 366, 372, 378], "iter": [0, 363, 366], "subsequ": [0, 363, 366, 372, 378], "version": [0, 15, 17, 210, 220, 221, 345, 346, 363, 366, 368, 371, 372, 374, 375], "process": [0, 17, 363, 366, 375], "paramet": [0, 5, 8, 13, 89, 146, 171, 220, 221, 363, 366, 368, 374], "check": [0, 2, 6, 9, 12, 14, 15, 363, 366, 375], "match": [0, 12, 13, 18, 220, 221, 355, 363, 366, 371, 372, 374, 375, 376], "origin": [0, 9, 16, 220, 221, 363, 366, 371], "implement": [0, 3, 5, 8, 13, 16, 17, 210, 344, 345, 347, 363, 366, 368], "tracer": [0, 5, 363, 374, 376, 377, 378], "modul": [0, 5, 6, 8, 13, 14, 220, 221, 363, 368, 370, 371, 372, 373, 377], "written": [0, 6, 163, 164, 363, 370, 373], "profil": [0, 3, 11, 151, 289, 363, 378], "resnet": [0, 14, 17, 363, 375], "block": [0, 13, 18, 77, 122, 210, 363], "torchvis": [0, 363, 368, 375, 377], "preprocess": [0, 6, 40, 220, 221, 363, 368], "displai": [0, 363, 364, 368], "pass": [0, 6, 8, 10, 13, 16, 111, 262, 263, 343, 347, 363, 368, 371, 374, 375], "constructor": [0, 12, 363, 368], "dit_xl_2": [0, 363, 377], "With": [0, 363], "pytorch": [0, 3, 5, 11, 13, 39, 210, 240, 309, 339, 344, 347, 363, 364, 375], "base": [0, 3, 4, 6, 13, 18, 77, 363, 364, 373], "librari": [0, 4, 5, 10, 11, 12, 363, 364], "http": [0, 14, 15, 363, 364, 375], "github": [0, 2, 14, 15, 363, 364], "com": [0, 14, 15, 363, 364], "facebookresearch": [0, 363, 364], "dit": [0, 363, 364], "git": [0, 15, 220, 221, 363, 364, 375], "download": [0, 15, 363, 364, 374, 375], "xl": [0, 363, 364], "sampl": [0, 13, 363, 364], "train": [0, 13, 363, 364], "onboard": 0, "new": [0, 9, 11, 17, 273, 371, 374], "step": [0, 9, 10, 15, 16, 38, 371, 377], "rewrit": 0, "switch": [0, 303], "ad": [0, 12, 13, 16, 178, 371], "faq": 0, "ar": [0, 3, 4, 8, 9, 10, 12, 13, 14, 15, 17, 18, 111, 210, 233, 262, 263, 324, 339, 346, 347, 355, 363, 364, 367, 368, 371, 372, 373, 378], "need": [0, 1, 2, 9, 10, 13, 15, 17, 18, 53, 54, 55, 56, 57, 58, 178, 210, 371, 372, 373, 378], "bind": [0, 375], "option": [0, 11, 12, 15, 17, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 43, 45, 46, 47, 48, 50, 53, 54, 55, 56, 57, 58, 59, 60, 65, 69, 70, 71, 72, 73, 74, 75, 79, 81, 83, 84, 85, 89, 91, 92, 94, 96, 97, 99, 101, 103, 104, 106, 107, 109, 113, 114, 115, 116, 117, 120, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 135, 137, 141, 146, 147, 148, 149, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 166, 167, 168, 169, 171, 173, 174, 175, 176, 177, 178, 180, 181, 183, 185, 187, 188, 190, 191, 192, 193, 194, 196, 197, 198, 200, 201, 203, 205, 207, 208, 210, 211, 212, 214, 215, 216, 217, 218, 219, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 242, 243, 244, 247, 248, 249, 251, 253, 255, 257, 258, 260, 264, 265, 266, 268, 269, 270, 271, 272, 277, 279, 282, 283, 284, 285, 286, 290, 291, 293, 295, 296, 297, 298, 300, 302, 303, 307, 309, 310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 325, 327, 328, 329, 335, 336, 337, 339, 340, 341, 343, 350, 353, 356, 357, 358, 359, 360, 361, 362, 377, 378], "golden": [0, 8, 378], "perf": [0, 371, 372, 373, 374, 375, 376], "header": [0, 3], "profile_thi": [0, 375], "descript": [0, 12, 13, 16, 21, 110, 111, 213], "depend": [0, 4, 5, 9, 15, 17, 18, 67, 210, 336, 363, 375], "lib": [0, 4, 11, 374, 375, 377], "overview": [0, 11], "infrastructur": [0, 5, 11], "member": [0, 2, 11, 12], "input": [0, 6, 9, 10, 11, 12, 17, 19, 21, 22, 23, 24, 25, 27, 39, 41, 42, 44, 46, 49, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 63, 65, 73, 74, 76, 80, 82, 83, 86, 87, 89, 90, 91, 92, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 110, 111, 112, 113, 114, 118, 121, 128, 129, 130, 131, 132, 133, 137, 139, 141, 143, 145, 146, 149, 150, 151, 154, 158, 159, 160, 161, 162, 163, 164, 165, 171, 172, 174, 175, 177, 178, 180, 181, 182, 184, 185, 186, 187, 188, 189, 196, 203, 204, 207, 210, 213, 222, 225, 229, 232, 233, 240, 242, 246, 248, 251, 252, 254, 255, 256, 259, 260, 261, 264, 265, 266, 267, 268, 269, 270, 273, 274, 279, 281, 282, 283, 284, 288, 290, 291, 292, 293, 294, 295, 296, 298, 299, 301, 302, 303, 304, 306, 308, 309, 310, 312, 313, 322, 325, 326, 327, 330, 332, 333, 334, 339, 340, 341, 342, 343, 347, 350, 351, 352, 353, 354, 355, 372, 373, 376, 377, 378], "fast": [0, 11, 97, 99, 103, 129, 282], "dispatch": [0, 11, 12, 17], "cpu": [0, 3, 10, 11, 12, 13, 17, 375, 377], "through": [0, 11, 15, 377], "primari": [0, 5, 11], "moreh_softmax": [0, 11, 13], "moreh_softmax_backward": [0, 11, 13], "moreh_softmin": [0, 11, 13], "moreh_softmin_backward": [0, 11, 13], "moreh_logsoftmax": [0, 11, 13], "moreh_logsoftmax_backward": [0, 11, 13], "moreh_groupnorm": [0, 11, 13], "moreh_groupnorm_backward": [0, 11, 13], "moreh_norm": [0, 11, 13], "moreh_norm_backward": [0, 11, 13], "enum": [0, 11], "bcastopmath": [0, 11, 13], "bcastopdim": [0, 11, 13], "fallback": [0, 10, 11, 16], "tensor_slic": [0, 11, 13], "chunk": [0, 11, 13, 344, 345, 346, 372, 377], "conv2d": [0, 11, 13, 371, 372, 373, 374, 376], "interpol": [0, 11, 13], "batchnorm2d": [0, 11, 13, 376], "groupnorm": [0, 11, 13], "layernorm": [0, 11, 13], "maxpool2d": [0, 11, 13], "adaptiveavgpool2d": [0, 11, 13], "unary_fmod": [0, 11, 13], "binary_fmod": [0, 11, 13], "unary_bitwise_or": [0, 11, 13], "unary_bitwise_and": [0, 11, 13], "unary_bitwise_xor": [0, 11, 13], "binary_bitwise_or": [0, 11, 13], "binary_bitwise_and": [0, 11, 13], "binary_bitwise_xor": [0, 11, 13], "unary_bitwise_left_shift": [0, 11, 13], "unary_bitwise_right_shift": [0, 11, 13], "binary_bitwise_left_shift": [0, 11, 13], "binary_bitwise_right_shift": [0, 11, 13], "torch_argmax": [0, 11, 13], "torch_argmin": [0, 11, 13], "fuse": [0, 8, 11, 373], "mini": [0, 11], "addandnorm": [0, 11, 13], "complex": [0, 11, 36, 37, 71, 72, 152, 153, 243, 244, 257, 258], "__init__": [0, 8, 11, 12, 376], "get_dtyp": [0, 6, 11, 12], "get_layout": [0, 11, 12], "get_legacy_shap": [0, 10, 11, 12], "pad_to_til": [0, 11, 12], "storage_typ": [0, 11, 12], "unpad": [0, 10, 11, 12, 13, 67, 119, 336, 354], "unpad_from_til": [0, 11, 12], "memoryconfig": [0, 11, 13, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 112, 113, 114, 115, 116, 117, 120, 121, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 166, 167, 168, 169, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 200, 201, 202, 203, 204, 205, 207, 208, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 274, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 339, 340, 341, 342, 343, 347, 348, 349, 350, 351, 352, 353, 354, 356, 357, 358, 359, 360, 361, 362], "between": [0, 5, 6, 11, 13, 15], "one": [0, 6, 11, 13, 15, 210], "op": [0, 6, 11, 13, 16, 17, 19, 39, 77, 92, 128, 133, 264, 302, 322, 344, 345, 346, 375, 378], "acceler": [0, 11, 12, 13, 333, 334, 352, 354, 372, 375], "odd": [0, 11], "size": [0, 11, 12, 13, 18, 87, 91, 92, 344, 345, 346, 347, 355, 371, 378], "last": [0, 11, 12, 13, 17, 18, 39, 67, 336, 339, 347], "dim": [0, 11, 12, 13, 18, 39, 69, 70, 77, 128, 133, 154, 211, 213, 215, 216, 242, 251, 252, 264, 275, 302, 309, 316, 321, 322, 339, 347, 356, 373, 377], "uplift": 0, "placehold": 0, "titl": 0, "prerequisit": 0, "next": [0, 13, 15, 18], "file": [0, 2, 4, 6, 9, 15, 17, 40, 375, 377, 378], "bug": 0, "propos": [0, 16], "request": [0, 16, 67, 336, 375, 377], "troubleshoot": [0, 9], "tip": 0, "commun": 0, "contribut": [0, 2, 14, 15], "develop": [0, 4, 14, 16, 17, 375], "index": [0, 6, 12, 15, 17, 163, 164, 339, 343, 345, 375, 377], "search": 0, "page": [0, 14, 15, 363], "If": [1, 2, 6, 12, 13, 14, 15, 16, 17, 39, 77, 178, 210, 220, 221, 238, 240, 251, 302, 324, 338, 339, 345, 346, 347, 374, 378], "you": [1, 2, 3, 4, 5, 6, 9, 10, 12, 13, 14, 15, 17, 303, 363, 374, 377, 378, 379], "would": [1, 15, 16, 17, 18, 151], "like": [1, 8, 10, 15, 18, 109, 112, 303, 371, 378], "thi": [1, 3, 4, 5, 6, 8, 9, 10, 12, 13, 15, 16, 17, 18, 19, 21, 39, 40, 92, 118, 119, 132, 151, 210, 240, 241, 269, 270, 303, 343, 344, 363, 364, 367, 368, 371, 372, 373, 377, 378], "project": [1, 2, 4, 14, 15], "pleas": [1, 2, 9, 13, 14, 15, 16, 363, 378], "review": [1, 14, 16], "standard": [1, 2, 9, 14, 270], "gain": 1, "access": [1, 2, 377], "repositori": 1, "read": [1, 13, 14, 18, 77, 353], "section": [1, 2, 9, 13, 18], "detail": [1, 6, 14, 377, 378], "contact": 1, "u": [1, 15, 16], "have": [2, 3, 4, 6, 9, 12, 13, 15, 17, 18, 39, 77, 255, 333, 334, 339, 352, 354, 363, 371, 377], "formal": 2, "permiss": 2, "cloud": 2, "issu": [2, 9, 15, 16, 17, 303, 371, 372, 373, 374, 375, 376], "can": [2, 3, 4, 5, 6, 8, 10, 12, 13, 14, 15, 17, 18, 52, 130, 210, 252, 256, 262, 263, 303, 363, 371, 372, 373, 374, 376, 377, 378], "out": [2, 6, 13, 15, 270, 338, 339, 371, 373, 376], "relev": [2, 9], "ever": 2, "hardwar": [2, 5, 6, 8, 9, 14, 18, 371, 378], "help": [2, 5, 16, 363], "we": [2, 3, 4, 5, 9, 10, 13, 15, 16, 18, 210, 364, 367, 368, 371, 372, 377, 378], "offici": 2, "discord": 2, "channel": [2, 13, 17, 87, 110, 111, 132, 355], "repres": [2, 12, 17, 18, 371], "both": [2, 8, 9, 12, 13, 17, 18, 196, 210, 371, 376], "tenstorr": [2, 5, 6, 8, 9, 14, 15, 363, 371, 372, 377, 378], "metal": [2, 5, 12, 14, 15, 363, 371, 372, 373, 374, 375, 376, 377], "join": [2, 375], "discuss": [2, 9], "board": [2, 15], "bounc": 2, "idea": [2, 9], "off": [2, 8, 269], "each": [2, 3, 5, 12, 13, 15, 17, 18, 111, 132, 240, 273, 275], "other": [2, 5, 6, 8, 9, 13, 14, 15, 18, 364, 367, 368, 378], "refer": [2, 3, 4, 5, 12, 16, 18, 225, 378], "code": [2, 6, 10, 13, 14, 15, 16, 17, 39, 240, 262, 263, 309, 339, 347, 371, 375, 378], "conduct": 2, "when": [2, 6, 9, 12, 13, 16, 18, 19, 21, 67, 77, 210, 220, 336, 343, 371, 373, 374, 376, 378], "interact": 2, "ensur": [3, 4, 9, 15], "tt": [3, 4, 5, 6, 11, 15, 21, 333, 334, 352, 354, 363, 364, 367, 368, 371, 372, 373, 374, 375, 376, 377], "metalium": [3, 4, 15], "sourc": [3, 4, 15, 163, 363, 367], "environ": [3, 4, 13, 14, 15, 373, 374, 375, 377, 378], "requir": [3, 6, 9, 12, 13, 15, 17, 21, 29, 31, 77, 84, 110, 111, 213, 220, 221, 223, 285, 317, 319, 367, 375, 377], "model": [3, 5, 9, 15, 16, 17, 220, 221, 363, 364, 366, 370, 371, 372, 375, 376], "follow": [3, 6, 8, 10, 12, 13, 14, 15, 16, 17, 18, 111, 128, 133, 210, 262, 263, 264, 322, 363, 377, 378], "instruct": [3, 4, 9, 14, 15, 363, 378], "readi": [3, 4, 9, 347], "come": [3, 14, 17], "typic": [3, 18, 132], "found": [3, 6, 8, 363, 375, 377], "under": [3, 4, 9, 10, 16, 17, 363, 378], "your_model": 3, "perf_model": 3, "py": [3, 6, 8, 9, 15, 17, 374, 375, 378], "To": [3, 10, 13, 16, 371, 372, 378], "pytest": [3, 4, 8, 9, 17, 375, 378], "test": [3, 4, 6, 8, 9, 16, 17, 373, 375, 378], "python_api_test": 3, "perf_your_model": 3, "csv": [3, 17, 375], "perf_your_model_d": 3, "contain": [3, 4, 12, 18, 91, 371], "tabl": [3, 375], "two": [3, 8, 12, 13, 18, 67, 128, 133, 210, 264, 322, 336, 347], "row": [3, 12, 17, 18, 111, 233, 353, 371, 372, 375], "set": [3, 4, 8, 10, 12, 13, 15, 17, 21, 110, 251, 342, 347, 363, 371, 374, 375, 376, 377, 378], "batch": [3, 12, 13, 17, 87, 110, 111, 154, 178, 210, 345, 346], "sec": 3, "second": [3, 12, 13, 15, 17, 128, 133, 178, 210, 240, 264, 322, 373, 375, 378], "compil": [3, 13, 372, 375, 378], "time": [3, 9, 13, 15, 17, 210, 220, 226, 273, 372, 373, 375, 378], "infer": [3, 9, 17, 375], "g": [3, 13, 17, 210], "throughput": 3, "inf": [3, 224, 344], "vit": 3, "patch16": 3, "30": [3, 375], "51": [3, 374], "16": [3, 6, 15, 18, 373, 375, 376, 377], "05": [3, 13, 377], "46": [3, 375], "0": [3, 6, 8, 10, 12, 13, 15, 17, 18, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 45, 47, 48, 54, 57, 70, 71, 81, 83, 84, 85, 91, 92, 94, 110, 111, 115, 117, 122, 123, 126, 135, 138, 139, 140, 142, 147, 148, 152, 154, 155, 156, 157, 165, 167, 168, 169, 172, 173, 174, 176, 190, 191, 192, 193, 194, 198, 200, 201, 202, 204, 205, 208, 212, 214, 217, 218, 222, 223, 226, 227, 231, 233, 239, 242, 243, 248, 251, 252, 255, 257, 269, 270, 272, 275, 278, 285, 286, 302, 303, 305, 306, 309, 314, 315, 317, 318, 319, 320, 333, 334, 336, 337, 338, 339, 344, 347, 348, 349, 352, 353, 354, 357, 358, 359, 360, 371, 372, 373, 374, 375, 376, 377, 378], "0623": 3, "29": [3, 375], "4960": 3, "includ": [3, 6, 9], "without": [3, 10, 12, 13], "ani": [3, 5, 9, 10, 13, 110, 111, 375], "abovement": 3, "grayskul": [3, 6, 8, 15, 40, 219, 278, 350, 363, 364, 367, 368, 373, 374, 378], "It": [3, 6, 12, 13, 15, 220, 221, 371], "sinc": [3, 10], "dure": 3, "do": [3, 9, 10, 15, 373], "pai": 3, "name": [3, 6, 8, 12, 13, 16, 17, 40, 220, 221, 344, 375, 376, 377, 378], "suggest": 3, "calcul": [3, 17, 77, 255, 284], "comput": [3, 5, 6, 13, 17, 132, 134, 166, 167, 178, 190, 191, 194, 195, 198, 199, 210, 251, 277, 302, 314, 340, 341, 345, 347, 371, 372], "": [3, 4, 5, 6, 9, 12, 13, 18, 220, 221, 344, 345, 346, 371, 372, 373, 377], "also": [3, 6, 9, 10, 12, 13, 14, 15, 17, 67, 377], "maintain": [3, 16], "script": [3, 4, 9, 15, 17, 375], "run_perform": [3, 9], "sh": [3, 4, 9, 15, 17, 375], "facilit": 3, "easi": [3, 371], "wai": [3, 5, 8, 13, 15, 220, 221, 309, 371], "attempt": [3, 13, 375], "fastest": 3, "command": [3, 17, 22, 24, 28, 29, 31, 39, 41, 45, 46, 53, 54, 55, 56, 57, 58, 60, 69, 73, 83, 84, 89, 91, 92, 94, 96, 97, 99, 101, 103, 104, 106, 107, 109, 113, 126, 128, 129, 130, 131, 133, 135, 137, 146, 149, 151, 154, 158, 159, 160, 161, 162, 165, 167, 169, 171, 175, 180, 181, 185, 188, 190, 191, 194, 196, 198, 205, 207, 222, 223, 226, 227, 229, 230, 232, 233, 240, 242, 248, 249, 255, 260, 264, 265, 266, 269, 270, 282, 283, 284, 285, 290, 291, 293, 295, 296, 297, 298, 303, 309, 310, 311, 312, 314, 317, 319, 320, 322, 324, 325, 327, 328, 333, 334, 339, 350, 352, 353, 354, 357, 358, 378], "execut": [3, 9, 10, 13, 17, 262, 263, 372, 373, 375, 378], "merg": [3, 16], "built": [4, 15, 375, 377], "now": [4, 14, 15, 18, 355, 371, 373], "root": [4, 311, 340, 341], "provid": [4, 5, 9, 12, 13, 15, 16, 17, 18, 39, 53, 54, 55, 56, 57, 58, 116, 220, 221, 255, 271, 302, 324, 334, 339, 371, 373, 378], "virtual": [4, 15], "which": [4, 5, 8, 12, 13, 17, 18, 77, 178, 210, 220, 221, 255, 284, 302, 344, 345, 346], "ll": 4, "work": [4, 8, 9, 363, 364, 367, 368, 378], "python_env": [4, 15, 374, 375, 377], "bin": [4, 15, 375], "python_env_dir": 4, "variabl": [4, 6, 13, 15, 374, 376, 378], "create_venv": [4, 15], "control": [4, 13, 371], "pythonpath": [4, 15, 377], "common": [4, 6, 9, 13, 15], "practic": 4, "export": [4, 13, 15, 378], "pwd": [4, 15], "folder": [4, 9, 14, 17, 375], "split": [4, 13, 18, 128, 133, 264, 322, 347], "them": [4, 13, 15, 17, 347, 371], "sub": [4, 5, 13], "In": [4, 6, 8, 10, 13, 17, 18, 67, 210, 336, 341, 371, 377], "find": [4, 15, 371, 372, 373, 374, 375, 376], "prepar": [4, 9, 375], "readm": [4, 9, 375, 377], "md": [4, 9], "give": [4, 17], "how": [4, 6, 9, 12, 13, 17, 18, 372, 373, 378], "progress": [4, 377], "yet": 4, "user": [4, 5, 6, 9, 14, 15, 16, 151, 220, 221, 334, 371, 372, 373, 374, 375, 376, 377], "mani": [4, 6, 8, 372, 377], "part": [4, 9, 13, 17, 373], "entir": [4, 132], "path_to_test_fil": 4, "test_in_fil": 4, "ttnn": [4, 7, 12, 13, 15, 16, 18, 363, 366, 368, 369, 372, 377], "friendli": [4, 5, 14], "top": [4, 12, 339, 363], "doc": [4, 6, 15, 375], "document": [4, 6, 9, 15, 16, 375], "interfac": [5, 13], "design": 5, "intuit": [5, 371], "familiar": [5, 15], "trust": 5, "valuabl": 5, "your": [5, 9, 10, 13, 15, 17, 363], "journei": 5, "take": [5, 6, 9, 12, 13, 14, 18, 262, 263, 342, 371], "advantag": 5, "n": [5, 13, 15, 17, 87, 110, 111, 210, 233, 344, 345, 355, 371, 372, 375, 377], "d": [5, 353, 375], "row_major_layout": [5, 18, 40, 67, 91, 123, 336, 371, 372, 373], "tile_layout": [5, 8, 18, 29, 31, 32, 34, 35, 48, 52, 67, 70, 84, 92, 148, 168, 173, 174, 192, 193, 208, 212, 217, 223, 275, 285, 315, 317, 319, 336, 339, 357, 360, 371, 372, 373, 376, 378], "stabl": 5, "The": [5, 6, 8, 9, 10, 12, 13, 14, 16, 17, 18, 87, 89, 91, 111, 132, 146, 163, 164, 171, 210, 262, 263, 269, 270, 273, 275, 339, 344, 345, 346, 355, 371, 372, 377, 378], "networkx": [5, 375, 377], "compat": [5, 15, 364, 367, 368], "some": [5, 6, 13, 378], "nn": [5, 6, 8, 12, 15, 220, 221, 363, 364, 367, 368, 376, 378], "object": [5, 8, 12, 16, 77, 377], "could": [5, 371, 372, 373, 374, 375, 376], "significantli": [5, 372], "speed": [5, 372], "up": [5, 13, 15, 17, 111, 363, 372, 377], "abil": [5, 13], "compar": [5, 6, 94, 126, 135, 205, 227, 372], "equival": [5, 6, 18, 39, 240, 309, 339, 347], "veri": [5, 17, 303, 371, 373], "meant": 6, "contributor": 6, "Not": [6, 8, 202, 228, 350, 378], "mai": [6, 8, 13, 15, 378], "wormhol": [6, 8, 15, 40, 363, 378], "A": [6, 9, 10, 12, 13, 15, 16, 18, 210, 303, 345, 346], "produc": [6, 9, 10, 13, 371, 372], "call": [6, 10, 12, 13, 16, 17, 18, 262, 263, 371, 373, 375, 378], "There": [6, 8, 12, 13, 18], "optiona": 6, "composit": 6, "struct": [6, 13], "specifi": [6, 8, 12, 13, 178, 220, 221, 240, 242, 251, 273, 334, 344, 345, 346, 372, 373], "simpli": [6, 12, 13, 67, 336, 371], "defin": [6, 12, 13, 16, 18, 40], "method": [6, 13, 15, 18, 375], "register_oper": 6, "exist": [6, 12, 238, 375, 377], "bind_registered_oper": 6, "auto": [6, 13], "attach": [6, 220, 221, 375], "attach_golden_funct": 6, "let": [6, 18, 371, 373], "just": [6, 15, 373, 377], "copi": [6, 12, 13, 67, 151, 371], "order": [6, 12, 13, 15, 17, 18, 77, 210, 285, 339, 363, 371, 373, 378], "directori": [6, 363, 377], "structur": [6, 8], "shown": [6, 15, 18], "below": [6, 9, 13, 17, 18, 363], "cpp": 6, "categori": 6, "operation_nam": 6, "_device_oper": 6, "hpp": 6, "program_factory_0": 6, "_program_factori": 6, "factori": 6, "But": [6, 10], "concret": 6, "example_device_oper": 6, "spdx": [6, 378], "filecopyrighttext": [6, 378], "2023": [6, 375, 377], "inc": [6, 378], "licens": [6, 378], "identifi": [6, 375, 378], "apach": [6, 378], "pragma": 6, "onc": [6, 372], "variant": 6, "device_oper": 6, "decor": [6, 16, 371, 372, 373, 374, 375, 376], "namespac": [6, 377], "exampledeviceoper": 6, "store": [6, 12, 17, 18, 371], "aren": [6, 19, 21], "t": [6, 10, 12, 13, 17, 19, 21, 151, 220, 221, 371, 373, 375], "operation_attributes_t": 6, "bool": [6, 13, 29, 31, 77, 78, 84, 97, 99, 103, 122, 129, 211, 215, 216, 220, 221, 223, 251, 252, 282, 285, 316, 317, 319, 321, 333, 334, 339, 340, 341, 346, 352, 353, 354, 356], "int": [6, 12, 13, 28, 29, 38, 39, 70, 77, 84, 91, 92, 93, 94, 110, 111, 122, 124, 125, 126, 134, 135, 154, 163, 164, 167, 169, 190, 191, 194, 198, 205, 209, 211, 215, 216, 223, 226, 227, 233, 238, 240, 242, 245, 248, 251, 275, 276, 302, 309, 314, 316, 317, 320, 321, 324, 333, 334, 338, 339, 340, 341, 343, 347, 352, 353, 354, 356, 376, 377], "some_other_attribut": 6, "argument": [6, 12, 13, 21, 110, 111, 178, 210, 213, 262, 263, 371, 374, 376], "don": [6, 13, 371], "thei": [6, 9, 13, 372, 373], "tensor_args_t": 6, "onli": [6, 8, 9, 12, 13, 17, 18, 29, 31, 32, 34, 35, 39, 48, 52, 53, 54, 55, 56, 57, 58, 60, 70, 77, 83, 84, 92, 111, 113, 116, 148, 165, 168, 173, 174, 192, 193, 208, 210, 212, 217, 222, 223, 245, 255, 271, 285, 315, 317, 319, 324, 344, 345, 347, 357, 360, 363, 364, 367, 368, 376, 378], "const": [6, 13], "input_tensor": [6, 13, 22, 23, 24, 25, 26, 27, 36, 37, 39, 41, 42, 43, 44, 46, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 84, 86, 87, 89, 90, 91, 92, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 112, 113, 114, 120, 121, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 149, 150, 151, 152, 153, 155, 156, 158, 159, 160, 161, 162, 166, 170, 171, 172, 175, 176, 177, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 196, 197, 202, 203, 204, 206, 207, 211, 215, 216, 219, 223, 224, 225, 228, 229, 230, 232, 233, 234, 235, 240, 242, 243, 244, 245, 246, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 264, 265, 266, 267, 268, 269, 270, 273, 274, 275, 277, 278, 279, 280, 281, 282, 283, 284, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 316, 321, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 339, 340, 341, 342, 343, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 374, 376, 378], "howev": [6, 13, 15, 371], "show": [6, 12, 18, 372, 373], "els": [6, 12, 83, 374, 376, 377], "done": [6, 9, 12, 17, 371, 375], "io_tensor": 6, "optional_output_tensor": [6, 92], "vector": [6, 12, 13, 25, 27, 31, 33, 35, 37, 42, 44, 45, 48, 49, 51, 52, 61, 62, 63, 64, 65, 66, 70, 72, 74, 76, 80, 82, 86, 90, 98, 100, 102, 105, 106, 108, 109, 112, 114, 117, 121, 130, 138, 139, 140, 141, 142, 143, 144, 145, 148, 150, 153, 168, 172, 174, 177, 182, 184, 186, 187, 189, 192, 193, 202, 203, 204, 210, 212, 217, 225, 230, 244, 245, 246, 247, 249, 252, 254, 255, 256, 258, 267, 268, 272, 274, 278, 279, 280, 281, 283, 285, 287, 288, 292, 294, 297, 299, 301, 304, 305, 306, 308, 311, 313, 315, 319, 326, 328, 330, 331, 332, 348, 349, 351, 358, 360], "vector_of_tensor": 6, "tupl": [6, 13, 77, 211, 215, 216, 240, 316, 321, 339, 347, 355, 356], "tuple_of_tensor": 6, "vector_of_optional_tensor": 6, "some_crazy_tuple_of_tensor": 6, "return": [6, 8, 10, 12, 13, 28, 39, 67, 71, 92, 94, 95, 109, 112, 126, 127, 132, 135, 136, 151, 155, 156, 165, 167, 169, 170, 178, 190, 191, 194, 195, 198, 199, 205, 206, 210, 222, 226, 227, 228, 233, 238, 240, 241, 259, 262, 263, 273, 284, 309, 314, 320, 336, 339, 342, 347, 373, 374, 375, 376, 377], "singl": [6, 8, 16, 17, 18, 345, 371], "shape_return_value_t": 6, "tensor_return_value_t": 6, "note": [6, 10, 12, 13, 15, 17, 18, 29, 31, 32, 34, 35, 48, 52, 67, 70, 84, 148, 168, 173, 174, 192, 193, 208, 210, 212, 217, 223, 285, 315, 317, 319, 357, 360, 371, 373, 377, 378], "should": [6, 9, 12, 14, 16, 17, 151, 233, 240, 336, 371, 372, 373, 374, 376], "same": [6, 13, 17, 18, 28, 67, 94, 95, 110, 126, 127, 135, 136, 151, 167, 169, 170, 190, 191, 194, 195, 198, 199, 205, 206, 210, 220, 221, 226, 227, 228, 233, 314, 320, 336, 339, 344, 371, 373], "pattern": [6, 16, 220], "e": [6, 13, 15, 17, 210, 378], "singlecor": 6, "share": [6, 12, 346], "override_runtime_argu": 6, "shared_variables_t": 6, "kernelhandl": 6, "unary_reader_kernel_id": [6, 13], "unary_writer_kernel_id": [6, 13], "cached_program_t": 6, "cachedprogram": 6, "static": 6, "operation_attribut": 6, "tensor_arg": 6, "tensor_return_valu": 6, "void": [6, 13], "cached_program": 6, "multicor": [6, 333, 334, 352, 354], "size_t": 6, "num_cor": 6, "num_cores_i": 6, "program_factory_t": 6, "mandatori": [6, 13], "select": [6, 12, 16], "arg": [6, 12, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 120, 121, 123, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 212, 214, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 317, 318, 319, 320, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 347, 348, 349, 350, 351, 352, 353, 354, 355, 357, 358, 359, 360, 378], "select_program_factori": 6, "valid": [6, 8, 9, 12, 13, 17, 21, 110, 111, 210, 213, 220, 221, 375], "usual": 6, "validate_on_program_cache_miss": 6, "reus": 6, "less": [6, 13, 15, 17, 169, 170, 205, 206, 378], "validate_on_program_cache_hit": 6, "compute_output_shap": [6, 13], "create_output_tensor": [6, 13], "map": [6, 40, 371, 372, 373, 374, 375, 376], "abl": 6, "prim": 6, "after": [6, 10, 16, 17, 210, 240, 262, 371, 378], "keep": [6, 8, 18], "mind": [6, 373], "overload": [6, 12, 276, 335, 371], "queue_id": [6, 22, 24, 28, 29, 31, 39, 41, 45, 46, 53, 54, 55, 56, 57, 58, 60, 69, 73, 83, 84, 89, 91, 92, 94, 96, 97, 99, 101, 103, 104, 106, 107, 109, 113, 126, 128, 129, 130, 131, 133, 135, 137, 146, 149, 151, 154, 158, 159, 160, 161, 162, 165, 167, 169, 171, 175, 180, 181, 185, 188, 190, 191, 194, 196, 198, 205, 207, 222, 223, 226, 227, 229, 230, 232, 233, 240, 242, 248, 249, 255, 260, 264, 265, 266, 269, 270, 282, 283, 284, 285, 290, 291, 293, 295, 296, 297, 298, 303, 309, 310, 311, 312, 314, 317, 319, 320, 322, 324, 325, 327, 328, 333, 334, 339, 350, 352, 353, 354, 357, 358], "automat": [6, 9, 10, 13, 17, 18, 67, 336, 371, 372], "primit": 6, "so": [6, 8, 10, 12, 13, 111, 346, 371, 377], "invok": [6, 15], "case": [6, 8, 9, 13, 18, 67, 210, 336, 371, 378], "custom": [6, 15, 262, 263, 373], "hash": [6, 17, 220, 221], "stl": 6, "hash_t": 6, "compute_program_hash": 6, "create_op_performance_model": 6, "opperformancemodel": 6, "make": [6, 8, 18, 220, 221, 303, 339, 347, 375, 378], "avail": [6, 10, 12, 60, 113, 202, 355, 363, 364, 367, 368, 378], "constexpr": 6, "some_condition_based_on_operation_attributes_and_or_tensor_arg": 6, "true": [6, 8, 12, 13, 29, 31, 77, 78, 83, 92, 97, 99, 103, 122, 129, 211, 215, 216, 251, 252, 282, 285, 316, 317, 319, 321, 339, 346, 352, 354, 356, 371, 372, 373, 374, 375, 376, 377, 378], "tensor_attribut": 6, "output_shap": 6, "create_device_tensor": 6, "dtype": [6, 8, 12, 13, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 212, 214, 217, 218, 219, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 274, 275, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 310, 311, 312, 313, 314, 315, 317, 318, 319, 320, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 336, 337, 338, 348, 349, 350, 351, 357, 358, 359, 360, 361, 362, 371, 372, 373, 374, 376, 378], "42": [6, 372, 375, 377], "single_core_program_factori": 6, "tt_metal": [6, 13, 15, 17, 375], "work_split": 6, "output_tensor": [6, 13, 22, 24, 28, 29, 31, 39, 41, 46, 53, 54, 55, 56, 57, 58, 60, 69, 73, 83, 84, 89, 91, 92, 94, 96, 97, 99, 101, 103, 104, 107, 113, 126, 128, 129, 130, 131, 133, 135, 137, 146, 149, 151, 158, 159, 160, 161, 162, 165, 167, 169, 171, 175, 180, 181, 185, 188, 190, 191, 194, 196, 198, 205, 207, 222, 223, 226, 227, 229, 232, 248, 255, 260, 264, 265, 266, 269, 270, 282, 283, 284, 285, 290, 291, 293, 295, 296, 298, 303, 309, 310, 312, 314, 317, 319, 320, 322, 325, 327, 339, 350, 357, 358, 371, 376, 378], "src_buffer": 6, "dst_buffer": 6, "dataformat": 6, "cb_data_format": 6, "datatype_to_dataformat_convert": 6, "uint32_t": [6, 213], "single_tile_s": 6, "tiles": 6, "cb_data_format_output": 6, "single_tile_size_output": 6, "num_til": 6, "volum": 6, "constant": [6, 13], "tile_hw": 6, "corecoord": [6, 13], "compute_with_storage_grid_s": 6, "num_cores_x": [6, 8, 373], "x": [6, 12, 13, 15, 17, 18, 178, 210, 243, 344, 345, 346, 372, 373, 376, 377], "y": [6, 12, 13, 17, 18, 243, 372, 373, 377], "all_cor": 6, "core_group_1": 6, "core_group_2": 6, "num_tiles_per_core_group_1": 6, "num_tiles_per_core_group_2": 6, "split_work_to_cor": 6, "src0_cb_index": 6, "num_input_til": 6, "circularbufferconfig": 6, "cb_src0_config": 6, "set_page_s": 6, "cb_src0": 6, "createcircularbuff": 6, "output_cb_index": 6, "operand": [6, 13], "num_output_til": 6, "cb_output_config": 6, "cb_output": 6, "src_is_dram": 6, "buffer_typ": [6, 12, 13], "buffertyp": [6, 12, 13], "dram": [6, 12, 13, 18, 337, 371], "reader_compile_time_arg": 6, "dst_is_dram": 6, "writer_compile_time_arg": 6, "createkernel": 6, "eltwis": [6, 13, 116, 271], "kernel": [6, 13, 15, 17, 178, 210, 213, 302, 372], "dataflow": 6, "reader_unary_interleaved_start_id": 6, "readerdatamovementconfig": 6, "writer_unary_interleaved_start_id": 6, "writerdatamovementconfig": 6, "compute_kernel_args_group_1": 6, "per_core_block_cnt": 6, "per_core_block_s": 6, "math_approx_mod": 6, "fals": [6, 8, 13, 77, 83, 97, 99, 103, 129, 178, 210, 251, 252, 282, 333, 334, 340, 341, 346, 353, 354, 371, 372, 373, 374, 375, 376, 377, 378], "eltwise_unary_kernel_group_1_id": 6, "eltwise_sfpu": 6, "computeconfig": 6, "math_fidel": [6, 376], "mathfidel": [6, 376], "hifi4": [6, 17, 375], "compile_arg": 6, "rang": [6, 12, 13, 21, 54, 55, 57, 75, 110, 111, 183, 213, 224, 245, 300], "compute_kernel_args_group_2": 6, "eltwise_unary_kernel_group_2_id": 6, "num_tiles_written": 6, "num_tiles_per_cor": 6, "core_coord_in_core_rang": 6, "tt_assert": 6, "setruntimearg": 6, "address": [6, 13], "move": [6, 9, 10, 12, 13, 371, 373, 374, 375], "shared_vari": 6, "runtime_arg": [6, 13], "getruntimearg": [6, 13], "multi_core_program_factori": 6, "sequenc": [6, 344, 345, 346], "compositeexampleoper": 6, "composite_exampl": 6, "another_copi": 6, "_pybind": 6, "example_pybind": 6, "pybind11": 6, "h": [6, 13, 18, 87, 110, 111, 355], "bind_example_oper": 6, "r": [6, 15, 243, 375], "pybind": 6, "expos": 6, "logic": [6, 194, 195, 198, 199, 210, 220, 221], "self": [6, 8, 12, 13, 18, 376], "correct": 6, "specif": [6, 9, 13, 15, 18, 39, 345, 346, 363, 375], "pybind_overload_t": 6, "decltyp": 6, "examples_pybind": 6, "py_modul": 6, "final": [6, 8, 9, 10, 16, 210, 240, 371], "wherev": 6, "want": [6, 10, 15, 372, 374, 378], "its": [6, 8, 9, 12, 13, 15, 16, 17, 18, 210, 240, 378], "import": [6, 8, 9, 10, 15, 17, 371, 372, 373, 374, 375, 376, 377, 378], "For": [6, 8, 9, 12, 15, 16, 17, 18, 40, 174, 210, 347], "signatur": 6, "And": [6, 8, 12, 13, 18, 371, 372], "ignor": [6, 252], "kwarg": [6, 12, 250, 262, 263, 276, 335, 378], "def": [6, 8, 373, 374, 375, 376, 377, 378], "golden_funct": 6, "befor": [6, 9, 13, 18, 40, 240, 263, 303], "postprocess": 6, "manual": [6, 8, 378], "pack": [6, 352, 354], "preprocess_golden_function_input": 6, "ttnn_input_tensor": 6, "postprocess_golden_function_output": 6, "torch_output_tensor": [6, 378], "becaus": [6, 18, 371, 372, 373], "wa": [6, 9, 17, 18, 371, 377], "particular": [8, 15, 252, 371, 378], "basi": 8, "recommend": [8, 15, 371, 378], "approach": [8, 16, 303, 378], "re": [8, 13, 14, 372], "given": [8, 12, 13, 17, 23, 25, 27, 29, 31, 33, 35, 37, 39, 42, 44, 45, 48, 49, 51, 52, 61, 63, 65, 67, 69, 70, 72, 74, 76, 80, 82, 84, 86, 87, 90, 98, 100, 102, 105, 106, 108, 109, 112, 114, 117, 121, 130, 139, 141, 143, 145, 148, 150, 153, 168, 172, 174, 177, 182, 184, 186, 187, 189, 192, 193, 203, 204, 212, 217, 220, 221, 223, 225, 230, 238, 244, 246, 249, 251, 254, 256, 258, 261, 267, 268, 272, 274, 275, 279, 281, 283, 285, 288, 292, 294, 297, 299, 301, 304, 306, 308, 311, 313, 315, 317, 319, 326, 328, 330, 332, 339, 345, 351, 355, 358, 360, 371], "rewritten": 8, "bert": [8, 14, 374, 375], "modeling_bert": [8, 374], "bertintermedi": 8, "class": [8, 9, 12, 13, 17, 18, 375, 376, 377], "super": [8, 376], "dens": 8, "hidden_s": [8, 347, 373, 374], "intermediate_s": 8, "forward": [8, 13, 376], "hidden_st": [8, 373, 374], "tdd": 8, "torch_bert": 8, "utility_funct": 8, "torch_random": 8, "utils_for_test": 8, "assert_with_pcc": 8, "mark": [8, 9], "parametr": 8, "model_nam": [8, 220, 221, 374, 375, 376], "phiyodr": [8, 374], "larg": [8, 303, 374], "finetun": [8, 374], "squad2": [8, 374], "batch_siz": [8, 92, 132, 342, 347, 373, 374, 375, 376], "sequence_s": [8, 342, 347, 373, 374, 375], "384": [8, 373, 374], "test_bert_intermedi": 8, "manual_se": [8, 371, 372, 373, 376, 377, 378], "bertconfig": [8, 374], "from_pretrain": [8, 374, 377], "eval": [8, 374, 375, 376, 377], "torch_hidden_st": [8, 373], "float32": [8, 12, 17, 18, 371, 376, 378], "torch_output": [8, 373], "initialize_model": [8, 220, 221, 374, 376], "lambda": [8, 139, 306, 374, 376], "convert_to_ttnn": [8, 220, 221], "_": [8, 17, 373, 377], "bert_intermedi": 8, "9999": [8, 371, 372, 373, 374, 376, 378], "bia": [8, 13, 52, 134, 166, 178, 373, 376], "dictionari": 8, "ha": [8, 9, 10, 12, 13, 17, 18, 220, 221, 347, 355, 371, 377, 378], "turn": 8, "ttnn_bert": [8, 374], "put": [8, 220, 221, 371, 373, 376], "bfloat16": [8, 10, 12, 13, 17, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 112, 113, 114, 115, 116, 117, 120, 121, 123, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 212, 214, 217, 218, 219, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 239, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 274, 275, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 310, 311, 312, 313, 314, 315, 317, 318, 319, 320, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 336, 337, 338, 339, 348, 349, 350, 351, 352, 354, 357, 358, 359, 360, 371, 372, 373, 375, 376, 378], "999": 8, "Then": [8, 347], "custom_preprocessor": [8, 220, 221, 374, 376], "bfloat8_b": [8, 12, 17, 18, 29, 31, 32, 34, 35, 44, 48, 52, 70, 80, 84, 103, 104, 105, 107, 108, 148, 150, 168, 173, 174, 192, 193, 208, 212, 217, 223, 230, 254, 283, 285, 288, 292, 299, 313, 315, 317, 319, 326, 330, 357, 360, 373], "bias": [8, 13, 373], "someth": 8, "ttnn_optimized_bert": [8, 374], "isinst": 8, "preprocess_linear_weight": [8, 373], "preprocess_linear_bia": [8, 373], "ff1_weight": 8, "ff1_bia": 8, "memory_config": [8, 12, 13, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 112, 113, 114, 115, 116, 117, 120, 121, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 166, 167, 168, 169, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 200, 201, 202, 203, 204, 205, 207, 208, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 339, 340, 341, 342, 343, 347, 348, 349, 350, 351, 352, 353, 354, 356, 357, 358, 359, 360, 361, 362, 372, 373, 376, 378], "l1_memory_config": [8, 18, 67, 372, 373, 378], "local": [8, 9, 18, 353], "core_grid": [8, 77, 178, 210, 372, 373], "grid": [8, 18, 178, 210, 344, 345, 346], "best": [8, 371], "possibl": [8, 13, 338, 374], "addit": [8, 13, 15], "integr": [8, 9, 10, 13], "incredibli": 9, "excit": 9, "exploratori": 9, "allow": [9, 12, 13, 16], "freedom": 9, "try": [9, 15, 371, 375], "improv": [9, 303], "showcas": 9, "few": [9, 18, 371], "question": 9, "answer": 9, "see": [9, 13, 16, 371, 375, 377], "highlight": [9, 18], "expect": [9, 12, 13, 16, 111, 210, 346, 347, 371], "successfulli": [9, 363, 375, 377], "migrat": [9, 371, 372, 373, 374, 376, 378], "doe": [9, 13, 17, 67], "good": 9, "documen": 9, "within": [9, 12, 13, 18, 54, 57, 363], "describ": [9, 13, 240], "credit": 9, "author": 9, "appropri": 9, "necessari": 9, "error": [9, 40, 165, 222, 338], "might": [9, 13, 372], "encount": 9, "demonstr": [9, 17], "adequ": 9, "achiev": [9, 16], "accept": [9, 16, 344, 345, 346], "pearson": 9, "correl": [9, 13], "coeffici": 9, "pcc": [9, 16], "been": [9, 220, 221, 377], "ci": 9, "pipelin": [9, 17], "unit": [9, 13], "metric": 9, "meet": 9, "accuraci": 9, "These": [9, 10, 13, 15, 363, 378], "continu": [9, 16], "against": [9, 378], "upon": 9, "everi": [9, 12, 15, 17, 373, 378], "commit": [9, 375], "ongo": 9, "complianc": 9, "catch": 9, "regress": 9, "earli": 9, "end": [9, 12, 13, 17, 38, 354, 373, 375], "collect": [9, 17, 363, 375], "limit": [9, 13], "usag": [9, 15], "varieti": [9, 13], "condit": 9, "measur": 9, "updat": [9, 15, 16, 164, 378], "special": [9, 13, 18], "run_device_perf_model": 9, "annot": 9, "models_device_performance_bare_met": 9, "schedul": 9, "appli": [9, 13, 22, 24, 28, 41, 46, 53, 54, 55, 56, 57, 58, 60, 73, 89, 94, 96, 97, 99, 101, 103, 104, 107, 113, 126, 128, 129, 131, 132, 133, 135, 137, 146, 149, 158, 159, 160, 161, 162, 167, 169, 171, 175, 178, 180, 181, 185, 188, 190, 191, 194, 196, 198, 205, 207, 210, 226, 227, 229, 232, 240, 248, 260, 264, 265, 266, 269, 270, 273, 275, 282, 284, 290, 291, 293, 295, 296, 298, 303, 310, 312, 314, 320, 322, 325, 327, 343, 350], "clear": [9, 16, 371, 372, 373, 374, 375, 376], "incorpor": 9, "autom": 9, "extern": [9, 16, 18], "servic": 9, "impact": 9, "either": [9, 13, 15, 18, 67, 77, 210, 336, 363], "run_perf_models_oth": 9, "run_perf_models_llm_javelin": 9, "run_perf_models_cnn_javelin": 9, "models_performance_bare_met": 9, "gener": [9, 13, 15, 17, 91, 111, 243, 371, 372, 373, 374, 375, 376, 379], "text": [9, 13], "run_demos_single_card_n150_test": 9, "run_demos_single_card_n300_test": 9, "run_t3000_demo_test": 9, "target": [9, 12, 13, 118, 119], "purpos": [9, 16, 18, 210], "organ": [9, 18, 67, 336], "manag": [9, 15, 209, 262, 263], "look": [9, 14, 18, 210, 339, 375, 377], "test_ttnn_functional_resnet50": 9, "resnet50testinfra": 9, "setup": [9, 14, 15, 375, 377], "handl": [10, 371], "machin": [10, 12, 13, 17, 363, 377], "send": [10, 12, 13], "print": [10, 12, 13, 17, 18, 40, 69, 123, 178, 210, 242, 273, 275, 302, 336, 338, 371, 372, 373, 375, 377, 378], "__name__": 10, "__main__": [10, 375], "pci": [10, 371, 372, 373, 374, 375, 376], "slot": 10, "tt_devic": [10, 12, 13], "createdevic": [10, 12, 376], "py_tensor": [10, 12], "randn": [10, 12, 40, 92, 123, 132, 178, 210, 336, 337, 338, 372, 373, 377], "32": [10, 12, 13, 18, 67, 69, 111, 132, 178, 210, 233, 242, 275, 302, 336, 337, 371, 372, 374, 375, 376, 378], "tt_tensor": [10, 12, 13], "tolist": [10, 12], "datatyp": [10, 12, 13, 28, 29, 38, 40, 67, 84, 92, 93, 94, 123, 124, 125, 126, 132, 135, 167, 169, 178, 190, 191, 194, 198, 205, 210, 223, 226, 227, 236, 237, 314, 317, 320, 333, 334, 336, 337, 361, 362, 371, 372, 376], "row_major": [10, 12, 13, 17, 29, 31, 39, 48, 52, 61, 70, 77, 84, 112, 121, 148, 168, 192, 193, 196, 212, 217, 223, 285, 294, 315, 317, 319, 333, 334, 351, 352, 354, 360, 371, 372, 376], "tt_relu_out": 10, "tt_output": [10, 12], "closedevic": 10, "previou": [10, 92], "expon": [10, 248, 249, 280, 281], "power": [10, 249], "scalar": [10, 13, 29, 45, 53, 55, 56, 58, 83, 85, 86, 115, 117, 173, 174, 208, 211, 215, 216, 246, 255, 256, 272, 281, 316, 317, 321, 356, 357, 378], "power_fp": 10, "float": [10, 12, 13, 28, 29, 31, 32, 33, 34, 35, 52, 62, 63, 64, 65, 66, 83, 84, 86, 89, 90, 94, 95, 111, 116, 117, 124, 125, 126, 127, 134, 135, 136, 138, 139, 140, 142, 144, 145, 146, 166, 167, 169, 170, 171, 172, 174, 190, 191, 194, 198, 202, 204, 205, 206, 208, 211, 215, 216, 223, 226, 227, 228, 240, 246, 247, 248, 249, 255, 256, 269, 270, 271, 272, 277, 278, 280, 281, 284, 285, 287, 303, 304, 305, 306, 314, 316, 317, 319, 320, 321, 331, 332, 334, 348, 349, 356, 357], "point": [10, 13, 17, 303], "posit": [10, 13, 53, 56, 58, 345], "actual": [10, 18, 255, 284, 371], "suppli": [10, 12, 13], "lastli": 10, "fallback_op": [10, 13], "py_tensor_exp": 10, "randint": [10, 92, 374], "py_relu_out": 10, "py_pow_out": 10, "tt_pow_out": 10, "behav": [10, 13], "regular": 10, "even": [10, 13, 210], "though": [10, 210], "hood": 10, "tt_silu_out": 10, "tt_exp_out": 10, "dimens": [10, 12, 13, 18, 39, 67, 69, 70, 128, 132, 133, 210, 242, 251, 252, 264, 275, 302, 322, 336, 338, 339, 342, 344, 345, 346, 347], "must": [10, 12, 13, 15, 18, 39, 210, 225, 255, 262, 263, 333, 334, 339, 344, 352, 354, 379], "default": [10, 12, 13, 17, 19, 21, 31, 32, 34, 52, 62, 63, 65, 70, 77, 90, 91, 92, 130, 138, 139, 140, 142, 144, 145, 154, 165, 172, 178, 202, 204, 210, 220, 221, 222, 251, 252, 255, 256, 278, 287, 302, 304, 305, 306, 309, 319, 340, 341, 346, 348, 349, 372, 378], "modifi": [10, 303, 371], "assign": [10, 45], "31": [10, 12, 54, 57, 375, 377], "leav": 10, "anyth": 10, "alreadi": [10, 220, 221, 238, 363, 375, 377], "manipul": 12, "sent": 12, "receiv": [12, 17], "platform": [12, 14, 375], "ttdnn": 12, "util": [12, 371, 372, 377], "differ": [12, 18, 210, 314, 371, 378], "w": [12, 13, 17, 87, 110, 111, 355], "z": [12, 13, 17, 377], "divis": [12, 13, 18, 255], "construct": [12, 353, 378], "nor": 12, "tile": [12, 13, 17, 18, 25, 29, 31, 32, 33, 34, 35, 42, 44, 48, 49, 51, 52, 61, 62, 70, 76, 80, 82, 84, 98, 100, 102, 103, 104, 105, 107, 108, 111, 112, 121, 138, 143, 148, 150, 173, 174, 182, 184, 186, 189, 192, 193, 196, 202, 208, 212, 217, 223, 230, 241, 254, 267, 283, 285, 288, 292, 294, 297, 299, 301, 305, 308, 313, 315, 317, 319, 326, 330, 332, 333, 334, 344, 345, 346, 351, 352, 353, 354, 357, 358, 360, 371, 372, 375], "subsect": 12, "major": [12, 18, 111, 233, 353, 371, 372], "insid": [12, 378], "64": [12, 18, 69, 77, 178, 210, 242, 302, 336, 337, 373, 374, 375, 376, 378], "63": [12, 375], "65": [12, 375, 377], "66": 12, "127": [12, 375], "3968": 12, "3969": 12, "3970": 12, "4031": 12, "4032": 12, "4033": 12, "4034": 12, "4095": 12, "4096": [12, 92], "4097": 12, "4098": 12, "4159": 12, "4160": 12, "4161": 12, "6462": 12, "4223": 12, "8064": 12, "8065": 12, "8066": 12, "8127": 12, "8128": 12, "8129": 12, "8130": 12, "8191": 12, "95": 12, "1984": 12, "1985": 12, "2015": [12, 375], "33": [12, 372, 375], "96": [12, 375, 378], "97": [12, 375], "2016": 12, "2017": [12, 375, 377], "2047": 12, "2080": 12, "2081": 12, "2111": 12, "2144": 12, "2145": 12, "2175": 12, "4064": 12, "4065": 12, "fourth": [12, 13], "6111": 12, "6176": 12, "ownedstorag": [12, 13], "borrowedstorag": 12, "devicestorag": [12, 13], "correspond": [12, 13, 91, 92, 345], "itself": 12, "pointer": 12, "That": [12, 18, 372], "underli": 12, "count": [12, 17, 110, 111, 375], "well": [12, 16, 233], "numpi": [12, 18, 375, 377], "l1": [12, 18, 77, 178, 210, 337, 373], "reason": 12, "list": [12, 13, 28, 29, 31, 77, 84, 87, 91, 93, 94, 126, 135, 167, 169, 190, 191, 194, 198, 205, 223, 226, 227, 240, 242, 276, 285, 314, 317, 319, 320, 345, 346], "ye": [12, 13, 21, 110, 111, 213], "data_typ": [12, 373], "number": [12, 13, 16, 17, 18, 28, 29, 30, 32, 34, 47, 83, 84, 85, 94, 95, 115, 116, 126, 127, 135, 136, 147, 157, 167, 169, 170, 173, 190, 191, 194, 198, 200, 201, 205, 206, 208, 214, 218, 223, 226, 227, 228, 231, 233, 239, 240, 271, 273, 275, 286, 309, 314, 317, 318, 320, 339, 340, 341, 353, 359, 373, 377], "uint32": [12, 17, 18, 39, 91, 92, 154, 371], "bfloat4_b": [12, 29, 31, 32, 34, 35, 48, 52, 70, 84, 148, 168, 173, 174, 192, 193, 208, 212, 217, 223, 285, 315, 317, 319, 357, 360], "No": [12, 13, 213], "mem_config": [12, 337], "bank": 12, "_ttnn": [12, 13, 18, 42, 44, 78, 91, 122, 259, 276, 335], "arg0": 12, "none": [12, 13, 22, 23, 24, 26, 28, 29, 30, 31, 32, 34, 38, 39, 40, 41, 43, 46, 47, 50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 68, 73, 75, 77, 78, 79, 81, 83, 84, 85, 89, 91, 92, 94, 96, 97, 99, 101, 103, 104, 107, 113, 115, 116, 118, 119, 120, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 137, 146, 147, 149, 151, 157, 158, 159, 160, 161, 162, 165, 166, 167, 169, 171, 173, 175, 176, 178, 180, 181, 183, 185, 188, 190, 191, 194, 196, 197, 198, 200, 201, 205, 207, 208, 210, 211, 214, 215, 216, 218, 219, 220, 221, 222, 223, 224, 226, 227, 229, 231, 232, 234, 235, 236, 237, 239, 240, 247, 248, 251, 253, 255, 256, 259, 260, 261, 262, 263, 264, 265, 266, 269, 270, 271, 277, 282, 284, 285, 286, 289, 290, 291, 293, 295, 296, 298, 300, 302, 303, 307, 309, 310, 312, 314, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 327, 329, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 347, 350, 352, 353, 354, 356, 357, 359, 361, 362, 371, 372, 373, 374, 375, 376], "arg1": 12, "arg2": 12, "arg3": 12, "arg4": 12, "divisbl": [12, 13], "arg5": 12, "tensormemorylayout": [12, 13, 376], "single_bank": 12, "strategi": [12, 13, 17, 18, 77, 210, 375], "dict": [12, 220, 221, 377], "str": [12, 13, 28, 29, 40, 84, 94, 126, 135, 167, 169, 178, 190, 191, 194, 198, 205, 210, 220, 221, 223, 226, 227, 289, 314, 317, 320, 377], "ptr": 12, "np": 12, "owned_buffer_for_uint8_t": 12, "owned_buffer_for_uint16_t": 12, "owned_buffer_for_int32_t": 12, "owned_buffer_for_uint32_t": 12, "owned_buffer_for_float32_t": 12, "owned_buffer_for_bfloat16_t": 12, "borrowed_buffer_for_uint8_t": 12, "borrowed_buffer_for_uint16_t": 12, "borrowed_buffer_for_int32_t": 12, "borrowed_buffer_for_uint32_t": 12, "borrowed_buffer_for_float32_t": 12, "borrowed_buffer_for_bfloat16_t": 12, "indic": [12, 13, 39, 91, 92, 233, 354], "everywher": 12, "place": [12, 18, 95, 127, 136, 163, 164, 170, 206, 228, 341], "along": [12, 13, 15, 18, 240, 251, 252, 302, 339, 342], "output_tensor_shap": [12, 240, 334], "input_tensor_shap": 12, "input_tensor_start": [12, 240], "pad_valu": [12, 118, 334], "inp": 12, "tt_tensor_pad": 12, "npad": 12, "right": [12, 13], "bottom": [12, 363], "storagetyp": 12, "memory_layout": [12, 13], "interleav": [12, 13, 18, 275, 337, 347], "shard_spec": [12, 13], "nullopt": [12, 13, 39, 65, 154, 233, 242, 339, 376], "mesh_devic": 12, "multi_devic": [12, 335], "meshdevic": [12, 335, 336], "target_layout": [12, 13, 118, 119], "worker": [12, 336, 373, 374], "thread": [12, 336, 375, 378], "ti": 12, "inclus": [12, 18], "output_tensor_end": [12, 354], "output_tensor_start": 12, "tt_tensor_unpad": 12, "nunpad": 12, "align": [12, 13, 17], "left": [12, 13, 16], "remov": [12, 68, 210, 338, 354, 371, 374, 375, 377], "apart": 12, "restrict": 12, "eight": 12, "shardspec": 12, "across": [12, 17, 18, 132, 346], "otherwis": [12, 324, 339, 373, 378], "dram_channel": 12, "rememb": 12, "py_output": 12, "unifi": 13, "locat": [13, 14, 17, 240, 375, 378], "tt_eager": [13, 375], "current": [13, 15, 18, 77, 165, 210, 220, 221, 222, 344, 345, 363, 371, 378], "dimension": [13, 18, 210, 371], "better": 13, "caller": 13, "launch": [13, 363], "plug": 13, "declar": 13, "newoper": 13, "legacyshap": 13, "programwithcallback": 13, "create_program": 13, "some_memb": 13, "optional_input_tensor": 13, "leverag": [13, 210], "instead": [13, 40, 151, 270, 371, 373, 377], "validate_with_output_tensor": 13, "programwithoptionaloutputtensor": 13, "box": [13, 15], "preferred_nam": 13, "parallelization_strategi": 13, "get_parallelization_strategi": 13, "parallel": [13, 17, 344, 345, 346, 375], "parallelizationstrategyenum": 13, "enqueu": 13, "wait": [13, 17, 324], "finish": [13, 17, 375], "asynchron": 13, "complet": [13, 324], "cq_id": [13, 122], "reload": 13, "program_cach": 13, "disabl": [13, 220, 221, 371, 372, 373, 374, 375, 376, 378], "disable_and_clear": 13, "entri": 13, "num_entri": 13, "cachabl": 13, "override_runtime_args_callback": 13, "input_buff": 13, "output_buff": 13, "src_dram_buff": 13, "dst_dram_buff": 13, "relat": 13, "tt_metal_logger_typ": [13, 378], "tt_metal_logger_level": [13, 378], "inform": [13, 363], "1280": 13, "layoutconversiononhost": 13, "320": [13, 77], "miss": [13, 378], "eltwiseunari": 13, "op_typ": 13, "unaryoptyp": 13, "param": [13, 87, 252, 377], "_tt": 13, "deprec": [13, 19, 21, 118, 119, 241, 374], "morehsoftmaxopparallelizationstrategi": 13, "output_mem_config": [13, 375], "compute_kernel_config": [13, 178, 210, 302, 343], "union": [13, 84, 124, 125, 211, 215, 216, 240, 248, 316, 321, 356], "grayskullcomputekernelconfig": 13, "wormholecomputekernelconfig": 13, "output_grad_tensor": 13, "input_grad_tensor": 13, "morehsoftmaxbackwardopparallelizationstrategi": 13, "backward": [13, 23, 25, 27, 29, 31, 33, 35, 37, 42, 44, 45, 48, 49, 51, 52, 61, 63, 65, 70, 72, 74, 76, 80, 82, 84, 86, 90, 92, 98, 100, 102, 105, 106, 108, 109, 112, 114, 117, 121, 130, 139, 141, 143, 145, 148, 150, 153, 168, 172, 174, 177, 182, 184, 186, 187, 189, 192, 193, 203, 204, 212, 217, 223, 225, 230, 244, 246, 249, 252, 254, 256, 258, 261, 267, 268, 272, 274, 279, 281, 283, 285, 288, 292, 294, 297, 299, 301, 304, 306, 308, 311, 313, 315, 317, 319, 326, 328, 330, 332, 351, 358, 360], "grad": 13, "softmin": 13, "logsoftmax": 13, "num_group": [13, 134], "ep": [13, 202, 204], "999999747378752e": 13, "06": [13, 377], "gamma": 13, "beta": [13, 303, 304], "are_required_output": [13, 29, 31, 84, 223, 285, 317, 319], "rstd": 13, "mean_mem_config": 13, "rstd_mem_config": 13, "output_grad": 13, "input_grad": [13, 106, 109, 230, 249, 297, 311, 328], "gamma_grad": 13, "beta_grad": 13, "input_grad_mem_config": 13, "gamma_grad_mem_config": 13, "beta_grad_mem_config": 13, "p": [13, 18, 210, 225], "keepdim": [13, 211, 215, 216, 316, 321, 356], "mul": [13, 128, 133, 264, 322, 373], "hw": [13, 111], "fill_valu": [13, 124, 125], "fill": [13, 17, 109, 111, 112, 273], "ellipsi": 13, "output_layout": 13, "output_on_devic": 13, "element": [13, 18, 22, 24, 39, 41, 46, 53, 54, 55, 56, 57, 58, 60, 73, 89, 96, 97, 99, 101, 103, 104, 107, 113, 128, 129, 131, 132, 133, 137, 146, 149, 158, 159, 160, 161, 162, 171, 175, 180, 181, 185, 188, 196, 207, 229, 232, 233, 240, 248, 255, 260, 264, 265, 266, 269, 270, 273, 275, 282, 284, 290, 291, 293, 295, 296, 298, 303, 310, 312, 322, 325, 327, 339, 350, 354, 371], "third": 13, "except": [13, 210, 371, 373, 375], "fewer": 13, "than": [13, 15, 17, 81, 126, 127, 135, 136, 169, 170, 176, 205, 206, 225, 373, 378], "stride": [13, 87, 213, 371, 376], "dilat": [13, 213, 376], "group": [13, 376], "2d": [13, 18, 87, 132, 210, 213, 355], "convolut": [13, 376], "over": [13, 132, 134, 166, 277, 302, 344, 345, 346], "compos": [13, 132], "sever": [13, 132], "plane": [13, 132, 376], "four": 13, "side": [13, 17, 371, 372, 373, 374, 375, 376], "space": [13, 18], "connect": 13, "1e": [13, 134, 166, 277], "paper": 13, "mathrm": [13, 22, 24, 28, 41, 46, 53, 54, 55, 56, 57, 58, 60, 73, 89, 94, 95, 96, 97, 99, 101, 103, 104, 107, 113, 126, 127, 128, 129, 131, 132, 133, 135, 136, 137, 146, 149, 151, 158, 159, 160, 161, 162, 167, 169, 170, 171, 175, 180, 181, 185, 188, 190, 191, 194, 195, 196, 198, 199, 205, 206, 207, 226, 227, 228, 229, 232, 248, 251, 260, 264, 265, 266, 269, 270, 282, 284, 290, 291, 293, 295, 296, 298, 303, 310, 312, 314, 320, 322, 325, 327, 350], "epsilon": [13, 134, 166, 277], "separ": 13, "denomin": [13, 255], "numer": [13, 255, 303], "stabil": [13, 16, 303], "normalized_shap": 13, "layer": [13, 17, 376], "mode": [13, 52, 97, 99, 103, 129, 130, 256, 282, 371, 372, 373, 374, 375, 376], "determin": [13, 210, 220, 221, 371, 372, 373, 374, 375, 376], "much": [13, 17, 372], "m": [13, 210, 372], "equal": [13, 94, 95, 126, 127, 169, 170, 227, 228, 378], "reflect": [13, 16], "replic": 13, "circular": 13, "scale_factor": [13, 355], "nearest": [13, 334, 355], "align_corn": 13, "recompute_scale_factor": 13, "antialia": 13, "down": 13, "algorithm": [13, 355], "spatial": [13, 87, 132, 355], "bilinear": 13, "bicub": 13, "trilinear": 13, "area": 13, "exact": [13, 18, 373], "whether": [13, 40, 220, 221, 333, 334, 339, 347, 352, 354], "center": 13, "corner": 13, "pixel": 13, "recomput": [13, 220], "flag": [13, 40, 375], "anti": 13, "alias": 13, "output_s": 13, "repetit": [13, 273, 275], "total": [13, 17], "axi": [13, 275], "concaten": [13, 69, 70, 342, 347], "wise": [13, 22, 24, 41, 46, 53, 54, 55, 56, 57, 58, 60, 73, 89, 96, 97, 99, 101, 103, 104, 107, 113, 128, 129, 131, 133, 137, 146, 149, 158, 159, 160, 161, 162, 171, 175, 180, 181, 185, 188, 196, 207, 229, 232, 248, 255, 260, 264, 265, 266, 269, 270, 282, 284, 290, 291, 293, 295, 296, 298, 303, 310, 312, 322, 325, 327, 350], "known": 13, "sigma": 13, "logist": 13, "x_": 13, "x_i": 13, "sum_j": 13, "x_j": 13, "scale": [13, 140, 142, 287, 303, 344, 345, 346], "lie": 13, "in_channel": [13, 376], "out_channel": [13, 376], "kernel_s": [13, 376], "padding_mod": 13, "signal": [13, 132, 375], "simplest": 13, "c_": 13, "h_": 13, "w_": 13, "precis": 13, "n_i": 13, "_j": 13, "sum_": 13, "k": [13, 210, 245, 339, 344, 345, 346, 372], "star": 13, "cross": 13, "denot": [13, 154], "height": [13, 17, 18, 67, 77, 110, 111, 132, 210, 213, 334, 336], "width": [13, 17, 67, 77, 110, 111, 132, 210, 213, 334, 336, 342], "convolv": 13, "learnabl": 13, "running_mean": 13, "running_var": 13, "num_batches_track": 13, "num_featur": 13, "momentum": 13, "affin": 13, "track_running_stat": 13, "4d": 13, "deep": 13, "network": 13, "reduc": [13, 39, 339], "intern": [13, 14], "covari": 13, "shift": [13, 140, 142], "track": 13, "varianc": 13, "num_channel": 13, "boolean": [13, 155, 156, 178, 210, 339], "lernabl": 13, "per": [13, 353, 375], "elementwise_affin": 13, "return_indic": 13, "ceil_mod": 13, "channels_last": [13, 17], "reshape_2d": 13, "kh": [13, 346], "kw": 13, "begin": [13, 17], "c_j": 13, "max_": 13, "ldot": 13, "window": [13, 213], "implicit": 13, "infin": 13, "adapt": [13, 132], "averag": [13, 17, 132, 375], "smallest": [13, 339], "integ": [13, 54, 57, 240, 248, 345, 346], "greater": [13, 81, 126, 127, 135, 136, 176, 225, 378], "largest": [13, 339], "truncat": [13, 40, 351], "mod": 13, "dividend": 13, "absolut": [13, 165], "bitwis": 13, "NOT": [13, 371, 372, 373, 374, 376], "OR": [13, 198, 199], "immedi": [13, 15], "AND": [13, 194, 195], "xor": 13, "arithmet": 13, "bit": [13, 40], "promot": 13, "behavior": [13, 210], "undefin": 13, "retain": 13, "argmin": 13, "unexpect": 13, "fusion": 13, "togeth": 13, "fused_op": 13, "in_featur": 13, "out_featur": 13, "num_dim": 13, "spec": [13, 77], "moment": 13, "add_and_norm": 13, "flexibl": 13, "earlier": 13, "while": [13, 16], "cost": 13, "contigu": [13, 347, 371], "ml": 14, "workload": 14, "guid": [14, 15], "falcon": 14, "7b": 14, "navig": 14, "mistral": 14, "llama2": 14, "70b": 14, "soon": [14, 19, 21, 118, 119, 241], "t3000": [14, 15], "learn": [14, 18, 372, 375], "dive": 14, "deeper": 14, "jupyt": [14, 363, 375], "notebook": [14, 363, 375, 377], "system": [15, 17], "tool": [15, 17, 375], "driver": [15, 371, 372, 373, 374, 375, 376], "releas": [15, 210], "softwar": [15, 363, 371, 372, 373, 374, 376], "packag": [15, 374, 375, 377], "asset": 15, "tag": 15, "firmwar": 15, "purchas": 15, "card": 15, "aib": 15, "html": [15, 375], "o": [15, 17, 371, 372, 373, 374, 375, 377], "kmd": 15, "flash": [15, 345], "smi": 15, "topologi": 15, "ubuntu": [15, 371, 372, 373, 374, 375, 376, 377], "20": [15, 17, 303, 304, 375, 377], "04": [15, 377], "v1": [15, 347], "27": [15, 375, 377], "fw_pack": 15, "80": 15, "v80": 15, "v2": 15, "abov": 15, "mesh": 15, "level": [15, 17], "sudo": [15, 17], "apt": 15, "properti": [15, 18], "99": [15, 375], "essenti": 15, "8ubuntu1": 15, "python3": [15, 374, 375, 377], "venv": 15, "libhwloc": 15, "dev": [15, 375], "graphviz": [15, 375], "cmake": 15, "1ubuntu1": 15, "ninja": 15, "wget": 15, "llvm": 15, "org": [15, 375, 377], "chmod": 15, "17": [15, 373, 374, 375, 376, 377], "libc": 15, "abi": 15, "hugepag": [15, 371, 372, 373, 374, 375, 376], "latest": [15, 17], "setup_hugepag": 15, "blob": [15, 377], "main": [15, 16, 363, 375, 377, 378], "infra": 15, "machine_setup": 15, "raw": [15, 377], "githubusercont": 15, "first_pass": 15, "reboot": [15, 17], "choos": 15, "wheel": [15, 375], "matter": 15, "still": [15, 18], "lf": 15, "repo": 15, "recurs": 15, "submodul": [15, 220, 221], "cd": [15, 17, 375], "foreach": 15, "fetch": 15, "pull": [15, 16], "highli": 15, "flow": [15, 17], "arch_nam": [15, 373, 374], "tt_metal_hom": [15, 17, 375], "build_met": 15, "wormhole_b0": [15, 53, 54, 55, 56, 57, 58, 60, 113, 202, 255], "blackhol": 15, "about": [15, 18, 372], "variou": 15, "pip": [15, 375, 377], "lower": [15, 151, 344], "highest": 15, "edit": [15, 375], "architectur": [15, 364, 367, 368, 378], "ie": 15, "choic": [15, 371], "wheel_fil": 15, "whl": [15, 375, 377], "further": [15, 372], "txt": [15, 375], "visit": 15, "codebas": 15, "pandoc": [15, 375], "libtbb": 15, "libcapston": 15, "pkg": 15, "doxygen": 15, "www": 15, "nl": 15, "higher": [15, 17, 18, 303], "intend": 16, "reliabl": 16, "simultan": 16, "fine": 16, "tune": 16, "themselv": [16, 18], "goal": 16, "ask": 16, "driven": 16, "popular": 16, "kent": 16, "beck": 16, "By": [16, 303, 372], "long": 16, "term": 16, "benefit": 16, "submit": 16, "label": [16, 18], "fulli": [16, 17], "branch": 16, "brief": 16, "4730": 16, "rst": 16, "format": [16, 40, 118, 119, 346, 353], "sweep": 16, "referenc": [16, 375], "verifi": 16, "codeown": 16, "pr": 16, "comparison": 16, "comment": 16, "build_script": [17, 375], "build_with_profiler_opt": [17, 375], "test_perf_resnet": 17, "test_perf_bare_met": 17, "0185": 17, "25": [17, 336, 375, 377], "consol": 17, "similar": [17, 371, 373], "shorter": 17, "append": [17, 210, 377], "cli": 17, "explain": 17, "reset": 17, "tt_smi": 17, "tensix_reset": 17, "due": [17, 18, 371], "tensix": 17, "skew": 17, "timer": 17, "wh": 17, "popul": [17, 163, 220], "least": [17, 210], "twice": 17, "being": [17, 18, 262, 263], "analyz": 17, "1000": [17, 376, 377], "fixtur": 17, "ttl": 17, "dumpdeviceprofil": 17, "avoid": [17, 220, 303], "drop": 17, "around": 17, "120": [17, 375], "eighth": 17, "warn": [17, 371, 372, 373, 374, 375, 376], "messag": 17, "mention": 17, "risc": 17, "faster": [17, 373], "those": 17, "analysi": 17, "affect": 17, "column": [17, 18, 375], "ran": [17, 373], "python_fallback": [17, 375], "tt_dnn_cpu": 17, "tt_dnn_devic": [17, 375], "global": [17, 375], "id": [17, 22, 24, 28, 29, 31, 39, 41, 45, 46, 53, 54, 55, 56, 57, 58, 60, 69, 73, 83, 84, 89, 91, 92, 94, 96, 97, 99, 101, 103, 104, 106, 107, 109, 113, 126, 128, 129, 130, 131, 133, 135, 137, 146, 149, 151, 154, 158, 159, 160, 161, 162, 165, 167, 169, 171, 175, 180, 181, 185, 188, 190, 191, 194, 196, 198, 205, 207, 222, 223, 226, 227, 229, 230, 232, 233, 240, 242, 248, 249, 255, 260, 264, 265, 266, 269, 270, 282, 283, 284, 285, 290, 291, 293, 295, 296, 297, 298, 303, 309, 310, 311, 312, 314, 317, 319, 320, 322, 325, 327, 328, 333, 334, 339, 350, 352, 353, 354, 357, 358], "math": [17, 28, 77, 94, 95, 126, 127, 135, 136, 167, 169, 170, 190, 191, 194, 195, 198, 199, 205, 206, 226, 227, 228, 314, 320, 375], "fidel": [17, 375], "field": 17, "lofi": [17, 376], "hifi2": 17, "hifi3": 17, "clock": 17, "stamp": 17, "durat": [17, 373, 375, 378], "nanosecond": 17, "end_t": 17, "start_t": 17, "fw": [17, 371, 372, 373, 374, 376], "cycl": 17, "earliest": 17, "core_frequ": 17, "marker": 17, "chosen": [17, 377], "brisc": 17, "ncrisc": 17, "trisc0": 17, "trisc1": 17, "trisc2": 17, "cb": 17, "front": [17, 377], "spent": 17, "cb_wait_front": 17, "reserv": 17, "cb_reserve_back": 17, "path": [17, 40, 375, 377, 378], "datamov": 17, "templat": 17, "io": 17, "input_0_memori": 17, "dev_0_dram": 17, "dec_0_l1": 17, "tgz": 17, "filenam": [17, 377], "item": [17, 375], "aggreg": 17, "timestamp": 17, "respect": [18, 92, 210, 284], "28": [18, 375, 377], "32x32": 18, "bracket": 18, "obtain": [18, 375], "consecut": 18, "4x4": 18, "transit": 18, "2x2": 18, "illustr": 18, "uint16": [18, 339], "byte": 18, "sizeof": 18, "owned_host_storag": 18, "borrowed_host_storag": 18, "device_storag": 18, "distribut": [18, 77, 178, 210], "ideal": 18, "abstract": 18, "awai": 18, "compress": 18, "upper": [18, 344], "remain": 18, "128x128": 18, "who": 18, "subset": 18, "orient": [18, 77], "know": 18, "understand": 18, "unshard": 18, "coordin": 18, "dram_memory_config": [18, 38, 67, 178, 210, 347], "physic": 18, "pybind11_object": 18, "complextensor": [22, 23, 29, 36, 37, 71, 72, 83, 84, 152, 153, 155, 156, 223, 243, 244, 257, 258, 260, 261, 317], "_tensor": [22, 24, 41, 46, 53, 54, 55, 56, 57, 58, 60, 73, 89, 96, 97, 99, 101, 103, 104, 107, 113, 128, 129, 131, 132, 133, 137, 146, 149, 151, 158, 159, 160, 161, 162, 171, 175, 180, 181, 185, 188, 196, 207, 229, 232, 248, 251, 260, 264, 265, 266, 269, 270, 282, 284, 290, 291, 293, 295, 296, 298, 303, 310, 312, 322, 325, 327, 350], "_i": [22, 24, 28, 41, 46, 53, 54, 55, 56, 57, 58, 60, 73, 89, 94, 95, 96, 97, 99, 101, 103, 104, 107, 113, 126, 127, 128, 129, 131, 132, 133, 135, 136, 137, 146, 149, 151, 158, 159, 160, 161, 162, 167, 169, 170, 171, 175, 180, 181, 185, 188, 190, 191, 194, 195, 196, 198, 199, 205, 206, 207, 226, 227, 228, 229, 232, 248, 251, 260, 264, 265, 266, 269, 270, 282, 284, 290, 291, 293, 295, 296, 298, 303, 310, 312, 314, 320, 322, 325, 327, 350], "keyword": [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 112, 113, 114, 115, 116, 117, 120, 121, 126, 128, 129, 130, 131, 132, 133, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 167, 168, 169, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 200, 201, 202, 203, 204, 205, 207, 208, 210, 212, 214, 217, 218, 219, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 233, 234, 235, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 317, 318, 319, 320, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 339, 340, 341, 342, 348, 349, 350, 351, 352, 353, 354, 357, 358, 359, 360], "prealloc": [22, 24, 28, 29, 31, 39, 41, 46, 53, 54, 55, 56, 57, 58, 60, 69, 73, 83, 84, 89, 94, 96, 97, 99, 101, 103, 104, 106, 107, 109, 113, 126, 128, 129, 130, 131, 133, 135, 137, 146, 149, 151, 158, 159, 160, 161, 162, 167, 169, 171, 175, 180, 181, 185, 188, 190, 191, 194, 196, 198, 205, 207, 223, 226, 227, 229, 230, 232, 248, 249, 255, 260, 264, 265, 266, 269, 270, 282, 283, 284, 285, 290, 291, 293, 295, 296, 297, 298, 303, 310, 311, 312, 314, 317, 319, 320, 322, 325, 327, 328, 339, 350, 357, 358], "uint8": [22, 24, 28, 29, 31, 39, 41, 45, 46, 53, 54, 55, 56, 57, 58, 60, 69, 73, 83, 84, 89, 94, 96, 97, 99, 101, 103, 104, 106, 107, 109, 113, 126, 128, 129, 130, 131, 133, 135, 137, 146, 149, 151, 158, 159, 160, 161, 162, 167, 169, 171, 175, 180, 181, 185, 188, 190, 191, 194, 196, 198, 205, 207, 223, 226, 227, 229, 230, 232, 240, 248, 249, 255, 260, 264, 265, 266, 269, 270, 282, 283, 284, 285, 290, 291, 293, 295, 296, 297, 298, 303, 309, 310, 311, 312, 314, 317, 319, 320, 322, 325, 327, 328, 339, 350, 357, 358], "queue": [22, 24, 28, 29, 31, 39, 41, 45, 46, 53, 54, 55, 56, 57, 58, 60, 69, 73, 83, 84, 89, 91, 92, 94, 96, 97, 99, 101, 103, 104, 106, 107, 109, 113, 126, 128, 129, 130, 131, 133, 135, 137, 146, 149, 151, 154, 158, 159, 160, 161, 162, 165, 167, 169, 171, 175, 180, 181, 185, 188, 190, 191, 194, 196, 198, 205, 207, 222, 223, 226, 227, 229, 230, 232, 233, 240, 242, 248, 249, 255, 260, 264, 265, 266, 269, 270, 282, 283, 284, 285, 290, 291, 293, 295, 296, 297, 298, 303, 309, 310, 311, 312, 314, 317, 319, 320, 322, 324, 325, 327, 328, 333, 334, 339, 350, 352, 353, 354, 357, 358], "input_tensor_a": [23, 28, 29, 30, 31, 32, 33, 34, 35, 45, 47, 48, 52, 70, 83, 84, 85, 94, 115, 117, 126, 130, 135, 147, 148, 154, 157, 167, 168, 169, 173, 174, 178, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 205, 208, 210, 212, 214, 217, 218, 223, 226, 227, 231, 239, 246, 247, 256, 261, 272, 274, 285, 286, 314, 315, 317, 318, 319, 320, 357, 358, 359, 360, 371, 378], "grad_tensor": [23, 25, 27, 29, 31, 33, 35, 37, 42, 44, 45, 48, 49, 51, 52, 61, 63, 65, 70, 72, 74, 76, 80, 82, 84, 86, 90, 92, 98, 100, 102, 105, 106, 108, 109, 112, 114, 117, 121, 130, 139, 141, 143, 145, 148, 150, 153, 168, 172, 174, 177, 182, 184, 186, 187, 189, 192, 193, 203, 204, 212, 217, 223, 225, 230, 244, 246, 249, 252, 254, 256, 258, 261, 267, 268, 272, 274, 279, 281, 283, 285, 288, 292, 294, 297, 299, 301, 304, 306, 308, 311, 313, 315, 317, 319, 326, 328, 330, 332, 351, 358, 360], "invers": [25, 27], "hyperbol": [25, 328], "cosin": [25, 27, 343], "input_tensor_b": [28, 29, 30, 31, 32, 33, 34, 35, 45, 47, 48, 52, 70, 83, 84, 85, 94, 115, 117, 126, 135, 147, 148, 154, 157, 167, 168, 169, 173, 174, 178, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 205, 208, 210, 212, 214, 217, 218, 223, 226, 227, 231, 239, 256, 272, 285, 286, 314, 315, 317, 318, 319, 320, 357, 358, 359, 360, 371, 378], "broadcast": [28, 29, 84, 94, 126, 135, 167, 169, 178, 190, 191, 194, 198, 205, 210, 223, 226, 227, 275, 314, 317, 320, 378], "tensor1": [28, 29, 30, 31, 32, 33, 34, 35, 45, 47, 48, 69, 70, 83, 84, 85, 94, 95, 106, 109, 115, 116, 117, 126, 127, 135, 136, 147, 148, 154, 157, 167, 168, 169, 170, 173, 174, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 205, 206, 208, 210, 212, 214, 217, 218, 223, 226, 227, 228, 230, 231, 239, 247, 249, 271, 272, 285, 286, 297, 311, 314, 315, 317, 318, 319, 320, 328, 357, 358, 359, 360], "tensor2": [28, 29, 30, 31, 32, 33, 34, 35, 45, 47, 48, 69, 70, 83, 84, 85, 94, 95, 115, 116, 117, 126, 127, 135, 136, 147, 148, 154, 157, 167, 168, 169, 170, 173, 174, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 205, 206, 208, 210, 212, 214, 217, 218, 223, 226, 227, 228, 231, 239, 271, 272, 285, 286, 314, 315, 317, 318, 319, 320, 357, 358, 359, 360], "gradient": [29, 31, 84, 92, 223, 285, 317, 319], "alpha": [30, 31, 33, 35, 62, 63, 84, 89, 90, 223, 287, 318, 319], "attr": [31, 70, 84, 117, 223, 246, 272, 285, 375], "input_tensor_c": [32, 33, 34, 35, 173, 174, 208, 357, 358], "tensor3": [32, 33, 34, 35, 173, 174, 208, 357, 358], "function_arg": [38, 69, 87, 88, 93, 110, 111, 124, 125, 166, 179, 213, 236, 237, 344, 345, 346, 353, 355, 361, 362], "function_kwarg": [38, 69, 87, 88, 93, 110, 111, 124, 125, 166, 179, 213, 236, 237, 344, 345, 346, 353, 355, 361, 362], "currenli": 39, "flatten": 39, "cache_file_nam": 40, "pathlib": 40, "serial": 40, "mesh_mapp": 40, "tensortomesh": 40, "use_device_til": 40, "toggl": 40, "mantissa": 40, "bfp": 40, "rais": [40, 376], "runtim": 40, "rte": 40, "bfp8": [40, 371], "bfp4": 40, "375": [40, 123, 374], "30469": [40, 123], "714844": [40, 123], "761719": [40, 123], "53125": [40, 123], "652344": [40, 123], "approxim": [52, 97, 99, 103, 129, 130, 282], "bias_gelu": 52, "int32_t": [53, 54, 55, 56, 57, 58, 128, 133, 264, 322], "int32": [53, 54, 55, 56, 57, 58, 374], "shift_bit": [54, 57], "2147483647": 55, "formula": [63, 90, 116, 139, 172, 204, 304, 306], "low": [64, 66, 111], "high": [64, 66, 110, 111, 378], "datayp": 67, "becom": [67, 336], "conjug": 71, "coregrid": [77, 178, 210, 372, 373], "corerang": 77, "shardstrategi": 77, "shardorient": 77, "halo": [77, 353], "use_height_and_width_as_shard_shap": 77, "travers": 77, "overlap": 77, "seen": [77, 371, 372], "forc": [78, 374], "accurate_mod": 83, "non": [83, 210, 233], "round_mod": [83, 84, 255, 256], "divid": [84, 340, 341], "assum": [87, 343, 355, 363], "form": [87, 355], "downsample_param": 87, "conv": [87, 376], "inxput_tensor": 91, "padding_idx": 91, "retriev": 91, "word": 91, "token": [91, 164, 343, 345, 375], "embeddings_typ": 91, "embeddingstyp": 91, "device_id": [91, 92, 209, 238, 336, 337, 371, 372, 373, 374, 376, 378], "rand": [91, 275, 371, 374, 376, 378], "106445": 91, "988281": 91, "59375": 91, "212891": 91, "964844": 91, "199219": 91, "996094": 91, "78362e": 91, "38": [91, 375], "89785e": 91, "39": [91, 371, 372, 373, 374, 375, 376, 377], "04479e": 91, "25815e": 91, "71833e": 91, "59995e": 91, "60398e": 91, "83671e": 91, "22242e": 91, "88263e": 91, "35917e": 91, "49994e": 91, "output_gradient_tensor": 92, "extract": 92, "vocabulari": 92, "seq_len": [92, 343], "embedding_dim": 92, "num_embed": 92, "1024": [92, 372], "3200": 92, "input_shap": [92, 376], "input_index": 92, "weights_shap": 92, "requires_grad": 92, "weights_ttnn": 92, "grad_shap": 92, "grad_data": 92, "input_a": [95, 116, 127, 136, 154, 170, 195, 199, 206, 228, 271], "input_b": [95, 116, 127, 136, 154, 170, 195, 199, 206, 228, 271], "fast_and_approximate_mod": [97, 99, 103, 129, 282], "exponenti": 106, "val_hi": [110, 111], "val_lo": [110, 111], "hone": [110, 111], "region": [110, 111], "wone": [110, 111], "desir": [110, 111, 337, 338], "nchw": 111, "rest": 111, "hfill": 111, "wfill": 111, "hi": 111, "lo": 111, "rounding_mod": [116, 255, 271], "wh_b0": [116, 271], "padded_shap": 118, "target_mem_config": [118, 119], "dimenst": [128, 133, 264, 322], "lambd": [138, 139, 305, 306], "16666667": [140, 142], "sfpu": 151, "shouldn": 151, "imaginari": 153, "batch_id": 154, "replac": 154, "rtol": 157, "atol": 157, "equal_nan": 157, "batch_index": 163, "update_index": 164, "batch_offset": 164, "input_refer": [165, 222], "input_predict": [165, 222], "lossreductionmod": [165, 222], "residual_input_tensor": 166, "program_config": [166, 178, 210, 340, 341], "programconfig": 166, "mcompar": 169, "slope": 171, "leaki": 171, "negative_slop": 172, "01": 172, "matmulprogramconfig": [178, 210], "devicecomputekernelconfig": [178, 210, 302, 343], "transpose_a": [178, 210], "transpose_b": [178, 210], "128": [178, 210, 378], "logarithm": 187, "inplac": [195, 197, 199, 376], "logitep": 204, "context": [209, 262, 263], "product": [210, 344, 345, 346], "prepend": 210, "behaviour": 210, "swap": 210, "j": 210, "implicitli": 210, "extend": 210, "patch": 210, "treat": 210, "accord": [210, 242, 273], "1d": [210, 233, 375], "_size": 210, "dot": [210, 344, 345, 346], "fix": 210, "upcom": 210, "0f": [211, 215, 216, 316, 321, 356], "in_n": 213, "nbatch": 213, "in_h": 213, "in_w": 213, "kernel_h": 213, "kernel_w": 213, "stride_h": 213, "stride_w": 213, "pad_h": [213, 241], "pad_w": [213, 241], "dilation_h": 213, "dilation_w": 213, "callabl": [220, 221], "parameterdict": [220, 221], "prefix": [220, 221], "run_model": [220, 376], "reader_patterns_cach": 220, "doesn": [220, 221, 371], "invalid": [220, 221], "preprocessor": [220, 221], "appear": [220, 221], "ttnn_module_arg": [220, 376], "dump": [220, 374, 377, 378], "tmp": [220, 371, 372, 373, 374, 376, 377], "model_graph": 220, "svg": [220, 376, 377, 378], "reader": 220, "taken": 225, "mvlgamma": 225, "5f": 225, "mutual": 240, "exclus": 240, "unpadded_shap": 241, "pad_c": 241, "pad_n": 241, "cartesian": 243, "theta": 243, "decim": [245, 278], "coeff": 247, "all_dimens": [251, 252], "irrespect": 251, "int64_t": 252, "radian": 254, "degre": 254, "divisor": 256, "upper_limit": 269, "cap": 269, "lower_limit": 270, "carri": 270, "modulu": 271, "repeat_dim": 273, "fit": 275, "expand": 275, "torch_input_tensor": [275, 376, 378], "torch_result": 275, "subtrah": 284, "minuend": 284, "subract": 285, "revers": 285, "0507": 287, "67326": 287, "88": [300, 377], "0310059": 302, "adjust": 303, "steep": 303, "steeper": 303, "hard": [303, 371], "softhrink": 305, "num_split": 309, "dim2": 309, "synchron": [324, 378], "associ": 324, "tangent": 328, "use_multicor": [333, 334, 352, 354], "whose": 336, "42188": 336, "398438": 336, "vice": 337, "versa": 337, "torch_rank": [338, 378], "Will": 338, "squeez": [338, 371, 372, 373, 374, 376], "reach": 338, "ttnn_tensor": [338, 371], "torch_tensor": [338, 371], "3008": 338, "8438": 338, "3242": 338, "9023": 338, "5820": 338, "5312": 338, "sort": 339, "sure": [339, 363, 378], "bfloat8": 339, "head_siz": [340, 341, 342, 347, 373], "attention_mask": [340, 341, 373], "softmaxprogramconfig": [340, 341], "softmaxdefaultprogramconfig": [340, 341], "causal_mask": [340, 341], "mask": [340, 341, 344, 345, 346], "causal": [340, 341, 344], "num_head": [342, 347, 373], "cos_cach": 343, "sin_cach": 343, "token_index": 343, "rotari": 343, "token_idx": 343, "transpos": [343, 347, 353], "head_dim": 343, "sine": 343, "mimick": 344, "flashattent": 344, "mqa": [344, 345], "q": [344, 345, 346], "nqh": 344, "dh": [344, 345, 346], "triangl": 344, "sdpaprogramconfig": 344, "length": [344, 345, 346], "decod": [345, 346, 377], "pnh": 345, "cur_po": [345, 346], "cur_pos_tensor": 345, "sdpamulticoreprogramconfig": [345, 346], "skip": [345, 371, 372, 373, 374, 375, 376, 377], "gqa": 346, "qh": 346, "transpose_q": 346, "share_cach": 346, "kv_input_tensor": 347, "num_kv_head": 347, "score": [347, 375], "q1": 347, "k1": 347, "qn": 347, "kn": 347, "vn": 347, "cat": [347, 373, 377], "transpose_kei": 347, "num": 347, "diagon": [348, 349], "use_pack_until": [352, 354], "padding_config": 353, "local_config": 353, "remote_config": 353, "pad_val": 353, "ncores_nhw": 353, "max_out_nsticks_per_cor": 353, "remote_read": 353, "transpose_mcast": 353, "remot": 353, "nhw": 353, "nstick": 353, "mcast": 353, "certain": 363, "ramp": 363, "skillset": 363, "tree": 363, "lab": 363, "port": 363, "8888": 363, "hint": 363, "Be": 363, "alwai": [363, 371, 374, 375], "cell": 363, "central": 371, "sens": 371, "sram": 371, "concept": 371, "2024": [371, 372, 373, 374, 375, 376, 377, 378], "07": [371, 372, 373, 374, 376], "18": [371, 372, 373, 374, 375, 377], "48": [371, 374, 375], "818": 371, "136": [371, 372, 373, 374], "cache_path": [371, 372, 373, 374, 376], "posixpath": [371, 372, 373, 374], "home": [371, 372, 373, 374, 375, 376, 377], "comparison_mode_pcc": [371, 372, 373, 374, 376, 378], "enable_comparison_mod": [371, 372, 373, 374, 376, 378], "enable_detailed_buffer_report": [371, 372, 373, 374, 376, 378], "enable_detailed_tensor_report": [371, 372, 373, 374, 376, 378], "enable_fast_runtime_mod": [371, 372, 373, 374, 376, 378], "enable_graph_report": [371, 372, 373, 374, 376, 378], "enable_log": [371, 372, 373, 374, 376, 378], "enable_model_cach": [371, 372, 373, 374, 376], "model_cache_path": [371, 372, 373, 374, 376], "report_nam": [371, 372, 373, 374, 376, 378], "root_report_path": [371, 372, 373, 374, 376], "throw_exception_on_fallback": [371, 372, 373, 374, 376], "tmp_dir": [371, 372, 373, 374, 376, 377], "905": 371, "operation_decor": [371, 372, 373, 374, 376], "758": [371, 372, 373, 374], "906": 371, "907": [371, 373], "908": [371, 373], "909": [371, 373], "910": [371, 373], "911": [371, 373], "914": [371, 373], "all_gath": [371, 372, 373, 374], "915": [371, 373], "pearson_correlation_coeffici": [371, 372, 373, 374, 376], "919": [371, 373], "920": 371, "921": [371, 373], "unsqueeze_to_4d": [371, 372, 373, 374, 376], "922": [371, 373], "923": [371, 373], "924": [371, 373], "925": [371, 373], "926": [371, 373], "allocate_tensor_on_devic": [371, 372, 373, 374, 376], "copy_host_to_device_tensor": [371, 372, 373, 374, 376], "927": [371, 373], "928": [371, 373], "929": [371, 373], "930": [371, 373], "931": [371, 373], "934": [371, 373], "935": 371, "936": [371, 373, 374], "937": 371, "938": [371, 373], "941": [371, 373], "942": [371, 373], "943": [371, 373], "948": [371, 373], "949": [371, 373, 374], "950": [371, 373], "951": [371, 373, 374], "952": [371, 373], "953": [371, 373], "954": [371, 373], "955": [371, 373], "958": [371, 373], "959": 371, "960": [371, 373, 374], "f": [371, 372, 373, 375, 377, 378], "As": [371, 372], "1234": 371, "again": 371, "action": 371, "98300": 371, "11301": 371, "37592": 371, "64318": 371, "53437": 371, "59434": 371, "69190": 371, "04268": 371, "33346": 371, "20231": 371, "15127": 371, "58303": 371, "00000": 371, "pick": 371, "float16": 371, "80078": 371, "69531": 371, "71484": 371, "33398": 371, "60156": 371, "36523": 371, "73047": 371, "90625": 371, "59766": 371, "83203": 371, "61719": 371, "53516": 371, "explicitli": 371, "most": 371, "effici": [371, 372], "transfer": 371, "constraint": 371, "nshape": 371, "nlayout": 371, "21680": 371, "24316": 371, "19336": 371, "40625": 371, "81641": 371, "50781": 371, "09961": 371, "54688": 371, "70703": 371, "93359": 371, "06787": 371, "75781": 371, "insert": 371, "cale": 371, "info": [371, 372, 373, 374, 375, 376, 377], "49": [371, 375, 377], "027": [371, 372], "silicondriv": [371, 372, 373, 374, 375, 376], "detect": [371, 372, 373, 374, 375, 376], "040": 371, "init_detect_tt_device_numanod": [371, 372, 373, 374, 375, 376], "numanodeset": [371, 372, 373, 374, 375, 376], "physical_device_id": [371, 372, 373, 374, 375, 376], "pci_bus_id": [371, 372, 373, 374, 375, 376], "0000": [371, 372, 373, 374, 375, 376], "00": [371, 372, 373, 374, 375, 376, 377], "041": 371, "bind_area_memory_nodeset": [371, 372, 373, 374, 375, 376], "unabl": [371, 372, 373, 374, 375, 376], "numanod": [371, 372, 373, 374, 375, 376], "membind": [371, 372, 373, 374, 375, 376], "ttsilicondevic": [371, 372, 373, 374, 375, 376], "init_hugepag": [371, 372, 373, 374, 375, 376], "bind_area_to_memory_nodeset": [371, 372, 373, 374, 375, 376], "fail": [371, 372, 373, 374, 375, 376], "ch": [371, 372, 373, 374, 375, 376], "effect": [371, 372, 373, 374, 375, 376], "decreas": [371, 372, 373, 374, 375, 376], "893": [371, 372, 373, 374, 375, 376], "082": 371, "ethernet": [371, 372, 373, 374, 376], "ai": [371, 372, 373, 374, 375, 376], "clk": [371, 372, 373, 374, 375, 376], "800": [371, 372, 373, 374], "mhz": [371, 372, 373, 374, 375, 376], "torch_input_tensor_a": [371, 378], "torch_input_tensor_b": [371, 378], "therefor": [371, 373], "stai": 371, "unless": [371, 378], "explicit": 371, "obviou": 371, "figur": 371, "hang": 371, "properli": 371, "41": [372, 375], "903": 372, "989": [372, 373], "990": [372, 373], "991": 372, "992": 372, "993": 372, "994": 372, "995": 372, "996": 372, "001": 372, "002": 372, "003": 372, "004": 372, "005": 372, "006": 372, "007": 372, "008": 372, "009": 372, "010": 372, "011": 372, "012": 372, "013": 372, "015": 372, "016": 372, "017": 372, "018": 372, "020": 372, "021": 372, "022": 372, "028": 372, "029": 372, "030": 372, "031": 372, "032": 372, "033": 372, "035": 372, "036": 372, "037": 372, "053": 372, "066": 372, "067": 372, "094": 372, "repeatedli": 372, "enable_program_cach": [372, 373, 378], "torch_a": 372, "torch_b": 372, "longer": 372, "signific": 372, "aslo": 372, "why": 372, "conver": 372, "todo": 372, "75000": 372, "25000": 372, "50000": 372, "62500": 372, "effeci": 372, "enjoi": 372, "massiv": 372, "dispatch_core_typ": [373, 374], "dispatchcoretyp": [373, 374], "eth": [373, 374], "l1_small_siz": [373, 374, 376], "8192": [373, 374], "54": [373, 375], "821": 373, "912": 373, "939": 373, "976": 373, "55": [373, 375], "014": 373, "fashion": 373, "multi_head_attent": 373, "query_weight": 373, "query_bia": 373, "key_weight": 373, "key_bia": 373, "value_weight": 373, "value_bia": 373, "output_weight": 373, "output_bia": 373, "fallback_reshap": 373, "get_fallback_funct": [373, 378], "attention_scor": 373, "attention_prob": 373, "context_lay": 373, "self_output": 373, "torch_attention_mask": [373, 374], "torch_query_weight": 373, "torch_query_bia": 373, "torch_key_weight": 373, "torch_key_bia": 373, "torch_value_weight": 373, "torch_value_bia": 373, "torch_output_weight": 373, "torch_output_bia": 373, "00607705116272": 373, "250946044921875": 373, "ahead": 373, "optimized_multi_head_attent": 373, "fused_qkv_weight": 373, "fused_qkv_bia": 373, "self_output_weight": 373, "self_output_bia": 373, "fused_qkv_output": 373, "context_layer_after_concatenate_head": 373, "qkv": 373, "torch_qkv_weight": 373, "torch_qkv_bia": 373, "qkv_weight": 373, "qkv_bia": 373, "optimized_output": 373, "474989175796509": 373, "020017147064208984": 373, "magnitud": 373, "torch_optimized_output": 373, "assert": [373, 378], "allclos": 373, "19": [373, 375, 377], "ttnn_config_overrid": [374, 376, 378], "47": [374, 375], "183": 374, "133": [374, 375], "load": [374, 375, 377], "overrid": [374, 375, 378], "184": 374, "354": 374, "355": 374, "356": 374, "357": 374, "358": 374, "359": 374, "360": [374, 377], "362": 374, "366": 374, "367": 374, "368": 374, "369": 374, "370": 374, "371": 374, "372": 374, "373": [374, 375], "374": 374, "378": 374, "379": 374, "380": [374, 375], "381": 374, "383": 374, "385": 374, "390": 374, "391": 374, "392": 374, "393": 374, "394": 374, "395": 374, "396": 374, "397": 374, "398": 374, "399": 374, "set_verbosity_error": 374, "100": [374, 375], "412": 374, "442": 374, "447": 374, "googl": [374, 377], "bert_uncased_l": 374, "4_h": 374, "256_a": 374, "bertselfoutput": 374, "site": [374, 375, 377], "huggingface_hub": [374, 375], "file_download": 374, "1132": 374, "futurewarn": 374, "resume_download": 374, "resum": 374, "force_download": 374, "874": 374, "num_hidden_lay": 374, "bertforquestionansw": 374, "input_id": 374, "vocab_s": 374, "torch_token_type_id": 374, "torch_position_id": 374, "ttnn_bert_input": 374, "preprocess_input": 374, "bert_for_question_answ": 374, "50": 374, "339": 374, "manage_config": [374, 376, 378], "144": 374, "340": 374, "341": 374, "555": 374, "_paramet": [374, 376], "env": [374, 375, 376, 377], "34": [374, 375, 376], "343": 374, "634": 374, "636": 374, "147": [374, 375], "restor": [374, 376], "02": [374, 375], "947": 374, "bash": 375, "unset": 375, "silent": 375, "nuke": 375, "jobserv": 375, "unavail": 375, "j1": 375, "parent": 375, "rule": 375, "artifact": 375, "conf": 375, "backend": 375, "pypi": [375, 377], "satisfi": [375, 377], "setuptool": 375, "44": 375, "py3": 375, "kb": 375, "statu": 375, "metadata": [375, 377], "click": 375, "loguru": 375, "58": 375, "ipywidget": 375, "139": 375, "90": [375, 376], "db": 375, "290ab3a34f2ef0b5a0f89235dc2d40fea83e77de84ed2dc05c": 375, "pyyaml": [375, 377], "cp38": 375, "linux_x86_64": 375, "jupyterlab": 375, "mb": 375, "pyelftool": 375, "py2": 375, "174": 375, "45": 375, "4f": 375, "ed": 375, "863cf4386fe6db3c09333712009ec1c5146a36f3904b469d13": 375, "curtsi": 375, "91": 375, "b7": 375, "0c117d73912c6c2beb1eb0d7d6884f4e79e6e5b5e91eeb34f5": 375, "torchtrail": 375, "manylinux_2_12_x86_64": 375, "manylinux2010_x86_64": 375, "matplotlib": 375, "toolz": 375, "pillow": [375, 377], "manylinux_2_17_x86_64": 375, "manylinux2014_x86_64": 375, "panda": 375, "2bcpu": 375, "199": 375, "dash": 375, "rich": 375, "238": 375, "seaborn": 375, "293": 375, "plotli": 375, "traitlet": 375, "85": 375, "widgetsnbextens": 375, "ipython": [375, 376, 377], "798": 375, "widget": 375, "jupyterlab_widget": 375, "215": 375, "comm": 375, "async": 375, "lru": 375, "async_lru": 375, "tomli": 375, "python_vers": 375, "server": 375, "jupyter_serv": 375, "jinja2": [375, 377], "ipykernel": 375, "116": 375, "shim": 375, "notebook_shim": 375, "jupyterlab_serv": 375, "lsp": 375, "jupyter_lsp": 375, "68": 375, "23": 375, "53": 375, "importlib": [375, 377], "resourc": 375, "importlib_resourc": 375, "importlib_metadata": 375, "jupyter_cor": 375, "tornado": 375, "abi3": 375, "manylinux_2_5_x86_64": 375, "manylinux1_x86_64": 375, "435": 375, "bless": 375, "cwcwidth": 375, "92": 375, "pyrsist": 375, "121": 375, "pypars": 375, "103": 375, "kiwisolv": 375, "contourpi": 375, "301": 375, "fonttool": 375, "22": 375, "dateutil": 375, "python_dateutil": 375, "247": [375, 376], "cycler": 375, "pytz": 375, "2020": 375, "505": 375, "extens": [375, 377], "typing_extens": 375, "compon": 375, "dash_html_compon": 375, "dash_tabl": 375, "flask": 375, "101": 375, "dash_core_compon": 375, "pygment": 375, "markdown": 375, "markdown_it_pi": 375, "84": 375, "tenac": 375, "24": [375, 377], "pickleshar": 375, "prompt": 375, "toolkit": 375, "37": 375, "prompt_toolkit": 375, "43": 375, "386": 375, "stack": 375, "stack_data": 375, "backcal": 375, "jedi": 375, "pexpect": 375, "sys_platform": 375, "win32": 375, "inlin": 375, "matplotlib_inlin": 375, "send2trash": 375, "anyio": 375, "termin": 375, "jupyter_server_termin": 375, "client": 375, "jupyter_cli": 375, "105": 375, "nbformat": 375, "77": 375, "nbconvert": 375, "257": [375, 376], "event": 375, "jupyter_ev": 375, "websocket": 375, "websocket_cli": 375, "pyzmq": 375, "prometheu": 375, "prometheus_cli": 375, "argon2": 375, "cffi": 375, "argon2_cffi": 375, "terminado": 375, "markupsaf": [375, 377], "26": 375, "nest": 375, "asyncio": 375, "nest_asyncio": 375, "psutil": 375, "cp36": 375, "288": 375, "debugpi": 375, "babel": 375, "62": 375, "jsonschema": 375, "21": [375, 377], "json5": 375, "zipp": [375, 377], "platformdir": 375, "six": 375, "wcwidth": 375, "itsdanger": 375, "blinker": 375, "werkzeug": 375, "226": 375, "mdurl": 375, "pure": 375, "pure_ev": 375, "asttoken": 375, "parso": 375, "ptyprocess": 375, "exceptiongroup": 375, "idna": [375, 377], "61": 375, "sniffio": 375, "fastjsonschema": 375, "defusedxml": 375, "beautifulsoup4": 375, "jupyterlab_pyg": 375, "pandocfilt": 375, "mistun": 375, "tinycss2": 375, "bleach": 375, "162": 375, "nbclient": 375, "rfc3986": 375, "rfc3986_valid": 375, "json": [375, 378], "logger": 375, "python_json_logg": 375, "rfc3339": 375, "rfc3339_valid": 375, "argon2_cffi_bind": 375, "86": 375, "urllib3": [375, 377], "charset": [375, 377], "charset_norm": 375, "141": 375, "certifi": [375, 377], "163": 375, "03": [375, 376], "jsonschema_specif": 375, "pkgutil": 375, "resolv": 375, "pkgutil_resolve_nam": 375, "60": 375, "rpd": 375, "rpds_py": 375, "soupsiev": 375, "36": 375, "webencod": 375, "444": 375, "pycpars": 375, "118": 375, "pre_commit": 375, "202": 375, "black": 375, "twine": 375, "yamllint": 375, "docutil": 375, "570": 375, "sphinx": 375, "rtd": 375, "theme": 375, "sphinx_rtd_them": 375, "sphinxcontrib": 375, "email": 375, "sphinxcontrib_email": 375, "lxml": 375, "manylinux_2_24_x86_64": 375, "breath": 375, "35": 375, "nbsphinx": 375, "jqueri": 375, "sphinxcontrib_jqueri": 375, "line": 375, "3a": 375, "a8": 375, "3237a93e3a6261bd24edabf3277ca59f64c1710b3d8c7c72a0": 375, "317": 375, "timeout": 375, "pytest_timeout": 375, "6c": 375, "40": 375, "5706d21e6b4dff52e7af12bff9ca126a3f15beb4dff386aa29": 375, "jsbeautifi": 375, "dataset": 375, "462": 375, "xlsxwriter": 375, "152": 375, "tiktoken": 375, "tqdm": [375, 377], "sentencepiec": 375, "numba": 375, "56": [375, 376], "librosa": 375, "252": [375, 376], "timm": [375, 377], "549": 375, "opencv": 375, "headless": 375, "74": 375, "opencv_python_headless": 375, "cp37": 375, "diffus": [375, 377], "604": 375, "219": 375, "ftfy": 375, "gitpython": 375, "188": 375, "einop": 375, "multiprocess": 375, "70": 375, "py38": 375, "132": 375, "evalu": 375, "81": 375, "bert_scor": 375, "fsspec": [375, 377], "173": 375, "nodeenv": 375, "cfgv": 375, "98": 375, "virtualenv": 375, "pathspec": 375, "mypi": 375, "mypy_extens": 375, "pyproject_hook": 375, "render": 375, "readme_render": 375, "pkginfo": 375, "toolbelt": 375, "requests_toolbelt": 375, "keyr": 375, "images": 375, "serializinghtml": 375, "sphinxcontrib_serializinghtml": 375, "94": 375, "jsmath": 375, "sphinxcontrib_jsmath": 375, "snowballstemm": 375, "93": [375, 376], "htmlhelp": 375, "sphinxcontrib_htmlhelp": 375, "alabast": 375, "applehelp": 375, "sphinxcontrib_applehelp": 375, "devhelp": 375, "sphinxcontrib_devhelp": 375, "qthelp": 375, "sphinxcontrib_qthelp": 375, "ply": 375, "plumbum": 375, "iniconfig": 375, "pluggi": 375, "0rc8": 375, "editorconfig": 375, "respons": 375, "pyarrow": 375, "xxhash": 375, "194": 375, "huggingfac": [375, 377], "hub": [375, 377], "330": 375, "aiohttp": 375, "dill": 375, "110": 375, "regex": [375, 377], "2019": [375, 377], "777": 375, "filelock": [375, 377], "llvmlite": 375, "0dev0": 375, "soxr": 375, "soundfil": 375, "pooch": 375, "lazi": 375, "loader": 375, "lazy_load": 375, "scipi": 375, "joblib": 375, "302": 375, "audioread": 375, "scikit": 375, "scikit_learn": 375, "msgpack": 375, "534": 375, "gitdb": 375, "distlib": 375, "468": 375, "nh3": 375, "secretstorag": 375, "linux": 375, "jeepnei": 375, "jaraco": 375, "frozenlist": 375, "240": [375, 376], "async_timeout": 375, "aiosign": 375, "yarl": 375, "308": 375, "multidict": 375, "129": 375, "threadpoolctl": 375, "smmap": 375, "cryptographi": 375, "itertool": 375, "more_itertool": 375, "57": 375, "pyproject": 375, "uninstal": 375, "msg": 375, "t5": 375, "integration_test": 375, "test_perform": 375, "test_t5_for_conditional_gener": 375, "functional_t5": 375, "ttnn_functional_t5": 375, "small": 375, "09": [375, 376], "ops_devic": 375, "session": 375, "cachedir": 375, "pytest_cach": 375, "rootdir": 375, "configfil": 375, "ini": 375, "plugin": 375, "600": 375, "func_onli": 375, "670": 375, "681": 375, "08": 375, "684": [375, 376], "1202": 375, "llruntim": 375, "watcher": 375, "watch": 375, "109": 375, "465": 375, "save": [375, 377], "ttnn_t5": 375, "6ba823894": 375, "149": 375, "484": 375, "487": 375, "216": 375, "489": 375, "721": 375, "359902381896973": 375, "07123565673828": 375, "722": 375, "102": 375, "44269247283137575": 375, "detach": 375, "short": [375, 378], "summari": 375, "627": 375, "638": 375, "639": 375, "458": 375, "224": 375, "460": 375, "292": 375, "164": 375, "22393798828125": 375, "165": 375, "322504758834839": 375, "407821983919596": 375, "pd": 375, "glob": 375, "getenv": 375, "get_latest_report": 375, "base_path": 375, "latest_dir": 375, "listdir": 375, "isdir": 375, "getmtim": 375, "valueerror": [375, 376], "latest_profile_report": 375, "df": 375, "read_csv": 375, "2024_02_09_01_38_37": 375, "ops_perf_results_resnet_2024_02_09_01_38_37": 375, "output_0_w": 375, "output_0_z": 375, "output_0_i": 375, "output_0_x": 375, "output_0_layout": 375, "output_0_data": 375, "output_0_memori": 375, "depth": 375, "compileprogram": 375, "load_tensor_ttnn": 375, "nan": 375, "137428381893955": 375, "137428382188762": 375, "294807": 375, "137428382500949": 375, "137428399402163": 375, "16901214": 375, "137428399802068": 375, "137428399873758": 375, "71690": 375, "137428400102635": 375, "137428400351033": 375, "248398": 375, "137428400548071": 375, "137428400792528": 375, "244457": 375, "4391": 375, "reshape_ttnn": 375, "4392": 375, "137450414555424": 375, "137450414599894": 375, "44470": 375, "4393": 375, "137450414740752": 375, "137450414782422": 375, "41670": 375, "4394": 375, "bcast_batch": 375, "tt_me": 375, "108": 375, "matmulparallelizationstrategi": 375, "multi_cor": 375, "137450414881851": 375, "137450414983440": 375, "101589": 375, "32128": 375, "dev_0_dram_interleav": 375, "4395": 375, "137450415113099": 375, "137450415158748": 375, "45649": 375, "from_device_ttnn": 375, "4396": 375, "137450415235897": 375, "137450453493048": 375, "38257151": 375, "fold_batch_norm2d_into_conv2d": 376, "168": 376, "82": 376, "768": 376, "242": 376, "conv1d": 376, "246": 376, "248": 376, "249": 376, "250": [376, 377], "251": 376, "253": 376, "254": 376, "255": 376, "256": [376, 377], "258": 376, "262": 376, "avg_pool2d": 376, "266": 376, "268": 376, "269": 376, "device_param": 376, "24576": 376, "310": 376, "324": 376, "325": 376, "363": 376, "conv3x3": 376, "in_plan": 376, "out_plan": 376, "3x3": 376, "torchbasicblock": 376, "expans": 376, "inplan": 376, "base_width": 376, "norm_lay": 376, "basicblock": 376, "notimplementederror": 376, "conv1": 376, "bn1": 376, "conv2": 376, "bn2": 376, "torch_model": 376, "state_dict": [376, 377], "create_custom_preprocessor": 376, "conv_weight_1": 376, "conv_bias_1": 376, "conv_weight_2": 376, "conv_bias_2": 376, "682": 376, "683": 376, "499": 376, "_initialize_model_and_preprocess_paramet": 376, "449": 376, "717": 376, "718": 376, "model_resnet_block_graph": 376, "conv_param": 376, "act_block_h": 376, "reshard": 376, "height_shard": 376, "shard_layout": 376, "block_shard": 376, "__call__": 376, "conv_config": 376, "conv2dconfig": 376, "weights_dtyp": 376, "fp32_dest_acc_en": 376, "packer_l1_accum_en": 376, "input_channels_align": 376, "deallocate_activ": 376, "act_block_h_overrid": 376, "_out_height": 376, "_out_width": 376, "weight_tensor": 376, "bias_tensor": 376, "input_height": 376, "input_width": 376, "ttnnbasicblock": 376, "get_memory_config": 376, "ttnn_model": 376, "12638": 376, "walk": 377, "mirror": 377, "colab": 377, "research": 377, "run_dit": 377, "ipynb": 377, "tab": 377, "ov": 377, "assumpt": 377, "chdir": 377, "content": 377, "upgrad": 377, "save_imag": 377, "create_diffus": 377, "autoencoderkl": 377, "find_model": 377, "collis": 377, "pil": 377, "set_grad_en": 377, "cuda": 377, "is_avail": 377, "gpu": 377, "322": 377, "fatal": 377, "destin": 377, "date": 377, "safetensor": 377, "sympi": 377, "mpmath": 377, "image_s": 377, "512": 377, "vae_model": 377, "stabilityai": 377, "sd": 377, "vae": 377, "ft": 377, "ema": 377, "mse": 377, "latent_s": 377, "input_s": 377, "pt": 377, "load_state_dict": 377, "seed": 377, "num_sampling_step": 377, "slider": 377, "cfg_scale": 377, "class_label": 377, "207": 377, "387": 377, "974": 377, "979": 377, "417": 377, "279": 377, "samples_per_row": 377, "nois": 377, "len": 377, "classifi": 377, "free": 377, "guidanc": 377, "y_null": 377, "model_kwarg": 377, "p_sample_loop": 377, "forward_with_cfg": 377, "clip_denois": 377, "null": 377, "18215": 377, "file_nam": [377, 378], "dit_model_graph": 377, "png": 377, "nrow": 377, "value_rang": 377, "987": 377, "210": 377, "show_svg": 377, "snippet": 378, "view": 378, "unlik": 378, "start_tim": 378, "end_tim": 378, "stdout": 378, "6391518115997314": 378, "0007393360137939453": 378, "9998": 378, "too": 378, "exp_trac": 378, "substitut": 378, "disk": 378, "implementaiton": 378, "addition": 378, "ttnn_config_path": 378, "2048": 378, "app": 378, "pre_hook_to_print_args_and_kwarg": 378, "post_hook_to_print_output": 378, "query_registered_oper": 378, "altern": 378, "begin_graph_captur": 378, "runmod": 378, "no_dispatch": 378, "captured_graph": 378, "end_graph_captur": 378, "pretty_print": 378, "ttnn_sweep": 379}, "objects": {"tt_lib.fallback_ops": [[13, 0, 1, "", "AdaptiveAvgPool2d"], [13, 0, 1, "", "BatchNorm2d"], [13, 0, 1, "", "Conv2d"], [13, 0, 1, "", "GroupNorm"], [13, 0, 1, "", "LayerNorm"], [13, 0, 1, "", "MaxPool2d"], [13, 0, 1, "", "binary_bitwise_and"], [13, 0, 1, "", "binary_bitwise_left_shift"], [13, 0, 1, "", "binary_bitwise_or"], [13, 0, 1, "", "binary_bitwise_right_shift"], [13, 0, 1, "", "binary_bitwise_xor"], [13, 0, 1, "", "binary_fmod"], [13, 0, 1, "", "bitwise_not"], [13, 0, 1, "", "ceil"], [13, 1, 1, "", "chunk"], [13, 1, 1, "", "concat"], [13, 1, 1, "", "conv2d"], [13, 0, 1, "", "floor"], [13, 1, 1, "", "full"], [13, 1, 1, "", "group_norm"], [13, 1, 1, "", "interpolate"], [13, 1, 1, "", "layer_norm"], [13, 1, 1, "", "pad"], [13, 1, 1, "", "repeat"], [13, 1, 1, "", "repeat_interleave"], [13, 1, 1, "", "reshape"], [13, 1, 1, "", "silu"], [13, 1, 1, "", "softmax"], [13, 1, 1, "", "tensor_slice"], [13, 0, 1, "", "torch_argmax"], [13, 0, 1, "", "torch_argmin"], [13, 0, 1, "", "trunc"], [13, 0, 1, "", "unary_bitwise_and"], [13, 0, 1, "", "unary_bitwise_left_shift"], [13, 0, 1, "", "unary_bitwise_or"], [13, 0, 1, "", "unary_bitwise_right_shift"], [13, 0, 1, "", "unary_bitwise_xor"], [13, 0, 1, "", "unary_fmod"]], "tt_lib.fused_ops.add_and_norm": [[13, 1, 1, "", "AddAndNorm"]], "tt_lib.fused_ops.layernorm": [[13, 1, 1, "", "Layernorm"]], "tt_lib.fused_ops.linear": [[13, 1, 1, "", "Linear"]], "tt_lib.operations.primary": [[13, 1, 1, "", "moreh_groupnorm"], [13, 1, 1, "", "moreh_groupnorm_backward"], [13, 1, 1, "", "moreh_logsoftmax"], [13, 1, 1, "", "moreh_logsoftmax_backward"], [13, 1, 1, "", "moreh_norm"], [13, 1, 1, "", "moreh_norm_backward"], [13, 1, 1, "", "moreh_softmax"], [13, 1, 1, "", "moreh_softmax_backward"], [13, 1, 1, "", "moreh_softmin"], [13, 1, 1, "", "moreh_softmin_backward"]], "ttnn": [[13, 0, 1, "", "BcastOpDim"], [13, 0, 1, "", "BcastOpMath"], [19, 1, 1, "", "GetDefaultDevice"], [12, 0, 1, "", "MemoryConfig"], [21, 1, 1, "", "SetDefaultDevice"], [18, 0, 1, "", "Shape"], [12, 0, 1, "", "Tensor"], [22, 1, 1, "", "abs"], [23, 1, 1, "", "abs_bw"], [24, 1, 1, "", "acos"], [25, 1, 1, "", "acos_bw"], [26, 1, 1, "", "acosh"], [27, 1, 1, "", "acosh_bw"], [28, 1, 1, "", "add"], [29, 1, 1, "", "add_bw"], [30, 1, 1, "", "addalpha"], [31, 1, 1, "", "addalpha_bw"], [32, 1, 1, "", "addcdiv"], [33, 1, 1, "", "addcdiv_bw"], [34, 1, 1, "", "addcmul"], [35, 1, 1, "", "addcmul_bw"], [36, 1, 1, "", "angle"], [37, 1, 1, "", "angle_bw"], [38, 1, 1, "", "arange"], [39, 1, 1, "", "argmax"], [40, 1, 1, "", "as_tensor"], [41, 1, 1, "", "asin"], [42, 1, 1, "", "asin_bw"], [43, 1, 1, "", "asinh"], [44, 1, 1, "", "asinh_bw"], [45, 1, 1, "", "assign_bw"], [46, 1, 1, "", "atan"], [47, 1, 1, "", "atan2"], [48, 1, 1, "", "atan2_bw"], [49, 1, 1, "", "atan_bw"], [50, 1, 1, "", "atanh"], [51, 1, 1, "", "atanh_bw"], [52, 1, 1, "", "bias_gelu_bw"], [53, 1, 1, "", "bitwise_and"], [54, 1, 1, "", "bitwise_left_shift"], [55, 1, 1, "", "bitwise_not"], [56, 1, 1, "", "bitwise_or"], [57, 1, 1, "", "bitwise_right_shift"], [58, 1, 1, "", "bitwise_xor"], [59, 1, 1, "", "cbrt"], [60, 1, 1, "", "ceil"], [61, 1, 1, "", "ceil_bw"], [62, 1, 1, "", "celu"], [63, 1, 1, "", "celu_bw"], [64, 1, 1, "", "clamp"], [65, 1, 1, "", "clamp_bw"], [66, 1, 1, "", "clip"], [67, 1, 1, "", "clone"], [68, 1, 1, "", "close_device"], [69, 1, 1, "", "concat"], [70, 1, 1, "", "concat_bw"], [71, 1, 1, "", "conj"], [72, 1, 1, "", "conj_bw"], [73, 1, 1, "", "cos"], [74, 1, 1, "", "cos_bw"], [75, 1, 1, "", "cosh"], [76, 1, 1, "", "cosh_bw"], [77, 1, 1, "", "create_sharded_memory_config"], [78, 1, 1, "", "deallocate"], [79, 1, 1, "", "deg2rad"], [80, 1, 1, "", "deg2rad_bw"], [81, 1, 1, "", "digamma"], [82, 1, 1, "", "digamma_bw"], [83, 1, 1, "", "div"], [84, 1, 1, "", "div_bw"], [85, 1, 1, "", "div_no_nan"], [86, 1, 1, "", "div_no_nan_bw"], [87, 1, 1, "", "downsample"], [88, 1, 1, "", "dump_tensor"], [89, 1, 1, "", "elu"], [90, 1, 1, "", "elu_bw"], [91, 1, 1, "", "embedding"], [92, 1, 1, "", "embedding_bw"], [93, 1, 1, "", "empty"], [94, 1, 1, "", "eq"], [95, 1, 1, "", "eq_"], [96, 1, 1, "", "eqz"], [97, 1, 1, "", "erf"], [98, 1, 1, "", "erf_bw"], [99, 1, 1, "", "erfc"], [100, 1, 1, "", "erfc_bw"], [101, 1, 1, "", "erfinv"], [102, 1, 1, "", "erfinv_bw"], [103, 1, 1, "", "exp"], [104, 1, 1, "", "exp2"], [105, 1, 1, "", "exp2_bw"], [106, 1, 1, "", "exp_bw"], [107, 1, 1, "", "expm1"], [108, 1, 1, "", "expm1_bw"], [109, 1, 1, "", "fill_bw"], [110, 1, 1, "", "fill_ones_rm"], [111, 1, 1, "", "fill_rm"], [112, 1, 1, "", "fill_zero_bw"], [113, 1, 1, "", "floor"], [114, 1, 1, "", "floor_bw"], [115, 1, 1, "", "floor_div"], [116, 1, 1, "", "fmod"], [117, 1, 1, "", "fmod_bw"], [118, 1, 1, "", "format_input_tensor"], [119, 1, 1, "", "format_output_tensor"], [120, 1, 1, "", "frac"], [121, 1, 1, "", "frac_bw"], [122, 1, 1, "", "from_device"], [123, 1, 1, "", "from_torch"], [124, 1, 1, "", "full"], [125, 1, 1, "", "full_like"], [126, 1, 1, "", "ge"], [127, 1, 1, "", "ge_"], [128, 1, 1, "", "geglu"], [129, 1, 1, "", "gelu"], [130, 1, 1, "", "gelu_bw"], [131, 1, 1, "", "gez"], [132, 1, 1, "", "global_avg_pool2d"], [133, 1, 1, "", "glu"], [134, 1, 1, "", "group_norm"], [135, 1, 1, "", "gt"], [136, 1, 1, "", "gt_"], [137, 1, 1, "", "gtz"], [138, 1, 1, "", "hardshrink"], [139, 1, 1, "", "hardshrink_bw"], [140, 1, 1, "", "hardsigmoid"], [141, 1, 1, "", "hardsigmoid_bw"], [142, 1, 1, "", "hardswish"], [143, 1, 1, "", "hardswish_bw"], [144, 1, 1, "", "hardtanh"], [145, 1, 1, "", "hardtanh_bw"], [146, 1, 1, "", "heaviside"], [147, 1, 1, "", "hypot"], [148, 1, 1, "", "hypot_bw"], [149, 1, 1, "", "i0"], [150, 1, 1, "", "i0_bw"], [151, 1, 1, "", "identity"], [152, 1, 1, "", "imag"], [153, 1, 1, "", "imag_bw"], [154, 1, 1, "", "indexed_fill"], [155, 1, 1, "", "is_imag"], [156, 1, 1, "", "is_real"], [157, 1, 1, "", "isclose"], [158, 1, 1, "", "isfinite"], [159, 1, 1, "", "isinf"], [160, 1, 1, "", "isnan"], [161, 1, 1, "", "isneginf"], [162, 1, 1, "", "isposinf"], [165, 1, 1, "", "l1_loss"], [166, 1, 1, "", "layer_norm"], [167, 1, 1, "", "ldexp"], [168, 1, 1, "", "ldexp_bw"], [169, 1, 1, "", "le"], [170, 1, 1, "", "le_"], [171, 1, 1, "", "leaky_relu"], [172, 1, 1, "", "leaky_relu_bw"], [173, 1, 1, "", "lerp"], [174, 1, 1, "", "lerp_bw"], [175, 1, 1, "", "lez"], [176, 1, 1, "", "lgamma"], [177, 1, 1, "", "lgamma_bw"], [178, 1, 1, "", "linear"], [179, 1, 1, "", "load_tensor"], [180, 1, 1, "", "log"], [181, 1, 1, "", "log10"], [182, 1, 1, "", "log10_bw"], [183, 1, 1, "", "log1p"], [184, 1, 1, "", "log1p_bw"], [185, 1, 1, "", "log2"], [186, 1, 1, "", "log2_bw"], [187, 1, 1, "", "log_bw"], [188, 1, 1, "", "log_sigmoid"], [189, 1, 1, "", "log_sigmoid_bw"], [190, 1, 1, "", "logaddexp"], [191, 1, 1, "", "logaddexp2"], [192, 1, 1, "", "logaddexp2_bw"], [193, 1, 1, "", "logaddexp_bw"], [194, 1, 1, "", "logical_and"], [195, 1, 1, "", "logical_and_"], [196, 1, 1, "", "logical_not"], [197, 1, 1, "", "logical_not_"], [198, 1, 1, "", "logical_or"], [199, 1, 1, "", "logical_or_"], [200, 1, 1, "", "logical_xor"], [201, 1, 1, "", "logical_xor_"], [202, 1, 1, "", "logit"], [203, 1, 1, "", "logit_bw"], [204, 1, 1, "", "logiteps_bw"], [205, 1, 1, "", "lt"], [206, 1, 1, "", "lt_"], [207, 1, 1, "", "ltz"], [208, 1, 1, "", "mac"], [209, 1, 1, "", "manage_device"], [210, 1, 1, "", "matmul"], [211, 1, 1, "", "max"], [212, 1, 1, "", "max_bw"], [213, 1, 1, "", "max_pool2d"], [214, 1, 1, "", "maximum"], [215, 1, 1, "", "mean"], [216, 1, 1, "", "min"], [217, 1, 1, "", "min_bw"], [218, 1, 1, "", "minimum"], [219, 1, 1, "", "mish"], [222, 1, 1, "", "mse_loss"], [223, 1, 1, "", "mul_bw"], [224, 1, 1, "", "multigammaln"], [225, 1, 1, "", "multigammaln_bw"], [226, 1, 1, "", "multiply"], [227, 1, 1, "", "ne"], [228, 1, 1, "", "ne_"], [229, 1, 1, "", "neg"], [230, 1, 1, "", "neg_bw"], [231, 1, 1, "", "nextafter"], [232, 1, 1, "", "nez"], [233, 1, 1, "", "nonzero"], [234, 1, 1, "", "normalize_global"], [235, 1, 1, "", "normalize_hw"], [236, 1, 1, "", "ones"], [237, 1, 1, "", "ones_like"], [238, 1, 1, "", "open_device"], [239, 1, 1, "", "outer"], [240, 1, 1, "", "pad"], [241, 1, 1, "", "pad_to_tile_shape"], [242, 1, 1, "", "permute"], [243, 1, 1, "", "polar"], [244, 1, 1, "", "polar_bw"], [245, 1, 1, "", "polygamma"], [246, 1, 1, "", "polygamma_bw"], [247, 1, 1, "", "polyval"], [248, 1, 1, "", "pow"], [249, 1, 1, "", "pow_bw"], [250, 1, 1, "", "prelu"], [251, 1, 1, "", "prod"], [252, 1, 1, "", "prod_bw"], [253, 1, 1, "", "rad2deg"], [254, 1, 1, "", "rad2deg_bw"], [255, 1, 1, "", "rdiv"], [256, 1, 1, "", "rdiv_bw"], [257, 1, 1, "", "real"], [258, 1, 1, "", "real_bw"], [259, 1, 1, "", "reallocate"], [260, 1, 1, "", "reciprocal"], [261, 1, 1, "", "reciprocal_bw"], [262, 1, 1, "", "register_post_operation_hook"], [263, 1, 1, "", "register_pre_operation_hook"], [264, 1, 1, "", "reglu"], [265, 1, 1, "", "relu"], [266, 1, 1, "", "relu6"], [267, 1, 1, "", "relu6_bw"], [268, 1, 1, "", "relu_bw"], [269, 1, 1, "", "relu_max"], [270, 1, 1, "", "relu_min"], [271, 1, 1, "", "remainder"], [272, 1, 1, "", "remainder_bw"], [273, 1, 1, "", "repeat"], [274, 1, 1, "", "repeat_bw"], [275, 1, 1, "", "repeat_interleave"], [276, 1, 1, "", "reshape"], [277, 1, 1, "", "rms_norm"], [278, 1, 1, "", "round"], [279, 1, 1, "", "round_bw"], [280, 1, 1, "", "rpow"], [281, 1, 1, "", "rpow_bw"], [282, 1, 1, "", "rsqrt"], [283, 1, 1, "", "rsqrt_bw"], [284, 1, 1, "", "rsub"], [285, 1, 1, "", "rsub_bw"], [286, 1, 1, "", "scatter"], [287, 1, 1, "", "selu"], [288, 1, 1, "", "selu_bw"], [289, 1, 1, "", "set_printoptions"], [290, 1, 1, "", "sigmoid"], [291, 1, 1, "", "sigmoid_accurate"], [292, 1, 1, "", "sigmoid_bw"], [293, 1, 1, "", "sign"], [294, 1, 1, "", "sign_bw"], [295, 1, 1, "", "signbit"], [296, 1, 1, "", "silu"], [297, 1, 1, "", "silu_bw"], [298, 1, 1, "", "sin"], [299, 1, 1, "", "sin_bw"], [300, 1, 1, "", "sinh"], [301, 1, 1, "", "sinh_bw"], [302, 1, 1, "", "softmax"], [303, 1, 1, "", "softplus"], [304, 1, 1, "", "softplus_bw"], [305, 1, 1, "", "softshrink"], [306, 1, 1, "", "softshrink_bw"], [307, 1, 1, "", "softsign"], [308, 1, 1, "", "softsign_bw"], [309, 1, 1, "", "split"], [310, 1, 1, "", "sqrt"], [311, 1, 1, "", "sqrt_bw"], [312, 1, 1, "", "square"], [313, 1, 1, "", "square_bw"], [314, 1, 1, "", "squared_difference"], [315, 1, 1, "", "squared_difference_bw"], [316, 1, 1, "", "std"], [317, 1, 1, "", "sub_bw"], [318, 1, 1, "", "subalpha"], [319, 1, 1, "", "subalpha_bw"], [320, 1, 1, "", "subtract"], [321, 1, 1, "", "sum"], [322, 1, 1, "", "swiglu"], [323, 1, 1, "", "swish"], [324, 1, 1, "", "synchronize_device"], [325, 1, 1, "", "tan"], [326, 1, 1, "", "tan_bw"], [327, 1, 1, "", "tanh"], [328, 1, 1, "", "tanh_bw"], [329, 1, 1, "", "tanhshrink"], [330, 1, 1, "", "tanhshrink_bw"], [331, 1, 1, "", "threshold"], [332, 1, 1, "", "threshold_bw"], [333, 1, 1, "", "tilize"], [334, 1, 1, "", "tilize_with_val_padding"], [334, 1, 1, "", "tilize_with_zero_padding"], [335, 1, 1, "", "to_device"], [336, 1, 1, "", "to_layout"], [337, 1, 1, "", "to_memory_config"], [338, 1, 1, "", "to_torch"], [339, 1, 1, "", "topk"], [348, 1, 1, "", "tril"], [349, 1, 1, "", "triu"], [350, 1, 1, "", "trunc"], [351, 1, 1, "", "trunc_bw"], [352, 1, 1, "", "untilize"], [353, 1, 1, "", "untilize_with_halo_v2"], [354, 1, 1, "", "untilize_with_unpadding"], [355, 1, 1, "", "upsample"], [356, 1, 1, "", "var"], [357, 1, 1, "", "where"], [358, 1, 1, "", "where_bw"], [359, 1, 1, "", "xlogy"], [360, 1, 1, "", "xlogy_bw"], [361, 1, 1, "", "zeros"], [362, 1, 1, "", "zeros_like"]], "ttnn.MemoryConfig": [[12, 2, 1, "", "__init__"]], "ttnn.Shape": [[18, 3, 1, "", "rank"], [18, 2, 1, "", "with_tile_padding"]], "ttnn.Tensor": [[12, 2, 1, "", "__init__"], [12, 2, 1, "", "buffer"], [13, 1, 1, "", "cpu"], [12, 2, 1, "", "device"], [12, 2, 1, "", "get_dtype"], [12, 2, 1, "", "get_layout"], [12, 2, 1, "", "get_legacy_shape"], [12, 2, 1, "", "pad"], [12, 2, 1, "", "pad_to_tile"], [12, 2, 1, "", "storage_type"], [12, 2, 1, "", "to"], [12, 2, 1, "", "unpad"], [12, 2, 1, "", "unpad_from_tile"]], "ttnn.experimental": [[343, 1, 1, "", "rotary_embedding"]], "ttnn.kv_cache": [[163, 1, 1, "", "fill_cache_for_user_"], [164, 1, 1, "", "update_cache_for_token_"]], "ttnn.model_preprocessing": [[220, 1, 1, "", "preprocess_model"], [221, 1, 1, "", "preprocess_model_parameters"]], "ttnn.transformer": [[340, 1, 1, "", "attention_softmax"], [341, 1, 1, "", "attention_softmax_"], [342, 1, 1, "", "concatenate_heads"], [344, 1, 1, "", "scaled_dot_product_attention"], [345, 1, 1, "", "scaled_dot_product_attention_decode"], [346, 1, 1, "", "scaled_dot_product_attention_decode_gqa"], [347, 1, 1, "", "split_query_key_value_and_split_heads"]]}, "objtypes": {"0": "py:class", "1": "py:function", "2": "py:method", "3": "py:property"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "function", "Python function"], "2": ["py", "method", "Python method"], "3": ["py", "property", "Python property"]}, "titleterms": {"welcom": 0, "tt": [0, 10, 12, 13, 14], "nn": [0, 14], "document": 0, "ttnn": [0, 5, 6, 8, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 367, 370, 371, 373, 374, 375, 376, 378], "model": [0, 4, 7, 8, 14, 373, 374, 377], "resourc": 0, "indic": 0, "tabl": 0, "contribut": 1, "develop": 1, "support": [2, 378], "report": [2, 7, 17], "bug": 2, "featur": [2, 5], "propos": 2, "request": 2, "troubleshoot": 2, "debug": [2, 378], "tip": 2, "commun": 2, "perform": [3, 372], "prerequisit": [3, 4], "run": [3, 4, 10, 373, 376, 378], "perf": [3, 17], "file": 3, "all": [3, 378], "get": [4, 14, 376], "start": [4, 14], "an": [4, 378], "exampl": [4, 6, 8, 10, 12, 378], "next": 4, "step": [4, 6, 8], "what": [5, 6], "i": [5, 6], "kei": 5, "ad": 6, "new": [6, 13, 16], "oper": [6, 7, 8, 13, 17, 365, 369, 371, 374, 375, 378], "faq": 6, "ar": 6, "need": 6, "add": [6, 28, 369, 371], "c": [6, 378], "python": [6, 378], "devic": [6, 7, 10, 13, 371, 372, 373, 378], "implement": [6, 373, 376], "1": [6, 8, 14, 378], "2": [6, 8, 13, 14, 377, 378], "bind": 6, "option": [6, 13], "golden": 6, "function": [6, 16, 374, 378], "api": [7, 12, 13, 18], "memori": [7, 18], "config": [7, 18, 372], "core": 7, "tensor": [7, 10, 12, 13, 18, 369, 371, 372, 378], "creation": 7, "matrix": [7, 372], "multipl": [7, 18, 372], "pointwis": 7, "unari": 7, "binari": 7, "ternari": 7, "loss": 7, "reduct": 7, "data": [7, 18, 371], "movement": 7, "normal": 7, "transform": [7, 340, 341, 342, 344, 345, 346, 347], "embed": [7, 91], "pool": 7, "vision": 7, "kv": 7, "cach": [7, 13, 372, 373, 378], "convers": 7, "hook": [7, 378], "convert": [8, 12, 371, 373, 378], "torch": [8, 364, 371, 372, 373, 374, 376, 378], "rewrit": 8, "switch": 8, "3": [8, 14, 378], "optim": [8, 14, 373], "more": [8, 372], "build": [9, 14, 377], "uplift": 9, "demo": [9, 14], "lib": [10, 13], "us": [10, 371, 372, 373, 374, 378], "one": 10, "op": 10, "from": [10, 13, 14, 376, 377, 378], "acceler": 10, "pytorch": [10, 12, 377], "odd": 10, "size": 10, "last": 10, "dim": 10, "depend": 11, "overview": [12, 13], "storag": [12, 18, 371], "memoryconfig": 12, "between": 12, "infrastructur": 13, "member": 13, "input": 13, "output": [13, 371, 372, 373], "profil": [13, 17, 367, 375], "fast": 13, "dispatch": 13, "program": [13, 372, 373, 378], "log": [13, 180, 378], "through": 13, "tt_lib": [13, 378], "primari": 13, "enum": 13, "fallback": 13, "experiment": [13, 343], "fuse": 13, "mini": 13, "graph": [13, 364, 376, 377, 378], "librari": [13, 377], "complex": 13, "type": [13, 18, 371], "instal": 14, "explor": 14, "our": 14, "tutori": [14, 363], "multi": [14, 366, 373], "head": [14, 366, 373], "attent": [14, 366, 373], "simpl": 14, "4": [14, 378], "where": [14, 357], "go": 14, "here": 14, "onboard": 16, "header": 17, "profile_thi": 17, "descript": 17, "shape": 18, "layout": [18, 371, 372], "requir": 18, "width": 18, "shard": 18, "getdefaultdevic": 19, "maxpool2d": 20, "setdefaultdevic": 21, "ab": 22, "abs_bw": 23, "aco": 24, "acos_bw": 25, "acosh": 26, "acosh_bw": 27, "add_bw": 29, "addalpha": 30, "addalpha_bw": 31, "addcdiv": 32, "addcdiv_bw": 33, "addcmul": 34, "addcmul_bw": 35, "angl": 36, "angle_bw": 37, "arang": 38, "argmax": 39, "as_tensor": 40, "asin": 41, "asin_bw": 42, "asinh": 43, "asinh_bw": 44, "assign_bw": 45, "atan": 46, "atan2": 47, "atan2_bw": 48, "atan_bw": 49, "atanh": 50, "atanh_bw": 51, "bias_gelu_bw": 52, "bitwise_and": 53, "bitwise_left_shift": 54, "bitwise_not": 55, "bitwise_or": 56, "bitwise_right_shift": 57, "bitwise_xor": 58, "cbrt": 59, "ceil": 60, "ceil_bw": 61, "celu": 62, "celu_bw": 63, "clamp": 64, "clamp_bw": 65, "clip": 66, "clone": [67, 377], "close_devic": 68, "concat": 69, "concat_bw": 70, "conj": 71, "conj_bw": 72, "co": 73, "cos_bw": 74, "cosh": 75, "cosh_bw": 76, "create_sharded_memory_config": 77, "dealloc": 78, "deg2rad": 79, "deg2rad_bw": 80, "digamma": 81, "digamma_bw": 82, "div": 83, "div_bw": 84, "div_no_nan": 85, "div_no_nan_bw": 86, "downsampl": 87, "dump_tensor": 88, "elu": 89, "elu_bw": 90, "embedding_bw": 92, "empti": 93, "eq": [94, 95], "eqz": 96, "erf": 97, "erf_bw": 98, "erfc": 99, "erfc_bw": 100, "erfinv": 101, "erfinv_bw": 102, "exp": 103, "exp2": 104, "exp2_bw": 105, "exp_bw": 106, "expm1": 107, "expm1_bw": 108, "fill_bw": 109, "fill_ones_rm": 110, "fill_rm": 111, "fill_zero_bw": 112, "floor": 113, "floor_bw": 114, "floor_div": 115, "fmod": 116, "fmod_bw": 117, "format_input_tensor": 118, "format_output_tensor": 119, "frac": 120, "frac_bw": 121, "from_devic": 122, "from_torch": 123, "full": 124, "full_lik": 125, "ge": [126, 127], "geglu": 128, "gelu": 129, "gelu_bw": 130, "gez": 131, "global_avg_pool2d": 132, "glu": 133, "group_norm": 134, "gt": [135, 136], "gtz": 137, "hardshrink": 138, "hardshrink_bw": 139, "hardsigmoid": 140, "hardsigmoid_bw": 141, "hardswish": 142, "hardswish_bw": 143, "hardtanh": 144, "hardtanh_bw": 145, "heavisid": 146, "hypot": 147, "hypot_bw": 148, "i0": 149, "i0_bw": 150, "ident": 151, "imag": 152, "imag_bw": 153, "indexed_fil": 154, "is_imag": 155, "is_real": 156, "isclos": 157, "isfinit": 158, "isinf": 159, "isnan": 160, "isneginf": 161, "isposinf": 162, "kv_cach": [163, 164], "fill_cache_for_user_": 163, "update_cache_for_token_": 164, "l1_loss": 165, "layer_norm": 166, "ldexp": 167, "ldexp_bw": 168, "le": [169, 170], "leaky_relu": 171, "leaky_relu_bw": 172, "lerp": 173, "lerp_bw": 174, "lez": 175, "lgamma": 176, "lgamma_bw": 177, "linear": 178, "load_tensor": 179, "log10": 181, "log10_bw": 182, "log1p": 183, "log1p_bw": 184, "log2": 185, "log2_bw": 186, "log_bw": 187, "log_sigmoid": 188, "log_sigmoid_bw": 189, "logaddexp": 190, "logaddexp2": 191, "logaddexp2_bw": 192, "logaddexp_bw": 193, "logical_and": [194, 195], "logical_not": [196, 197], "logical_or": [198, 199], "logical_xor": [200, 201], "logit": 202, "logit_bw": 203, "logiteps_bw": 204, "lt": [205, 206], "ltz": 207, "mac": 208, "manage_devic": 209, "matmul": [210, 365], "max": 211, "max_bw": 212, "max_pool2d": 213, "maximum": 214, "mean": 215, "min": 216, "min_bw": 217, "minimum": 218, "mish": 219, "model_preprocess": [220, 221], "preprocess_model": 220, "preprocess_model_paramet": 221, "mse_loss": 222, "mul_bw": 223, "multigammaln": 224, "multigammaln_bw": 225, "multipli": [226, 372], "ne": [227, 228], "neg": 229, "neg_bw": 230, "nextaft": 231, "nez": 232, "nonzero": 233, "normalize_glob": 234, "normalize_hw": 235, "ones": 236, "ones_lik": 237, "open_devic": 238, "outer": 239, "pad": 240, "pad_to_tile_shap": 241, "permut": 242, "polar": 243, "polar_bw": 244, "polygamma": 245, "polygamma_bw": 246, "polyv": 247, "pow": 248, "pow_bw": 249, "prelu": 250, "prod": 251, "prod_bw": 252, "rad2deg": 253, "rad2deg_bw": 254, "rdiv": 255, "rdiv_bw": 256, "real": 257, "real_bw": 258, "realloc": 259, "reciproc": 260, "reciprocal_bw": 261, "register_post_operation_hook": 262, "register_pre_operation_hook": 263, "reglu": 264, "relu": 265, "relu6": 266, "relu6_bw": 267, "relu_bw": 268, "relu_max": 269, "relu_min": 270, "remaind": 271, "remainder_bw": 272, "repeat": 273, "repeat_bw": 274, "repeat_interleav": 275, "reshap": 276, "rms_norm": 277, "round": 278, "round_bw": 279, "rpow": 280, "rpow_bw": 281, "rsqrt": 282, "rsqrt_bw": 283, "rsub": 284, "rsub_bw": 285, "scatter": 286, "selu": 287, "selu_bw": 288, "set_printopt": 289, "sigmoid": 290, "sigmoid_accur": 291, "sigmoid_bw": 292, "sign": 293, "sign_bw": 294, "signbit": 295, "silu": 296, "silu_bw": 297, "sin": 298, "sin_bw": 299, "sinh": 300, "sinh_bw": 301, "softmax": 302, "softplu": 303, "softplus_bw": 304, "softshrink": 305, "softshrink_bw": 306, "softsign": 307, "softsign_bw": 308, "split": 309, "sqrt": 310, "sqrt_bw": 311, "squar": 312, "square_bw": 313, "squared_differ": 314, "squared_difference_bw": 315, "std": 316, "sub_bw": 317, "subalpha": 318, "subalpha_bw": 319, "subtract": 320, "sum": 321, "swiglu": 322, "swish": 323, "synchronize_devic": 324, "tan": 325, "tan_bw": 326, "tanh": 327, "tanh_bw": 328, "tanhshrink": 329, "tanhshrink_bw": 330, "threshold": 331, "threshold_bw": 332, "tiliz": 333, "tilize_with_val_pad": 334, "to_devic": 335, "to_layout": 336, "to_memory_config": 337, "to_torch": 338, "topk": 339, "attention_softmax": 340, "attention_softmax_": 341, "concatenate_head": 342, "rotary_embed": 343, "scaled_dot_product_attent": 344, "scaled_dot_product_attention_decod": 345, "scaled_dot_product_attention_decode_gqa": 346, "split_query_key_value_and_split_head": 347, "tril": 348, "triu": 349, "trunc": 350, "trunc_bw": 351, "until": 352, "untilize_with_halo_v2": 353, "untilize_with_unpad": 354, "upsampl": 355, "var": 356, "where_bw": 358, "xlogi": 359, "xlogy_bw": 360, "zero": 361, "zeros_lik": 362, "dit_xl_2": 364, "With": 364, "resnet": [368, 376], "basic": [368, 378], "block": [368, 376], "tracer": 370, "creat": [371, 376], "host": 371, "borrow": 371, "v": 371, "own": 371, "open": 371, "initi": [371, 372, 373], "b": [371, 372], "random": [371, 372], "valu": [371, 372], "inspect": [371, 372], "attribut": 371, "close": [371, 372, 373], "enabl": [372, 373, 378], "configur": [372, 373], "result": 372, "write": 373, "activ": 373, "weight": 373, "first": 373, "iter": 373, "subsequ": 373, "version": [373, 376], "pre": [373, 377, 378], "process": 373, "paramet": [373, 376], "check": 373, "match": 373, "origin": 373, "trace": [374, 376, 378], "modul": [374, 376], "written": 374, "torchvis": 376, "preprocess": 376, "displai": [376, 377], "pass": 376, "constructor": 376, "base": 377, "http": 377, "github": 377, "com": 377, "facebookresearch": 377, "dit": 377, "git": 377, "download": 377, "xl": 377, "sampl": 377, "train": 377, "__getitem__": 378, "slice": 378, "5": 378, "intermedi": 378, "6": 378, "7": 378, "8": 378, "9": 378, "10": 378, "chang": 378, "string": 378, "represent": 378, "11": 378, "visual": 378, "web": 378, "browser": 378, "12": 378, "regist": 378, "post": 378, "13": 378, "queri": 378, "14": 378, "fall": 378, "back": 378, "15": 378, "captur": 378, "buffer": 378, "alloc": 378, "etc": 378, "placehold": 379, "titl": 379}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "nbsphinx": 4, "sphinx": 58}, "alltitles": {"Welcome to TT-NN documentation!": [[0, "welcome-to-tt-nn-documentation"]], "TTNN": [[0, null]], "Models": [[0, null]], "Resources": [[0, null]], "Indices and tables": [[0, "indices-and-tables"]], "Contributing as a developer": [[1, "contributing-as-a-developer"]], "Support": [[2, "support"]], "Reporting bugs, feature proposals, or support requests": [[2, "reporting-bugs-feature-proposals-or-support-requests"]], "Troubleshooting and debugging tips": [[2, "troubleshooting-and-debugging-tips"]], "Community": [[2, "community"]], "Performance": [[3, "performance"]], "Prerequisites": [[3, "prerequisites"], [4, "prerequisites"]], "Running a perf file": [[3, "running-a-perf-file"]], "Running all the perf files": [[3, "running-all-the-perf-files"]], "Getting Started": [[4, "getting-started"], [14, "getting-started"]], "Running an example model": [[4, "running-an-example-model"]], "Next steps": [[4, "next-steps"]], "What is ttnn?": [[5, "what-is-ttnn"]], "Key features of ttnn": [[5, "key-features-of-ttnn"]], "Adding New ttnn Operation": [[6, "adding-new-ttnn-operation"]], "FAQ": [[6, "faq"]], "What is a ttnn operation?": [[6, "what-is-a-ttnn-operation"]], "What steps are needed to add ttnn operation in C++?": [[6, "what-steps-are-needed-to-add-ttnn-operation-in-c"]], "What steps are needed to add ttnn operation in Python?": [[6, "what-steps-are-needed-to-add-ttnn-operation-in-python"]], "Example of Adding a new Device Operation": [[6, "example-of-adding-a-new-device-operation"]], "C++ Implementation": [[6, "c-implementation"]], "Step 1: Implement device operation": [[6, "step-1-implement-device-operation"]], "Step 2: Implement the operation in C++": [[6, "step-2-implement-the-operation-in-c"]], "Python Implementation": [[6, "python-implementation"]], "Step 1: Add Python binding": [[6, "step-1-add-python-binding"]], "Step 2: (Optional) Add golden function for the operation in Python": [[6, "step-2-optional-add-golden-function-for-the-operation-in-python"]], "APIs": [[7, "apis"], [18, "apis"]], "Device": [[7, "device"]], "Memory Config": [[7, "memory-config"], [18, "memory-config"]], "Operations": [[7, "operations"]], "Core": [[7, "core"]], "Tensor Creation": [[7, "tensor-creation"]], "Matrix Multiplication": [[7, "matrix-multiplication"], [372, "Matrix-Multiplication"]], "Pointwise Unary": [[7, "pointwise-unary"]], "Pointwise Binary": [[7, "pointwise-binary"]], "Pointwise Ternary": [[7, "pointwise-ternary"]], "Losses": [[7, "losses"]], "Reduction": [[7, "reduction"]], "Data Movement": [[7, "data-movement"]], "Normalization": [[7, "normalization"]], "Transformer": [[7, "transformer"]], "Embedding": [[7, "embedding"]], "Pooling": [[7, "pooling"]], "Vision": [[7, "vision"]], "KV Cache": [[7, "kv-cache"]], "Model Conversion": [[7, "model-conversion"]], "Reports": [[7, "reports"]], "Operation Hooks": [[7, "operation-hooks"]], "Converting torch Model to ttnn": [[8, "converting-torch-model-to-ttnn"]], "Step 1 - Rewriting the Model": [[8, "step-1-rewriting-the-model"]], "Step 2 - Switching to ttnn Operations": [[8, "step-2-switching-to-ttnn-operations"]], "Step 3 - Optimizing the Model": [[8, "step-3-optimizing-the-model"]], "More examples": [[8, "more-examples"]], "Building and Uplifting Demos": [[9, "building-and-uplifting-demos"]], "Examples of Tensor and TT-LIB Use": [[10, "examples-of-tensor-and-tt-lib-use"]], "Run one OP from TT-LIB on TT Accelerator device": [[10, "run-one-op-from-tt-lib-on-tt-accelerator-device"]], "Run TT-LIB and PyTorch OPs": [[10, "run-tt-lib-and-pytorch-ops"]], "Tensors with odd size of last dim": [[10, "tensors-with-odd-size-of-last-dim"]], "Dependencies": [[11, "dependencies"]], "Tensor": [[12, "tensor"], [18, "tensor"]], "Overview": [[12, "overview"], [13, "overview"]], "Tensor Storage": [[12, "tensor-storage"]], "Tensor API": [[12, "tensor-api"]], "MemoryConfig": [[12, "memoryconfig"]], "Examples of converting between PyTorch Tensor and TT Tensor": [[12, "examples-of-converting-between-pytorch-tensor-and-tt-tensor"]], "Converting a PyTorch Tensor to a TT Tensor": [[12, "converting-a-pytorch-tensor-to-a-tt-tensor"]], "Converting a TT Tensor to a PyTorch Tensor": [[12, "converting-a-tt-tensor-to-a-pytorch-tensor"]], "TT-LIB": [[13, "tt-lib"]], "Operation Infrastructure": [[13, "operation-infrastructure"]], "New Device Operation": [[13, "new-device-operation"]], "New Device Operation with a member": [[13, "new-device-operation-with-a-member"]], "New Device Operation with Optional Input Tensors": [[13, "new-device-operation-with-optional-input-tensors"]], "New Device Operation with Optional Output Tensors": [[13, "new-device-operation-with-optional-output-tensors"]], "Profiler": [[13, "profiler"]], "Fast Dispatch": [[13, "fast-dispatch"]], "Program Caching": [[13, "program-caching"]], "Logs": [[13, "logs"]], "TT-LIB API through tt_lib": [[13, "tt-lib-api-through-tt-lib"]], "Primary Operations": [[13, "primary-operations"]], "Enums": [[13, "enums"]], "Fallback Operations": [[13, "fallback-operations"]], "Experimental Operations": [[13, "experimental-operations"]], "Fused Operations from tt_lib Mini-Graph Library": [[13, "fused-operations-from-tt-lib-mini-graph-library"]], "Complex Operations (Type 2)": [[13, "complex-operations-type-2"]], "1. Install and Build": [[14, "install-and-build"]], "2. Explore our model demos": [[14, "explore-our-model-demos"]], "3. TT-NN Tutorial: Multi-Head Attention (Simple)": [[14, "tt-nn-tutorial-multi-head-attention-simple"]], "4. TT-NN Tutorial: Multi-Head Attention (Optimized)": [[14, "tt-nn-tutorial-multi-head-attention-optimized"]], "Where to go from here": [[14, "where-to-go-from-here"]], "Onboarding New Functionality": [[16, "onboarding-new-functionality"]], "Profiling ttnn Operations": [[17, "profiling-ttnn-operations"]], "Perf Report Headers": [[17, "perf-report-headers"]], "profile_this description": [[17, "profile-this-description"]], "Shape": [[18, "shape"]], "Layout": [[18, "layout"], [371, "Layout"]], "Data Type": [[18, "data-type"], [371, "Data-Type"]], "Required Width Multiples for Data Types": [[18, "id5"]], "Storage": [[18, "storage"]], "Tensor Sharding": [[18, "tensor-sharding"]], "ttnn.GetDefaultDevice": [[19, "ttnn-getdefaultdevice"]], "ttnn.MaxPool2d": [[20, "ttnn-maxpool2d"]], "ttnn.SetDefaultDevice": [[21, "ttnn-setdefaultdevice"]], "ttnn.abs": [[22, "ttnn-abs"]], "ttnn.abs_bw": [[23, "ttnn-abs-bw"]], "ttnn.acos": [[24, "ttnn-acos"]], "ttnn.acos_bw": [[25, "ttnn-acos-bw"]], "ttnn.acosh": [[26, "ttnn-acosh"]], "ttnn.acosh_bw": [[27, "ttnn-acosh-bw"]], "ttnn.add": [[28, "ttnn-add"]], "ttnn.add_bw": [[29, "ttnn-add-bw"]], "ttnn.addalpha": [[30, "ttnn-addalpha"]], "ttnn.addalpha_bw": [[31, "ttnn-addalpha-bw"]], "ttnn.addcdiv": [[32, "ttnn-addcdiv"]], "ttnn.addcdiv_bw": [[33, "ttnn-addcdiv-bw"]], "ttnn.addcmul": [[34, "ttnn-addcmul"]], "ttnn.addcmul_bw": [[35, "ttnn-addcmul-bw"]], "ttnn.angle": [[36, "ttnn-angle"]], "ttnn.angle_bw": [[37, "ttnn-angle-bw"]], "ttnn.arange": [[38, "ttnn-arange"]], "ttnn.argmax": [[39, "ttnn-argmax"]], "ttnn.as_tensor": [[40, "ttnn-as-tensor"]], "ttnn.asin": [[41, "ttnn-asin"]], "ttnn.asin_bw": [[42, "ttnn-asin-bw"]], "ttnn.asinh": [[43, "ttnn-asinh"]], "ttnn.asinh_bw": [[44, "ttnn-asinh-bw"]], "ttnn.assign_bw": [[45, "ttnn-assign-bw"]], "ttnn.atan": [[46, "ttnn-atan"]], "ttnn.atan2": [[47, "ttnn-atan2"]], "ttnn.atan2_bw": [[48, "ttnn-atan2-bw"]], "ttnn.atan_bw": [[49, "ttnn-atan-bw"]], "ttnn.atanh": [[50, "ttnn-atanh"]], "ttnn.atanh_bw": [[51, "ttnn-atanh-bw"]], "ttnn.bias_gelu_bw": [[52, "ttnn-bias-gelu-bw"]], "ttnn.bitwise_and": [[53, "ttnn-bitwise-and"]], "ttnn.bitwise_left_shift": [[54, "ttnn-bitwise-left-shift"]], "ttnn.bitwise_not": [[55, "ttnn-bitwise-not"]], "ttnn.bitwise_or": [[56, "ttnn-bitwise-or"]], "ttnn.bitwise_right_shift": [[57, "ttnn-bitwise-right-shift"]], "ttnn.bitwise_xor": [[58, "ttnn-bitwise-xor"]], "ttnn.cbrt": [[59, "ttnn-cbrt"]], "ttnn.ceil": [[60, "ttnn-ceil"]], "ttnn.ceil_bw": [[61, "ttnn-ceil-bw"]], "ttnn.celu": [[62, "ttnn-celu"]], "ttnn.celu_bw": [[63, "ttnn-celu-bw"]], "ttnn.clamp": [[64, "ttnn-clamp"]], "ttnn.clamp_bw": [[65, "ttnn-clamp-bw"]], "ttnn.clip": [[66, "ttnn-clip"]], "ttnn.clone": [[67, "ttnn-clone"]], "ttnn.close_device": [[68, "ttnn-close-device"]], "ttnn.concat": [[69, "ttnn-concat"]], "ttnn.concat_bw": [[70, "ttnn-concat-bw"]], "ttnn.conj": [[71, "ttnn-conj"]], "ttnn.conj_bw": [[72, "ttnn-conj-bw"]], "ttnn.cos": [[73, "ttnn-cos"]], "ttnn.cos_bw": [[74, "ttnn-cos-bw"]], "ttnn.cosh": [[75, "ttnn-cosh"]], "ttnn.cosh_bw": [[76, "ttnn-cosh-bw"]], "ttnn.create_sharded_memory_config": [[77, "ttnn-create-sharded-memory-config"]], "ttnn.deallocate": [[78, "ttnn-deallocate"]], "ttnn.deg2rad": [[79, "ttnn-deg2rad"]], "ttnn.deg2rad_bw": [[80, "ttnn-deg2rad-bw"]], "ttnn.digamma": [[81, "ttnn-digamma"]], "ttnn.digamma_bw": [[82, "ttnn-digamma-bw"]], "ttnn.div": [[83, "ttnn-div"]], "ttnn.div_bw": [[84, "ttnn-div-bw"]], "ttnn.div_no_nan": [[85, "ttnn-div-no-nan"]], "ttnn.div_no_nan_bw": [[86, "ttnn-div-no-nan-bw"]], "ttnn.downsample": [[87, "ttnn-downsample"]], "ttnn.dump_tensor": [[88, "ttnn-dump-tensor"]], "ttnn.elu": [[89, "ttnn-elu"]], "ttnn.elu_bw": [[90, "ttnn-elu-bw"]], "ttnn.embedding": [[91, "ttnn-embedding"]], "ttnn.embedding_bw": [[92, "ttnn-embedding-bw"]], "ttnn.empty": [[93, "ttnn-empty"]], "ttnn.eq": [[94, "ttnn-eq"], [95, "ttnn-eq"]], "ttnn.eqz": [[96, "ttnn-eqz"]], "ttnn.erf": [[97, "ttnn-erf"]], "ttnn.erf_bw": [[98, "ttnn-erf-bw"]], "ttnn.erfc": [[99, "ttnn-erfc"]], "ttnn.erfc_bw": [[100, "ttnn-erfc-bw"]], "ttnn.erfinv": [[101, "ttnn-erfinv"]], "ttnn.erfinv_bw": [[102, "ttnn-erfinv-bw"]], "ttnn.exp": [[103, "ttnn-exp"]], "ttnn.exp2": [[104, "ttnn-exp2"]], "ttnn.exp2_bw": [[105, "ttnn-exp2-bw"]], "ttnn.exp_bw": [[106, "ttnn-exp-bw"]], "ttnn.expm1": [[107, "ttnn-expm1"]], "ttnn.expm1_bw": [[108, "ttnn-expm1-bw"]], "ttnn.fill_bw": [[109, "ttnn-fill-bw"]], "ttnn.fill_ones_rm": [[110, "ttnn-fill-ones-rm"]], "ttnn.fill_rm": [[111, "ttnn-fill-rm"]], "ttnn.fill_zero_bw": [[112, "ttnn-fill-zero-bw"]], "ttnn.floor": [[113, "ttnn-floor"]], "ttnn.floor_bw": [[114, "ttnn-floor-bw"]], "ttnn.floor_div": [[115, "ttnn-floor-div"]], "ttnn.fmod": [[116, "ttnn-fmod"]], "ttnn.fmod_bw": [[117, "ttnn-fmod-bw"]], "ttnn.format_input_tensor": [[118, "ttnn-format-input-tensor"]], "ttnn.format_output_tensor": [[119, "ttnn-format-output-tensor"]], "ttnn.frac": [[120, "ttnn-frac"]], "ttnn.frac_bw": [[121, "ttnn-frac-bw"]], "ttnn.from_device": [[122, "ttnn-from-device"]], "ttnn.from_torch": [[123, "ttnn-from-torch"]], "ttnn.full": [[124, "ttnn-full"]], "ttnn.full_like": [[125, "ttnn-full-like"]], "ttnn.ge": [[126, "ttnn-ge"], [127, "ttnn-ge"]], "ttnn.geglu": [[128, "ttnn-geglu"]], "ttnn.gelu": [[129, "ttnn-gelu"]], "ttnn.gelu_bw": [[130, "ttnn-gelu-bw"]], "ttnn.gez": [[131, "ttnn-gez"]], "ttnn.global_avg_pool2d": [[132, "ttnn-global-avg-pool2d"]], "ttnn.glu": [[133, "ttnn-glu"]], "ttnn.group_norm": [[134, "ttnn-group-norm"]], "ttnn.gt": [[135, "ttnn-gt"], [136, "ttnn-gt"]], "ttnn.gtz": [[137, "ttnn-gtz"]], "ttnn.hardshrink": [[138, "ttnn-hardshrink"]], "ttnn.hardshrink_bw": [[139, "ttnn-hardshrink-bw"]], "ttnn.hardsigmoid": [[140, "ttnn-hardsigmoid"]], "ttnn.hardsigmoid_bw": [[141, "ttnn-hardsigmoid-bw"]], "ttnn.hardswish": [[142, "ttnn-hardswish"]], "ttnn.hardswish_bw": [[143, "ttnn-hardswish-bw"]], "ttnn.hardtanh": [[144, "ttnn-hardtanh"]], "ttnn.hardtanh_bw": [[145, "ttnn-hardtanh-bw"]], "ttnn.heaviside": [[146, "ttnn-heaviside"]], "ttnn.hypot": [[147, "ttnn-hypot"]], "ttnn.hypot_bw": [[148, "ttnn-hypot-bw"]], "ttnn.i0": [[149, "ttnn-i0"]], "ttnn.i0_bw": [[150, "ttnn-i0-bw"]], "ttnn.identity": [[151, "ttnn-identity"]], "ttnn.imag": [[152, "ttnn-imag"]], "ttnn.imag_bw": [[153, "ttnn-imag-bw"]], "ttnn.indexed_fill": [[154, "ttnn-indexed-fill"]], "ttnn.is_imag": [[155, "ttnn-is-imag"]], "ttnn.is_real": [[156, "ttnn-is-real"]], "ttnn.isclose": [[157, "ttnn-isclose"]], "ttnn.isfinite": [[158, "ttnn-isfinite"]], "ttnn.isinf": [[159, "ttnn-isinf"]], "ttnn.isnan": [[160, "ttnn-isnan"]], "ttnn.isneginf": [[161, "ttnn-isneginf"]], "ttnn.isposinf": [[162, "ttnn-isposinf"]], "ttnn.kv_cache.fill_cache_for_user_": [[163, "ttnn-kv-cache-fill-cache-for-user"]], "ttnn.kv_cache.update_cache_for_token_": [[164, "ttnn-kv-cache-update-cache-for-token"]], "ttnn.l1_loss": [[165, "ttnn-l1-loss"]], "ttnn.layer_norm": [[166, "ttnn-layer-norm"]], "ttnn.ldexp": [[167, "ttnn-ldexp"]], "ttnn.ldexp_bw": [[168, "ttnn-ldexp-bw"]], "ttnn.le": [[169, "ttnn-le"], [170, "ttnn-le"]], "ttnn.leaky_relu": [[171, "ttnn-leaky-relu"]], "ttnn.leaky_relu_bw": [[172, "ttnn-leaky-relu-bw"]], "ttnn.lerp": [[173, "ttnn-lerp"]], "ttnn.lerp_bw": [[174, "ttnn-lerp-bw"]], "ttnn.lez": [[175, "ttnn-lez"]], "ttnn.lgamma": [[176, "ttnn-lgamma"]], "ttnn.lgamma_bw": [[177, "ttnn-lgamma-bw"]], "ttnn.linear": [[178, "ttnn-linear"]], "ttnn.load_tensor": [[179, "ttnn-load-tensor"]], "ttnn.log": [[180, "ttnn-log"]], "ttnn.log10": [[181, "ttnn-log10"]], "ttnn.log10_bw": [[182, "ttnn-log10-bw"]], "ttnn.log1p": [[183, "ttnn-log1p"]], "ttnn.log1p_bw": [[184, "ttnn-log1p-bw"]], "ttnn.log2": [[185, "ttnn-log2"]], "ttnn.log2_bw": [[186, "ttnn-log2-bw"]], "ttnn.log_bw": [[187, "ttnn-log-bw"]], "ttnn.log_sigmoid": [[188, "ttnn-log-sigmoid"]], "ttnn.log_sigmoid_bw": [[189, "ttnn-log-sigmoid-bw"]], "ttnn.logaddexp": [[190, "ttnn-logaddexp"]], "ttnn.logaddexp2": [[191, "ttnn-logaddexp2"]], "ttnn.logaddexp2_bw": [[192, "ttnn-logaddexp2-bw"]], "ttnn.logaddexp_bw": [[193, "ttnn-logaddexp-bw"]], "ttnn.logical_and": [[194, "ttnn-logical-and"], [195, "ttnn-logical-and"]], "ttnn.logical_not": [[196, "ttnn-logical-not"], [197, "ttnn-logical-not"]], "ttnn.logical_or": [[198, "ttnn-logical-or"], [199, "ttnn-logical-or"]], "ttnn.logical_xor": [[200, "ttnn-logical-xor"], [201, "ttnn-logical-xor"]], "ttnn.logit": [[202, "ttnn-logit"]], "ttnn.logit_bw": [[203, "ttnn-logit-bw"]], "ttnn.logiteps_bw": [[204, "ttnn-logiteps-bw"]], "ttnn.lt": [[205, "ttnn-lt"], [206, "ttnn-lt"]], "ttnn.ltz": [[207, "ttnn-ltz"]], "ttnn.mac": [[208, "ttnn-mac"]], "ttnn.manage_device": [[209, "ttnn-manage-device"]], "ttnn.matmul": [[210, "ttnn-matmul"]], "ttnn.max": [[211, "ttnn-max"]], "ttnn.max_bw": [[212, "ttnn-max-bw"]], "ttnn.max_pool2d": [[213, "ttnn-max-pool2d"]], "ttnn.maximum": [[214, "ttnn-maximum"]], "ttnn.mean": [[215, "ttnn-mean"]], "ttnn.min": [[216, "ttnn-min"]], "ttnn.min_bw": [[217, "ttnn-min-bw"]], "ttnn.minimum": [[218, "ttnn-minimum"]], "ttnn.mish": [[219, "ttnn-mish"]], "ttnn.model_preprocessing.preprocess_model": [[220, "ttnn-model-preprocessing-preprocess-model"]], "ttnn.model_preprocessing.preprocess_model_parameters": [[221, "ttnn-model-preprocessing-preprocess-model-parameters"]], "ttnn.mse_loss": [[222, "ttnn-mse-loss"]], "ttnn.mul_bw": [[223, "ttnn-mul-bw"]], "ttnn.multigammaln": [[224, "ttnn-multigammaln"]], "ttnn.multigammaln_bw": [[225, "ttnn-multigammaln-bw"]], "ttnn.multiply": [[226, "ttnn-multiply"]], "ttnn.ne": [[227, "ttnn-ne"], [228, "ttnn-ne"]], "ttnn.neg": [[229, "ttnn-neg"]], "ttnn.neg_bw": [[230, "ttnn-neg-bw"]], "ttnn.nextafter": [[231, "ttnn-nextafter"]], "ttnn.nez": [[232, "ttnn-nez"]], "ttnn.nonzero": [[233, "ttnn-nonzero"]], "ttnn.normalize_global": [[234, "ttnn-normalize-global"]], "ttnn.normalize_hw": [[235, "ttnn-normalize-hw"]], "ttnn.ones": [[236, "ttnn-ones"]], "ttnn.ones_like": [[237, "ttnn-ones-like"]], "ttnn.open_device": [[238, "ttnn-open-device"]], "ttnn.outer": [[239, "ttnn-outer"]], "ttnn.pad": [[240, "ttnn-pad"]], "ttnn.pad_to_tile_shape": [[241, "ttnn-pad-to-tile-shape"]], "ttnn.permute": [[242, "ttnn-permute"]], "ttnn.polar": [[243, "ttnn-polar"]], "ttnn.polar_bw": [[244, "ttnn-polar-bw"]], "ttnn.polygamma": [[245, "ttnn-polygamma"]], "ttnn.polygamma_bw": [[246, "ttnn-polygamma-bw"]], "ttnn.polyval": [[247, "ttnn-polyval"]], "ttnn.pow": [[248, "ttnn-pow"]], "ttnn.pow_bw": [[249, "ttnn-pow-bw"]], "ttnn.prelu": [[250, "ttnn-prelu"]], "ttnn.prod": [[251, "ttnn-prod"]], "ttnn.prod_bw": [[252, "ttnn-prod-bw"]], "ttnn.rad2deg": [[253, "ttnn-rad2deg"]], "ttnn.rad2deg_bw": [[254, "ttnn-rad2deg-bw"]], "ttnn.rdiv": [[255, "ttnn-rdiv"]], "ttnn.rdiv_bw": [[256, "ttnn-rdiv-bw"]], "ttnn.real": [[257, "ttnn-real"]], "ttnn.real_bw": [[258, "ttnn-real-bw"]], "ttnn.reallocate": [[259, "ttnn-reallocate"]], "ttnn.reciprocal": [[260, "ttnn-reciprocal"]], "ttnn.reciprocal_bw": [[261, "ttnn-reciprocal-bw"]], "ttnn.register_post_operation_hook": [[262, "ttnn-register-post-operation-hook"]], "ttnn.register_pre_operation_hook": [[263, "ttnn-register-pre-operation-hook"]], "ttnn.reglu": [[264, "ttnn-reglu"]], "ttnn.relu": [[265, "ttnn-relu"]], "ttnn.relu6": [[266, "ttnn-relu6"]], "ttnn.relu6_bw": [[267, "ttnn-relu6-bw"]], "ttnn.relu_bw": [[268, "ttnn-relu-bw"]], "ttnn.relu_max": [[269, "ttnn-relu-max"]], "ttnn.relu_min": [[270, "ttnn-relu-min"]], "ttnn.remainder": [[271, "ttnn-remainder"]], "ttnn.remainder_bw": [[272, "ttnn-remainder-bw"]], "ttnn.repeat": [[273, "ttnn-repeat"]], "ttnn.repeat_bw": [[274, "ttnn-repeat-bw"]], "ttnn.repeat_interleave": [[275, "ttnn-repeat-interleave"]], "ttnn.reshape": [[276, "ttnn-reshape"]], "ttnn.rms_norm": [[277, "ttnn-rms-norm"]], "ttnn.round": [[278, "ttnn-round"]], "ttnn.round_bw": [[279, "ttnn-round-bw"]], "ttnn.rpow": [[280, "ttnn-rpow"]], "ttnn.rpow_bw": [[281, "ttnn-rpow-bw"]], "ttnn.rsqrt": [[282, "ttnn-rsqrt"]], "ttnn.rsqrt_bw": [[283, "ttnn-rsqrt-bw"]], "ttnn.rsub": [[284, "ttnn-rsub"]], "ttnn.rsub_bw": [[285, "ttnn-rsub-bw"]], "ttnn.scatter": [[286, "ttnn-scatter"]], "ttnn.selu": [[287, "ttnn-selu"]], "ttnn.selu_bw": [[288, "ttnn-selu-bw"]], "ttnn.set_printoptions": [[289, "ttnn-set-printoptions"]], "ttnn.sigmoid": [[290, "ttnn-sigmoid"]], "ttnn.sigmoid_accurate": [[291, "ttnn-sigmoid-accurate"]], "ttnn.sigmoid_bw": [[292, "ttnn-sigmoid-bw"]], "ttnn.sign": [[293, "ttnn-sign"]], "ttnn.sign_bw": [[294, "ttnn-sign-bw"]], "ttnn.signbit": [[295, "ttnn-signbit"]], "ttnn.silu": [[296, "ttnn-silu"]], "ttnn.silu_bw": [[297, "ttnn-silu-bw"]], "ttnn.sin": [[298, "ttnn-sin"]], "ttnn.sin_bw": [[299, "ttnn-sin-bw"]], "ttnn.sinh": [[300, "ttnn-sinh"]], "ttnn.sinh_bw": [[301, "ttnn-sinh-bw"]], "ttnn.softmax": [[302, "ttnn-softmax"]], "ttnn.softplus": [[303, "ttnn-softplus"]], "ttnn.softplus_bw": [[304, "ttnn-softplus-bw"]], "ttnn.softshrink": [[305, "ttnn-softshrink"]], "ttnn.softshrink_bw": [[306, "ttnn-softshrink-bw"]], "ttnn.softsign": [[307, "ttnn-softsign"]], "ttnn.softsign_bw": [[308, "ttnn-softsign-bw"]], "ttnn.split": [[309, "ttnn-split"]], "ttnn.sqrt": [[310, "ttnn-sqrt"]], "ttnn.sqrt_bw": [[311, "ttnn-sqrt-bw"]], "ttnn.square": [[312, "ttnn-square"]], "ttnn.square_bw": [[313, "ttnn-square-bw"]], "ttnn.squared_difference": [[314, "ttnn-squared-difference"]], "ttnn.squared_difference_bw": [[315, "ttnn-squared-difference-bw"]], "ttnn.std": [[316, "ttnn-std"]], "ttnn.sub_bw": [[317, "ttnn-sub-bw"]], "ttnn.subalpha": [[318, "ttnn-subalpha"]], "ttnn.subalpha_bw": [[319, "ttnn-subalpha-bw"]], "ttnn.subtract": [[320, "ttnn-subtract"]], "ttnn.sum": [[321, "ttnn-sum"]], "ttnn.swiglu": [[322, "ttnn-swiglu"]], "ttnn.swish": [[323, "ttnn-swish"]], "ttnn.synchronize_device": [[324, "ttnn-synchronize-device"]], "ttnn.tan": [[325, "ttnn-tan"]], "ttnn.tan_bw": [[326, "ttnn-tan-bw"]], "ttnn.tanh": [[327, "ttnn-tanh"]], "ttnn.tanh_bw": [[328, "ttnn-tanh-bw"]], "ttnn.tanhshrink": [[329, "ttnn-tanhshrink"]], "ttnn.tanhshrink_bw": [[330, "ttnn-tanhshrink-bw"]], "ttnn.threshold": [[331, "ttnn-threshold"]], "ttnn.threshold_bw": [[332, "ttnn-threshold-bw"]], "ttnn.tilize": [[333, "ttnn-tilize"]], "ttnn.tilize_with_val_padding": [[334, "ttnn-tilize-with-val-padding"]], "ttnn.to_device": [[335, "ttnn-to-device"]], "ttnn.to_layout": [[336, "ttnn-to-layout"]], "ttnn.to_memory_config": [[337, "ttnn-to-memory-config"]], "ttnn.to_torch": [[338, "ttnn-to-torch"]], "ttnn.topk": [[339, "ttnn-topk"]], "ttnn.transformer.attention_softmax": [[340, "ttnn-transformer-attention-softmax"]], "ttnn.transformer.attention_softmax_": [[341, "ttnn-transformer-attention-softmax"]], "ttnn.transformer.concatenate_heads": [[342, "ttnn-transformer-concatenate-heads"]], "ttnn.experimental.rotary_embedding": [[343, "ttnn-experimental-rotary-embedding"]], "ttnn.transformer.scaled_dot_product_attention": [[344, "ttnn-transformer-scaled-dot-product-attention"]], "ttnn.transformer.scaled_dot_product_attention_decode": [[345, "ttnn-transformer-scaled-dot-product-attention-decode"]], "ttnn.transformer.scaled_dot_product_attention_decode_gqa": [[346, "ttnn-transformer-scaled-dot-product-attention-decode-gqa"]], "ttnn.transformer.split_query_key_value_and_split_heads": [[347, "ttnn-transformer-split-query-key-value-and-split-heads"]], "ttnn.tril": [[348, "ttnn-tril"]], "ttnn.triu": [[349, "ttnn-triu"]], "ttnn.trunc": [[350, "ttnn-trunc"]], "ttnn.trunc_bw": [[351, "ttnn-trunc-bw"]], "ttnn.untilize": [[352, "ttnn-untilize"]], "ttnn.untilize_with_halo_v2": [[353, "ttnn-untilize-with-halo-v2"]], "ttnn.untilize_with_unpadding": [[354, "ttnn-untilize-with-unpadding"]], "ttnn.upsample": [[355, "ttnn-upsample"]], "ttnn.var": [[356, "ttnn-var"]], "ttnn.where": [[357, "ttnn-where"]], "ttnn.where_bw": [[358, "ttnn-where-bw"]], "ttnn.xlogy": [[359, "ttnn-xlogy"]], "ttnn.xlogy_bw": [[360, "ttnn-xlogy-bw"]], "ttnn.zeros": [[361, "ttnn-zeros"]], "ttnn.zeros_like": [[362, "ttnn-zeros-like"]], "Tutorials": [[363, "id1"]], "Graphing Torch DiT_XL_2 With TTNN": [[364, "graphing-torch-dit-xl-2-with-ttnn"]], "Matmul Operation": [[365, "matmul-operation"]], "Multi-Head Attention": [[366, "multi-head-attention"], [373, "Multi-Head-Attention"]], "ttnn Profiling": [[367, "ttnn-profiling"]], "Resnet Basic Block": [[368, "resnet-basic-block"]], "Tensor and Add Operation": [[369, "tensor-and-add-operation"], [371, "Tensor-and-Add-Operation"]], "ttnn Tracer": [[370, "ttnn-tracer"]], "Creating a tensor": [[371, "Creating-a-tensor"]], "Host Storage: Borrowed vs Owned": [[371, "Host-Storage:-Borrowed-vs-Owned"]], "Device storage": [[371, "Device-storage"]], "Open the device": [[371, "Open-the-device"]], "Initialize tensors a and b with random values using torch": [[371, "Initialize-tensors-a-and-b-with-random-values-using-torch"], [372, "Initialize-tensors-a-and-b-with-random-values-using-torch"]], "Add tensor a and b": [[371, "Add-tensor-a-and-b"]], "Inspect the output tensor of the add in ttnn": [[371, "Inspect-the-output-tensor-of-the-add-in-ttnn"]], "Convert to torch and inspect the attributes of the torch tensor": [[371, "Convert-to-torch-and-inspect-the-attributes-of-the-torch-tensor"]], "Close the device": [[371, "Close-the-device"], [372, "Close-the-device"], [373, "Close-the-device"]], "Enable program cache": [[372, "Enable-program-cache"], [373, "Enable-program-cache"]], "Configuration": [[372, "Configuration"], [373, "Configuration"]], "Matrix multiply tensor a and b": [[372, "Matrix-multiply-tensor-a-and-b"]], "Inspect the layout of matrix multiplication output": [[372, "Inspect-the-layout-of-matrix-multiplication-output"]], "Inspect the result of the matrix multiplication": [[372, "Inspect-the-result-of-the-matrix-multiplication"]], "Matrix multiply tensor a and b by using more performant config": [[372, "Matrix-multiply-tensor-a-and-b-by-using-more-performant-config"]], "Write Multi-Head Attention using ttnn": [[373, "Write-Multi-Head-Attention-using-ttnn"]], "Initialize activations and weights using torch": [[373, "Initialize-activations-and-weights-using-torch"]], "Convert activations and weights to ttnn": [[373, "Convert-activations-and-weights-to-ttnn"]], "Run the first iteration of Multi-Head Attention": [[373, "Run-the-first-iteration-of-Multi-Head-Attention"]], "Run a subsequent iteration of Multi-Head Attention": [[373, "Run-a-subsequent-iteration-of-Multi-Head-Attention"]], "Write optimized version of Multi-Head Attention": [[373, "Write-optimized-version-of-Multi-Head-Attention"]], "Pre-process the parameters of the optimized model": [[373, "Pre-process-the-parameters-of-the-optimized-model"]], "Run the first iteration of the optimized Multi-Head Attention": [[373, "Run-the-first-iteration-of-the-optimized-Multi-Head-Attention"]], "Run a subsequent iteration of the optimized Multi-Head Attention": [[373, "Run-a-subsequent-iteration-of-the-optimized-Multi-Head-Attention"]], "Check that the output of the optimized version matches the output of the original implementation": [[373, "Check-that-the-output-of-the-optimized-version-matches-the-output-of-the-original-implementation"]], "Tracing ttnn operations and torch modules/functions": [[374, "Tracing-ttnn-operations-and-torch-modules/functions"]], "Trace torch functions": [[374, "Trace-torch-functions"]], "Trace torch functions and ttnn operations": [[374, "Trace-torch-functions-and-ttnn-operations"]], "Trace torch functions, torch modules and ttnn operations": [[374, "Trace-torch-functions,-torch-modules-and-ttnn-operations"]], "Trace models written using ttnn": [[374, "Trace-models-written-using-ttnn"]], "Profiling ttnn operations": [[375, "Profiling-ttnn-operations"]], "Resnet Block": [[376, "Resnet-Block"]], "Torch Module (from torchvision)": [[376, "Torch-Module-(from-torchvision)"]], "Create torch module and preprocess it to get ttnn parameters": [[376, "Create-torch-module-and-preprocess-it-to-get-ttnn-parameters"]], "Display the parameters of the module": [[376, "Display-the-parameters-of-the-module"]], "Display the traced torch graph": [[376, "Display-the-traced-torch-graph"]], "Implement ttnn version of the module. Pass in the parameters into the constructor.": [[376, "Implement-ttnn-version-of-the-module.-Pass-in-the-parameters-into-the-constructor."]], "Run ttnn module and display the traced graph": [[376, "Run-ttnn-module-and-display-the-traced-graph"]], "Build a graph of a pytorch based model": [[377, "Build-a-graph-of-a-pytorch-based-model"]], "Clone the library from https://github.com/facebookresearch/DiT.git": [[377, "Clone-the-library-from-https://github.com/facebookresearch/DiT.git"]], "Download DiT-XL/2 Models": [[377, "Download-DiT-XL/2-Models"]], "Sample from Pre-trained DiT Models and build the graph": [[377, "Sample-from-Pre-trained-DiT-Models-and-build-the-graph"]], "Display the graph": [[377, "Display-the-graph"]], "Using ttnn": [[378, "using-ttnn"]], "Basic Examples": [[378, "basic-examples"]], "1. Converting from and to torch tensor": [[378, "converting-from-and-to-torch-tensor"]], "2. Running an operation on the device": [[378, "running-an-operation-on-the-device"]], "3. Using __getitem__ to slice the tensor": [[378, "using-getitem-to-slice-the-tensor"]], "4. Enabling program cache": [[378, "enabling-program-cache"]], "5. Debugging intermediate tensors": [[378, "debugging-intermediate-tensors"]], "6. Tracing the graph of operations": [[378, "tracing-the-graph-of-operations"]], "7. Using tt_lib operation in ttnn": [[378, "using-tt-lib-operation-in-ttnn"]], "8. Enabling Logging": [[378, "enabling-logging"]], "9. Supported Python Operators": [[378, "supported-python-operators"]], "10. Changing the string representation of the tensor": [[378, "changing-the-string-representation-of-the-tensor"]], "11. Visualize using Web Browser": [[378, "visualize-using-web-browser"]], "12. Register pre- and/or post-operation hooks": [[378, "register-pre-and-or-post-operation-hooks"]], "13. Query all operations": [[378, "query-all-operations"]], "14. Falling back to torch": [[378, "falling-back-to-torch"]], "15. Capturing graph of C++ functions, buffer allocations, etc": [[378, "capturing-graph-of-c-functions-buffer-allocations-etc"]], "Placeholder title": [[379, "placeholder-title"]]}, "indexentries": {"memoryconfig (class in ttnn)": [[12, "ttnn.MemoryConfig"]], "tensor (class in ttnn)": [[12, "ttnn.Tensor"]], "__init__() (ttnn.memoryconfig method)": [[12, "ttnn.MemoryConfig.__init__"]], "__init__() (ttnn.tensor method)": [[12, "ttnn.Tensor.__init__"]], "buffer() (ttnn.tensor method)": [[12, "ttnn.Tensor.buffer"]], "device() (ttnn.tensor method)": [[12, "ttnn.Tensor.device"]], "get_dtype() (ttnn.tensor method)": [[12, "ttnn.Tensor.get_dtype"]], "get_layout() (ttnn.tensor method)": [[12, "ttnn.Tensor.get_layout"]], "get_legacy_shape() (ttnn.tensor method)": [[12, "ttnn.Tensor.get_legacy_shape"]], "pad() (ttnn.tensor method)": [[12, "ttnn.Tensor.pad"]], "pad_to_tile() (ttnn.tensor method)": [[12, "ttnn.Tensor.pad_to_tile"]], "storage_type() (ttnn.tensor method)": [[12, "ttnn.Tensor.storage_type"]], "to() (ttnn.tensor method)": [[12, "ttnn.Tensor.to"]], "unpad() (ttnn.tensor method)": [[12, "ttnn.Tensor.unpad"]], "unpad_from_tile() (ttnn.tensor method)": [[12, "ttnn.Tensor.unpad_from_tile"]], "adaptiveavgpool2d (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.AdaptiveAvgPool2d"]], "addandnorm() (in module tt_lib.fused_ops.add_and_norm)": [[13, "tt_lib.fused_ops.add_and_norm.AddAndNorm"]], "batchnorm2d (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.BatchNorm2d"]], "bcastopdim (class in ttnn)": [[13, "ttnn.BcastOpDim"]], "bcastopmath (class in ttnn)": [[13, "ttnn.BcastOpMath"]], "conv2d (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.Conv2d"]], "groupnorm (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.GroupNorm"]], "layernorm (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.LayerNorm"]], "layernorm() (in module tt_lib.fused_ops.layernorm)": [[13, "tt_lib.fused_ops.layernorm.Layernorm"]], "linear() (in module tt_lib.fused_ops.linear)": [[13, "tt_lib.fused_ops.linear.Linear"]], "maxpool2d (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.MaxPool2d"]], "binary_bitwise_and (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.binary_bitwise_and"]], "binary_bitwise_left_shift (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.binary_bitwise_left_shift"]], "binary_bitwise_or (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.binary_bitwise_or"]], "binary_bitwise_right_shift (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.binary_bitwise_right_shift"]], "binary_bitwise_xor (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.binary_bitwise_xor"]], "binary_fmod (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.binary_fmod"]], "bitwise_not (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.bitwise_not"]], "ceil (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.ceil"]], "chunk() (in module tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.chunk"]], "concat() (in module tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.concat"]], "conv2d() (in module tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.conv2d"]], "cpu() (in module ttnn.tensor)": [[13, "ttnn.Tensor.cpu"]], "floor (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.floor"]], "full() (in module tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.full"]], "group_norm() (in module tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.group_norm"]], "interpolate() (in module tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.interpolate"]], "layer_norm() (in module tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.layer_norm"]], "moreh_groupnorm() (in module tt_lib.operations.primary)": [[13, "tt_lib.operations.primary.moreh_groupnorm"]], "moreh_groupnorm_backward() (in module tt_lib.operations.primary)": [[13, "tt_lib.operations.primary.moreh_groupnorm_backward"]], "moreh_logsoftmax() (in module tt_lib.operations.primary)": [[13, "tt_lib.operations.primary.moreh_logsoftmax"]], "moreh_logsoftmax_backward() (in module tt_lib.operations.primary)": [[13, "tt_lib.operations.primary.moreh_logsoftmax_backward"]], "moreh_norm() (in module tt_lib.operations.primary)": [[13, "tt_lib.operations.primary.moreh_norm"]], "moreh_norm_backward() (in module tt_lib.operations.primary)": [[13, "tt_lib.operations.primary.moreh_norm_backward"]], "moreh_softmax() (in module tt_lib.operations.primary)": [[13, "tt_lib.operations.primary.moreh_softmax"]], "moreh_softmax_backward() (in module tt_lib.operations.primary)": [[13, "tt_lib.operations.primary.moreh_softmax_backward"]], "moreh_softmin() (in module tt_lib.operations.primary)": [[13, "tt_lib.operations.primary.moreh_softmin"]], "moreh_softmin_backward() (in module tt_lib.operations.primary)": [[13, "tt_lib.operations.primary.moreh_softmin_backward"]], "pad() (in module tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.pad"]], "repeat() (in module tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.repeat"]], "repeat_interleave() (in module tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.repeat_interleave"]], "reshape() (in module tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.reshape"]], "silu() (in module tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.silu"]], "softmax() (in module tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.softmax"]], "tensor_slice() (in module tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.tensor_slice"]], "torch_argmax (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.torch_argmax"]], "torch_argmin (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.torch_argmin"]], "trunc (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.trunc"]], "unary_bitwise_and (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.unary_bitwise_and"]], "unary_bitwise_left_shift (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.unary_bitwise_left_shift"]], "unary_bitwise_or (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.unary_bitwise_or"]], "unary_bitwise_right_shift (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.unary_bitwise_right_shift"]], "unary_bitwise_xor (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.unary_bitwise_xor"]], "unary_fmod (class in tt_lib.fallback_ops)": [[13, "tt_lib.fallback_ops.unary_fmod"]], "shape (class in ttnn)": [[18, "ttnn.Shape"]], "rank (ttnn.shape property)": [[18, "ttnn.Shape.rank"]], "with_tile_padding() (ttnn.shape method)": [[18, "ttnn.Shape.with_tile_padding"]], "getdefaultdevice() (in module ttnn)": [[19, "ttnn.GetDefaultDevice"]], "setdefaultdevice() (in module ttnn)": [[21, "ttnn.SetDefaultDevice"]], "abs() (in module ttnn)": [[22, "ttnn.abs"]], "abs_bw() (in module ttnn)": [[23, "ttnn.abs_bw"]], "acos() (in module ttnn)": [[24, "ttnn.acos"]], "acos_bw() (in module ttnn)": [[25, "ttnn.acos_bw"]], "acosh() (in module ttnn)": [[26, "ttnn.acosh"]], "acosh_bw() (in module ttnn)": [[27, "ttnn.acosh_bw"]], "add() (in module ttnn)": [[28, "ttnn.add"]], "add_bw() (in module ttnn)": [[29, "ttnn.add_bw"]], "addalpha() (in module ttnn)": [[30, "ttnn.addalpha"]], "addalpha_bw() (in module ttnn)": [[31, "ttnn.addalpha_bw"]], "addcdiv() (in module ttnn)": [[32, "ttnn.addcdiv"]], "addcdiv_bw() (in module ttnn)": [[33, "ttnn.addcdiv_bw"]], "addcmul() (in module ttnn)": [[34, "ttnn.addcmul"]], "addcmul_bw() (in module ttnn)": [[35, "ttnn.addcmul_bw"]], "angle() (in module ttnn)": [[36, "ttnn.angle"]], "angle_bw() (in module ttnn)": [[37, "ttnn.angle_bw"]], "arange() (in module ttnn)": [[38, "ttnn.arange"]], "argmax() (in module ttnn)": [[39, "ttnn.argmax"]], "as_tensor() (in module ttnn)": [[40, "ttnn.as_tensor"]], "asin() (in module ttnn)": [[41, "ttnn.asin"]], "asin_bw() (in module ttnn)": [[42, "ttnn.asin_bw"]], "asinh() (in module ttnn)": [[43, "ttnn.asinh"]], "asinh_bw() (in module ttnn)": [[44, "ttnn.asinh_bw"]], "assign_bw() (in module ttnn)": [[45, "ttnn.assign_bw"]], "atan() (in module ttnn)": [[46, "ttnn.atan"]], "atan2() (in module ttnn)": [[47, "ttnn.atan2"]], "atan2_bw() (in module ttnn)": [[48, "ttnn.atan2_bw"]], "atan_bw() (in module ttnn)": [[49, "ttnn.atan_bw"]], "atanh() (in module ttnn)": [[50, "ttnn.atanh"]], "atanh_bw() (in module ttnn)": [[51, "ttnn.atanh_bw"]], "bias_gelu_bw() (in module ttnn)": [[52, "ttnn.bias_gelu_bw"]], "bitwise_and() (in module ttnn)": [[53, "ttnn.bitwise_and"]], "bitwise_left_shift() (in module ttnn)": [[54, "ttnn.bitwise_left_shift"]], "bitwise_not() (in module ttnn)": [[55, "ttnn.bitwise_not"]], "bitwise_or() (in module ttnn)": [[56, "ttnn.bitwise_or"]], "bitwise_right_shift() (in module ttnn)": [[57, "ttnn.bitwise_right_shift"]], "bitwise_xor() (in module ttnn)": [[58, "ttnn.bitwise_xor"]], "cbrt() (in module ttnn)": [[59, "ttnn.cbrt"]], "ceil() (in module ttnn)": [[60, "ttnn.ceil"]], "ceil_bw() (in module ttnn)": [[61, "ttnn.ceil_bw"]], "celu() (in module ttnn)": [[62, "ttnn.celu"]], "celu_bw() (in module ttnn)": [[63, "ttnn.celu_bw"]], "clamp() (in module ttnn)": [[64, "ttnn.clamp"]], "clamp_bw() (in module ttnn)": [[65, "ttnn.clamp_bw"]], "clip() (in module ttnn)": [[66, "ttnn.clip"]], "clone() (in module ttnn)": [[67, "ttnn.clone"]], "close_device() (in module ttnn)": [[68, "ttnn.close_device"]], "concat() (in module ttnn)": [[69, "ttnn.concat"]], "concat_bw() (in module ttnn)": [[70, "ttnn.concat_bw"]], "conj() (in module ttnn)": [[71, "ttnn.conj"]], "conj_bw() (in module ttnn)": [[72, "ttnn.conj_bw"]], "cos() (in module ttnn)": [[73, "ttnn.cos"]], "cos_bw() (in module ttnn)": [[74, "ttnn.cos_bw"]], "cosh() (in module ttnn)": [[75, "ttnn.cosh"]], "cosh_bw() (in module ttnn)": [[76, "ttnn.cosh_bw"]], "create_sharded_memory_config() (in module ttnn)": [[77, "ttnn.create_sharded_memory_config"]], "deallocate() (in module ttnn)": [[78, "ttnn.deallocate"]], "deg2rad() (in module ttnn)": [[79, "ttnn.deg2rad"]], "deg2rad_bw() (in module ttnn)": [[80, "ttnn.deg2rad_bw"]], "digamma() (in module ttnn)": [[81, "ttnn.digamma"]], "digamma_bw() (in module ttnn)": [[82, "ttnn.digamma_bw"]], "div() (in module ttnn)": [[83, "ttnn.div"]], "div_bw() (in module ttnn)": [[84, "ttnn.div_bw"]], "div_no_nan() (in module ttnn)": [[85, "ttnn.div_no_nan"]], "div_no_nan_bw() (in module ttnn)": [[86, "ttnn.div_no_nan_bw"]], "downsample() (in module ttnn)": [[87, "ttnn.downsample"]], "dump_tensor() (in module ttnn)": [[88, "ttnn.dump_tensor"]], "elu() (in module ttnn)": [[89, "ttnn.elu"]], "elu_bw() (in module ttnn)": [[90, "ttnn.elu_bw"]], "embedding() (in module ttnn)": [[91, "ttnn.embedding"]], "embedding_bw() (in module ttnn)": [[92, "ttnn.embedding_bw"]], "empty() (in module ttnn)": [[93, "ttnn.empty"]], "eq() (in module ttnn)": [[94, "ttnn.eq"]], "eq_() (in module ttnn)": [[95, "ttnn.eq_"]], "eqz() (in module ttnn)": [[96, "ttnn.eqz"]], "erf() (in module ttnn)": [[97, "ttnn.erf"]], "erf_bw() (in module ttnn)": [[98, "ttnn.erf_bw"]], "erfc() (in module ttnn)": [[99, "ttnn.erfc"]], "erfc_bw() (in module ttnn)": [[100, "ttnn.erfc_bw"]], "erfinv() (in module ttnn)": [[101, "ttnn.erfinv"]], "erfinv_bw() (in module ttnn)": [[102, "ttnn.erfinv_bw"]], "exp() (in module ttnn)": [[103, "ttnn.exp"]], "exp2() (in module ttnn)": [[104, "ttnn.exp2"]], "exp2_bw() (in module ttnn)": [[105, "ttnn.exp2_bw"]], "exp_bw() (in module ttnn)": [[106, "ttnn.exp_bw"]], "expm1() (in module ttnn)": [[107, "ttnn.expm1"]], "expm1_bw() (in module ttnn)": [[108, "ttnn.expm1_bw"]], "fill_bw() (in module ttnn)": [[109, "ttnn.fill_bw"]], "fill_ones_rm() (in module ttnn)": [[110, "ttnn.fill_ones_rm"]], "fill_rm() (in module ttnn)": [[111, "ttnn.fill_rm"]], "fill_zero_bw() (in module ttnn)": [[112, "ttnn.fill_zero_bw"]], "floor() (in module ttnn)": [[113, "ttnn.floor"]], "floor_bw() (in module ttnn)": [[114, "ttnn.floor_bw"]], "floor_div() (in module ttnn)": [[115, "ttnn.floor_div"]], "fmod() (in module ttnn)": [[116, "ttnn.fmod"]], "fmod_bw() (in module ttnn)": [[117, "ttnn.fmod_bw"]], "format_input_tensor() (in module ttnn)": [[118, "ttnn.format_input_tensor"]], "format_output_tensor() (in module ttnn)": [[119, "ttnn.format_output_tensor"]], "frac() (in module ttnn)": [[120, "ttnn.frac"]], "frac_bw() (in module ttnn)": [[121, "ttnn.frac_bw"]], "from_device() (in module ttnn)": [[122, "ttnn.from_device"]], "from_torch() (in module ttnn)": [[123, "ttnn.from_torch"]], "full() (in module ttnn)": [[124, "ttnn.full"]], "full_like() (in module ttnn)": [[125, "ttnn.full_like"]], "ge() (in module ttnn)": [[126, "ttnn.ge"]], "ge_() (in module ttnn)": [[127, "ttnn.ge_"]], "geglu() (in module ttnn)": [[128, "ttnn.geglu"]], "gelu() (in module ttnn)": [[129, "ttnn.gelu"]], "gelu_bw() (in module ttnn)": [[130, "ttnn.gelu_bw"]], "gez() (in module ttnn)": [[131, "ttnn.gez"]], "global_avg_pool2d() (in module ttnn)": [[132, "ttnn.global_avg_pool2d"]], "glu() (in module ttnn)": [[133, "ttnn.glu"]], "group_norm() (in module ttnn)": [[134, "ttnn.group_norm"]], "gt() (in module ttnn)": [[135, "ttnn.gt"]], "gt_() (in module ttnn)": [[136, "ttnn.gt_"]], "gtz() (in module ttnn)": [[137, "ttnn.gtz"]], "hardshrink() (in module ttnn)": [[138, "ttnn.hardshrink"]], "hardshrink_bw() (in module ttnn)": [[139, "ttnn.hardshrink_bw"]], "hardsigmoid() (in module ttnn)": [[140, "ttnn.hardsigmoid"]], "hardsigmoid_bw() (in module ttnn)": [[141, "ttnn.hardsigmoid_bw"]], "hardswish() (in module ttnn)": [[142, "ttnn.hardswish"]], "hardswish_bw() (in module ttnn)": [[143, "ttnn.hardswish_bw"]], "hardtanh() (in module ttnn)": [[144, "ttnn.hardtanh"]], "hardtanh_bw() (in module ttnn)": [[145, "ttnn.hardtanh_bw"]], "heaviside() (in module ttnn)": [[146, "ttnn.heaviside"]], "hypot() (in module ttnn)": [[147, "ttnn.hypot"]], "hypot_bw() (in module ttnn)": [[148, "ttnn.hypot_bw"]], "i0() (in module ttnn)": [[149, "ttnn.i0"]], "i0_bw() (in module ttnn)": [[150, "ttnn.i0_bw"]], "identity() (in module ttnn)": [[151, "ttnn.identity"]], "imag() (in module ttnn)": [[152, "ttnn.imag"]], "imag_bw() (in module ttnn)": [[153, "ttnn.imag_bw"]], "indexed_fill() (in module ttnn)": [[154, "ttnn.indexed_fill"]], "is_imag() (in module ttnn)": [[155, "ttnn.is_imag"]], "is_real() (in module ttnn)": [[156, "ttnn.is_real"]], "isclose() (in module ttnn)": [[157, "ttnn.isclose"]], "isfinite() (in module ttnn)": [[158, "ttnn.isfinite"]], "isinf() (in module ttnn)": [[159, "ttnn.isinf"]], "isnan() (in module ttnn)": [[160, "ttnn.isnan"]], "isneginf() (in module ttnn)": [[161, "ttnn.isneginf"]], "isposinf() (in module ttnn)": [[162, "ttnn.isposinf"]], "fill_cache_for_user_() (in module ttnn.kv_cache)": [[163, "ttnn.kv_cache.fill_cache_for_user_"]], "update_cache_for_token_() (in module ttnn.kv_cache)": [[164, "ttnn.kv_cache.update_cache_for_token_"]], "l1_loss() (in module ttnn)": [[165, "ttnn.l1_loss"]], "layer_norm() (in module ttnn)": [[166, "ttnn.layer_norm"]], "ldexp() (in module ttnn)": [[167, "ttnn.ldexp"]], "ldexp_bw() (in module ttnn)": [[168, "ttnn.ldexp_bw"]], "le() (in module ttnn)": [[169, "ttnn.le"]], "le_() (in module ttnn)": [[170, "ttnn.le_"]], "leaky_relu() (in module ttnn)": [[171, "ttnn.leaky_relu"]], "leaky_relu_bw() (in module ttnn)": [[172, "ttnn.leaky_relu_bw"]], "lerp() (in module ttnn)": [[173, "ttnn.lerp"]], "lerp_bw() (in module ttnn)": [[174, "ttnn.lerp_bw"]], "lez() (in module ttnn)": [[175, "ttnn.lez"]], "lgamma() (in module ttnn)": [[176, "ttnn.lgamma"]], "lgamma_bw() (in module ttnn)": [[177, "ttnn.lgamma_bw"]], "linear() (in module ttnn)": [[178, "ttnn.linear"]], "load_tensor() (in module ttnn)": [[179, "ttnn.load_tensor"]], "log() (in module ttnn)": [[180, "ttnn.log"]], "log10() (in module ttnn)": [[181, "ttnn.log10"]], "log10_bw() (in module ttnn)": [[182, "ttnn.log10_bw"]], "log1p() (in module ttnn)": [[183, "ttnn.log1p"]], "log1p_bw() (in module ttnn)": [[184, "ttnn.log1p_bw"]], "log2() (in module ttnn)": [[185, "ttnn.log2"]], "log2_bw() (in module ttnn)": [[186, "ttnn.log2_bw"]], "log_bw() (in module ttnn)": [[187, "ttnn.log_bw"]], "log_sigmoid() (in module ttnn)": [[188, "ttnn.log_sigmoid"]], "log_sigmoid_bw() (in module ttnn)": [[189, "ttnn.log_sigmoid_bw"]], "logaddexp() (in module ttnn)": [[190, "ttnn.logaddexp"]], "logaddexp2() (in module ttnn)": [[191, "ttnn.logaddexp2"]], "logaddexp2_bw() (in module ttnn)": [[192, "ttnn.logaddexp2_bw"]], "logaddexp_bw() (in module ttnn)": [[193, "ttnn.logaddexp_bw"]], "logical_and() (in module ttnn)": [[194, "ttnn.logical_and"]], "logical_and_() (in module ttnn)": [[195, "ttnn.logical_and_"]], "logical_not() (in module ttnn)": [[196, "ttnn.logical_not"]], "logical_not_() (in module ttnn)": [[197, "ttnn.logical_not_"]], "logical_or() (in module ttnn)": [[198, "ttnn.logical_or"]], "logical_or_() (in module ttnn)": [[199, "ttnn.logical_or_"]], "logical_xor() (in module ttnn)": [[200, "ttnn.logical_xor"]], "logical_xor_() (in module ttnn)": [[201, "ttnn.logical_xor_"]], "logit() (in module ttnn)": [[202, "ttnn.logit"]], "logit_bw() (in module ttnn)": [[203, "ttnn.logit_bw"]], "logiteps_bw() (in module ttnn)": [[204, "ttnn.logiteps_bw"]], "lt() (in module ttnn)": [[205, "ttnn.lt"]], "lt_() (in module ttnn)": [[206, "ttnn.lt_"]], "ltz() (in module ttnn)": [[207, "ttnn.ltz"]], "mac() (in module ttnn)": [[208, "ttnn.mac"]], "manage_device() (in module ttnn)": [[209, "ttnn.manage_device"]], "matmul() (in module ttnn)": [[210, "ttnn.matmul"]], "max() (in module ttnn)": [[211, "ttnn.max"]], "max_bw() (in module ttnn)": [[212, "ttnn.max_bw"]], "max_pool2d() (in module ttnn)": [[213, "ttnn.max_pool2d"]], "maximum() (in module ttnn)": [[214, "ttnn.maximum"]], "mean() (in module ttnn)": [[215, "ttnn.mean"]], "min() (in module ttnn)": [[216, "ttnn.min"]], "min_bw() (in module ttnn)": [[217, "ttnn.min_bw"]], "minimum() (in module ttnn)": [[218, "ttnn.minimum"]], "mish() (in module ttnn)": [[219, "ttnn.mish"]], "preprocess_model() (in module ttnn.model_preprocessing)": [[220, "ttnn.model_preprocessing.preprocess_model"]], "preprocess_model_parameters() (in module ttnn.model_preprocessing)": [[221, "ttnn.model_preprocessing.preprocess_model_parameters"]], "mse_loss() (in module ttnn)": [[222, "ttnn.mse_loss"]], "mul_bw() (in module ttnn)": [[223, "ttnn.mul_bw"]], "multigammaln() (in module ttnn)": [[224, "ttnn.multigammaln"]], "multigammaln_bw() (in module ttnn)": [[225, "ttnn.multigammaln_bw"]], "multiply() (in module ttnn)": [[226, "ttnn.multiply"]], "ne() (in module ttnn)": [[227, "ttnn.ne"]], "ne_() (in module ttnn)": [[228, "ttnn.ne_"]], "neg() (in module ttnn)": [[229, "ttnn.neg"]], "neg_bw() (in module ttnn)": [[230, "ttnn.neg_bw"]], "nextafter() (in module ttnn)": [[231, "ttnn.nextafter"]], "nez() (in module ttnn)": [[232, "ttnn.nez"]], "nonzero() (in module ttnn)": [[233, "ttnn.nonzero"]], "normalize_global() (in module ttnn)": [[234, "ttnn.normalize_global"]], "normalize_hw() (in module ttnn)": [[235, "ttnn.normalize_hw"]], "ones() (in module ttnn)": [[236, "ttnn.ones"]], "ones_like() (in module ttnn)": [[237, "ttnn.ones_like"]], "open_device() (in module ttnn)": [[238, "ttnn.open_device"]], "outer() (in module ttnn)": [[239, "ttnn.outer"]], "pad() (in module ttnn)": [[240, "ttnn.pad"]], "pad_to_tile_shape() (in module ttnn)": [[241, "ttnn.pad_to_tile_shape"]], "permute() (in module ttnn)": [[242, "ttnn.permute"]], "polar() (in module ttnn)": [[243, "ttnn.polar"]], "polar_bw() (in module ttnn)": [[244, "ttnn.polar_bw"]], "polygamma() (in module ttnn)": [[245, "ttnn.polygamma"]], "polygamma_bw() (in module ttnn)": [[246, "ttnn.polygamma_bw"]], "polyval() (in module ttnn)": [[247, "ttnn.polyval"]], "pow() (in module ttnn)": [[248, "ttnn.pow"]], "pow_bw() (in module ttnn)": [[249, "ttnn.pow_bw"]], "prelu() (in module ttnn)": [[250, "ttnn.prelu"]], "prod() (in module ttnn)": [[251, "ttnn.prod"]], "prod_bw() (in module ttnn)": [[252, "ttnn.prod_bw"]], "rad2deg() (in module ttnn)": [[253, "ttnn.rad2deg"]], "rad2deg_bw() (in module ttnn)": [[254, "ttnn.rad2deg_bw"]], "rdiv() (in module ttnn)": [[255, "ttnn.rdiv"]], "rdiv_bw() (in module ttnn)": [[256, "ttnn.rdiv_bw"]], "real() (in module ttnn)": [[257, "ttnn.real"]], "real_bw() (in module ttnn)": [[258, "ttnn.real_bw"]], "reallocate() (in module ttnn)": [[259, "ttnn.reallocate"]], "reciprocal() (in module ttnn)": [[260, "ttnn.reciprocal"]], "reciprocal_bw() (in module ttnn)": [[261, "ttnn.reciprocal_bw"]], "register_post_operation_hook() (in module ttnn)": [[262, "ttnn.register_post_operation_hook"]], "register_pre_operation_hook() (in module ttnn)": [[263, "ttnn.register_pre_operation_hook"]], "reglu() (in module ttnn)": [[264, "ttnn.reglu"]], "relu() (in module ttnn)": [[265, "ttnn.relu"]], "relu6() (in module ttnn)": [[266, "ttnn.relu6"]], "relu6_bw() (in module ttnn)": [[267, "ttnn.relu6_bw"]], "relu_bw() (in module ttnn)": [[268, "ttnn.relu_bw"]], "relu_max() (in module ttnn)": [[269, "ttnn.relu_max"]], "relu_min() (in module ttnn)": [[270, "ttnn.relu_min"]], "remainder() (in module ttnn)": [[271, "ttnn.remainder"]], "remainder_bw() (in module ttnn)": [[272, "ttnn.remainder_bw"]], "repeat() (in module ttnn)": [[273, "ttnn.repeat"]], "repeat_bw() (in module ttnn)": [[274, "ttnn.repeat_bw"]], "repeat_interleave() (in module ttnn)": [[275, "ttnn.repeat_interleave"]], "reshape() (in module ttnn)": [[276, "ttnn.reshape"]], "rms_norm() (in module ttnn)": [[277, "ttnn.rms_norm"]], "round() (in module ttnn)": [[278, "ttnn.round"]], "round_bw() (in module ttnn)": [[279, "ttnn.round_bw"]], "rpow() (in module ttnn)": [[280, "ttnn.rpow"]], "rpow_bw() (in module ttnn)": [[281, "ttnn.rpow_bw"]], "rsqrt() (in module ttnn)": [[282, "ttnn.rsqrt"]], "rsqrt_bw() (in module ttnn)": [[283, "ttnn.rsqrt_bw"]], "rsub() (in module ttnn)": [[284, "ttnn.rsub"]], "rsub_bw() (in module ttnn)": [[285, "ttnn.rsub_bw"]], "scatter() (in module ttnn)": [[286, "ttnn.scatter"]], "selu() (in module ttnn)": [[287, "ttnn.selu"]], "selu_bw() (in module ttnn)": [[288, "ttnn.selu_bw"]], "set_printoptions() (in module ttnn)": [[289, "ttnn.set_printoptions"]], "sigmoid() (in module ttnn)": [[290, "ttnn.sigmoid"]], "sigmoid_accurate() (in module ttnn)": [[291, "ttnn.sigmoid_accurate"]], "sigmoid_bw() (in module ttnn)": [[292, "ttnn.sigmoid_bw"]], "sign() (in module ttnn)": [[293, "ttnn.sign"]], "sign_bw() (in module ttnn)": [[294, "ttnn.sign_bw"]], "signbit() (in module ttnn)": [[295, "ttnn.signbit"]], "silu() (in module ttnn)": [[296, "ttnn.silu"]], "silu_bw() (in module ttnn)": [[297, "ttnn.silu_bw"]], "sin() (in module ttnn)": [[298, "ttnn.sin"]], "sin_bw() (in module ttnn)": [[299, "ttnn.sin_bw"]], "sinh() (in module ttnn)": [[300, "ttnn.sinh"]], "sinh_bw() (in module ttnn)": [[301, "ttnn.sinh_bw"]], "softmax() (in module ttnn)": [[302, "ttnn.softmax"]], "softplus() (in module ttnn)": [[303, "ttnn.softplus"]], "softplus_bw() (in module ttnn)": [[304, "ttnn.softplus_bw"]], "softshrink() (in module ttnn)": [[305, "ttnn.softshrink"]], "softshrink_bw() (in module ttnn)": [[306, "ttnn.softshrink_bw"]], "softsign() (in module ttnn)": [[307, "ttnn.softsign"]], "softsign_bw() (in module ttnn)": [[308, "ttnn.softsign_bw"]], "split() (in module ttnn)": [[309, "ttnn.split"]], "sqrt() (in module ttnn)": [[310, "ttnn.sqrt"]], "sqrt_bw() (in module ttnn)": [[311, "ttnn.sqrt_bw"]], "square() (in module ttnn)": [[312, "ttnn.square"]], "square_bw() (in module ttnn)": [[313, "ttnn.square_bw"]], "squared_difference() (in module ttnn)": [[314, "ttnn.squared_difference"]], "squared_difference_bw() (in module ttnn)": [[315, "ttnn.squared_difference_bw"]], "std() (in module ttnn)": [[316, "ttnn.std"]], "sub_bw() (in module ttnn)": [[317, "ttnn.sub_bw"]], "subalpha() (in module ttnn)": [[318, "ttnn.subalpha"]], "subalpha_bw() (in module ttnn)": [[319, "ttnn.subalpha_bw"]], "subtract() (in module ttnn)": [[320, "ttnn.subtract"]], "sum() (in module ttnn)": [[321, "ttnn.sum"]], "swiglu() (in module ttnn)": [[322, "ttnn.swiglu"]], "swish() (in module ttnn)": [[323, "ttnn.swish"]], "synchronize_device() (in module ttnn)": [[324, "ttnn.synchronize_device"]], "tan() (in module ttnn)": [[325, "ttnn.tan"]], "tan_bw() (in module ttnn)": [[326, "ttnn.tan_bw"]], "tanh() (in module ttnn)": [[327, "ttnn.tanh"]], "tanh_bw() (in module ttnn)": [[328, "ttnn.tanh_bw"]], "tanhshrink() (in module ttnn)": [[329, "ttnn.tanhshrink"]], "tanhshrink_bw() (in module ttnn)": [[330, "ttnn.tanhshrink_bw"]], "threshold() (in module ttnn)": [[331, "ttnn.threshold"]], "threshold_bw() (in module ttnn)": [[332, "ttnn.threshold_bw"]], "tilize() (in module ttnn)": [[333, "ttnn.tilize"]], "tilize_with_val_padding() (in module ttnn)": [[334, "ttnn.tilize_with_val_padding"]], "tilize_with_zero_padding() (in module ttnn)": [[334, "ttnn.tilize_with_zero_padding"]], "to_device() (in module ttnn)": [[335, "ttnn.to_device"]], "to_layout() (in module ttnn)": [[336, "ttnn.to_layout"]], "to_memory_config() (in module ttnn)": [[337, "ttnn.to_memory_config"]], "to_torch() (in module ttnn)": [[338, "ttnn.to_torch"]], "topk() (in module ttnn)": [[339, "ttnn.topk"]], "attention_softmax() (in module ttnn.transformer)": [[340, "ttnn.transformer.attention_softmax"]], "attention_softmax_() (in module ttnn.transformer)": [[341, "ttnn.transformer.attention_softmax_"]], "concatenate_heads() (in module ttnn.transformer)": [[342, "ttnn.transformer.concatenate_heads"]], "rotary_embedding() (in module ttnn.experimental)": [[343, "ttnn.experimental.rotary_embedding"]], "scaled_dot_product_attention() (in module ttnn.transformer)": [[344, "ttnn.transformer.scaled_dot_product_attention"]], "scaled_dot_product_attention_decode() (in module ttnn.transformer)": [[345, "ttnn.transformer.scaled_dot_product_attention_decode"]], "scaled_dot_product_attention_decode_gqa() (in module ttnn.transformer)": [[346, "ttnn.transformer.scaled_dot_product_attention_decode_gqa"]], "split_query_key_value_and_split_heads() (in module ttnn.transformer)": [[347, "ttnn.transformer.split_query_key_value_and_split_heads"]], "tril() (in module ttnn)": [[348, "ttnn.tril"]], "triu() (in module ttnn)": [[349, "ttnn.triu"]], "trunc() (in module ttnn)": [[350, "ttnn.trunc"]], "trunc_bw() (in module ttnn)": [[351, "ttnn.trunc_bw"]], "untilize() (in module ttnn)": [[352, "ttnn.untilize"]], "untilize_with_halo_v2() (in module ttnn)": [[353, "ttnn.untilize_with_halo_v2"]], "untilize_with_unpadding() (in module ttnn)": [[354, "ttnn.untilize_with_unpadding"]], "upsample() (in module ttnn)": [[355, "ttnn.upsample"]], "var() (in module ttnn)": [[356, "ttnn.var"]], "where() (in module ttnn)": [[357, "ttnn.where"]], "where_bw() (in module ttnn)": [[358, "ttnn.where_bw"]], "xlogy() (in module ttnn)": [[359, "ttnn.xlogy"]], "xlogy_bw() (in module ttnn)": [[360, "ttnn.xlogy_bw"]], "zeros() (in module ttnn)": [[361, "ttnn.zeros"]], "zeros_like() (in module ttnn)": [[362, "ttnn.zeros_like"]]}})